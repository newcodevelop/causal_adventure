{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 1246182,
          "sourceType": "datasetVersion",
          "datasetId": 715500
        }
      ],
      "dockerImageVersionId": 30648,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!ls /kaggle/working/"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T14:05:16.904640Z",
          "iopub.execute_input": "2024-09-20T14:05:16.905046Z",
          "iopub.status.idle": "2024-09-20T14:05:17.964202Z",
          "shell.execute_reply.started": "2024-09-20T14:05:16.905004Z",
          "shell.execute_reply": "2024-09-20T14:05:17.963087Z"
        },
        "trusted": true,
        "id": "oooK6rArXapy",
        "outputId": "f4beef70-1163-4589-b369-3cf80b410d2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "25461.png\t   cr1.pt\t\t\t  mean_comparison_plot.png\n28076.png\t   cr2.pt\t\t\t  mr.pt\n94162.png\t   final_model_concept.pt\t  rr.npy\n96704.png\t   final_model_concept_inlp.pt\t  rr1.npy\ncausal_vb.pt\t   final_model_wo_adversarial.pt  transformers\ncausal_vb_inlp.pt  foo1.png\t\t\t  vs_tensors.pt\ncr.npy\t\t   full_annotation.pkl\t\t  y_p.pt\ncr.pt\t\t   image1.jpg\ncr1.npy\t\t   image2.jpg\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN\n",
        "\n",
        "import ipywidgets as widgets\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:05:55.193327Z",
          "iopub.execute_input": "2024-09-20T20:05:55.194127Z",
          "iopub.status.idle": "2024-09-20T20:05:55.279556Z",
          "shell.execute_reply.started": "2024-09-20T20:05:55.194092Z",
          "shell.execute_reply": "2024-09-20T20:05:55.278838Z"
        },
        "trusted": true,
        "id": "QC7pULXwXap8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "jD2hXbIqXap9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN\n",
        "ff = []"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:05:56.141693Z",
          "iopub.execute_input": "2024-09-20T20:05:56.142068Z",
          "iopub.status.idle": "2024-09-20T20:05:56.145975Z",
          "shell.execute_reply.started": "2024-09-20T20:05:56.142040Z",
          "shell.execute_reply": "2024-09-20T20:05:56.145067Z"
        },
        "trusted": true,
        "id": "eG9x24S-Xap9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN\n",
        "!pip install jsonlines\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:05:57.627155Z",
          "iopub.execute_input": "2024-09-20T20:05:57.627743Z",
          "iopub.status.idle": "2024-09-20T20:06:10.854880Z",
          "shell.execute_reply.started": "2024-09-20T20:05:57.627687Z",
          "shell.execute_reply": "2024-09-20T20:06:10.853762Z"
        },
        "trusted": true,
        "id": "_vFjx5sVXap-",
        "outputId": "6963b614-6d72-4d57-c060-c0daf75c1938"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting jsonlines\n  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\nRequirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonlines) (23.2.0)\nDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\nInstalling collected packages: jsonlines\nSuccessfully installed jsonlines-4.0.0\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN\n",
        "!pip install captum==0.7.0\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:06:10.856923Z",
          "iopub.execute_input": "2024-09-20T20:06:10.857224Z",
          "iopub.status.idle": "2024-09-20T20:06:23.521682Z",
          "shell.execute_reply.started": "2024-09-20T20:06:10.857196Z",
          "shell.execute_reply": "2024-09-20T20:06:23.520515Z"
        },
        "trusted": true,
        "id": "HfGEN1HaXap-",
        "outputId": "da18cb32-21e3-49d8-eb9c-6a6633a97a5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting captum==0.7.0\n  Downloading captum-0.7.0-py3-none-any.whl.metadata (26 kB)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from captum==0.7.0) (3.7.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from captum==0.7.0) (1.24.4)\nRequirement already satisfied: torch>=1.6 in /opt/conda/lib/python3.10/site-packages (from captum==0.7.0) (2.1.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from captum==0.7.0) (4.66.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum==0.7.0) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum==0.7.0) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum==0.7.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum==0.7.0) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum==0.7.0) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum==0.7.0) (2023.12.2)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum==0.7.0) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum==0.7.0) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum==0.7.0) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum==0.7.0) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum==0.7.0) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum==0.7.0) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum==0.7.0) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum==0.7.0) (2.8.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->captum==0.7.0) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6->captum==0.7.0) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6->captum==0.7.0) (1.3.0)\nDownloading captum-0.7.0-py3-none-any.whl (1.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: captum\nSuccessfully installed captum-0.7.0\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "import jsonlines"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:06:23.523040Z",
          "iopub.execute_input": "2024-09-20T20:06:23.523345Z",
          "iopub.status.idle": "2024-09-20T20:06:23.562058Z",
          "shell.execute_reply.started": "2024-09-20T20:06:23.523317Z",
          "shell.execute_reply": "2024-09-20T20:06:23.561174Z"
        },
        "trusted": true,
        "id": "nmhzCxhwXap_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "!pip install wget\n",
        "#!pip install transformers"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:06:23.564372Z",
          "iopub.execute_input": "2024-09-20T20:06:23.564625Z",
          "iopub.status.idle": "2024-09-20T20:06:38.183809Z",
          "shell.execute_reply.started": "2024-09-20T20:06:23.564603Z",
          "shell.execute_reply": "2024-09-20T20:06:38.182454Z"
        },
        "trusted": true,
        "id": "Z2gDYrcyXaqA",
        "outputId": "e4790afc-e219-48d3-91a7-b26b646c6ec5"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting wget\n  Downloading wget-3.2.zip (10 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: wget\n  Building wheel for wget (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=a80596a43fccd4ca4f9b4258f85d9bbe45f5f39de5a5e0afc078a787dcc805ca\n  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\nSuccessfully built wget\nInstalling collected packages: wget\nSuccessfully installed wget-3.2\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "import sys\n",
        "sys.path.append('/kaggle/working/transformers/examples/research_projects/visual_bert')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:06:38.185315Z",
          "iopub.execute_input": "2024-09-20T20:06:38.185623Z",
          "iopub.status.idle": "2024-09-20T20:06:38.190162Z",
          "shell.execute_reply.started": "2024-09-20T20:06:38.185596Z",
          "shell.execute_reply": "2024-09-20T20:06:38.189258Z"
        },
        "trusted": true,
        "id": "87HSBvdnXaqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "import numpy as np\n",
        "import jsonlines\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import sys\n",
        "import os\n",
        "from IPython.display import Image, display\n",
        "import PIL.Image\n",
        "import io\n",
        "import torch\n",
        "import numpy as np\n",
        "from processing_image import Preprocess\n",
        "from visualizing_image import SingleImageViz\n",
        "from modeling_frcnn import GeneralizedRCNN\n",
        "from utils import Config\n",
        "import utils\n",
        "from transformers import VisualBertForQuestionAnswering, BertTokenizerFast\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
        "from sklearn import svm\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:06:38.191327Z",
          "iopub.execute_input": "2024-09-20T20:06:38.191586Z",
          "iopub.status.idle": "2024-09-20T20:06:47.779527Z",
          "shell.execute_reply.started": "2024-09-20T20:06:38.191564Z",
          "shell.execute_reply": "2024-09-20T20:06:47.778766Z"
        },
        "trusted": true,
        "id": "1U4J7pHuXaqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "train_file = '/kaggle/input/facebook-hateful-meme-dataset/data/train.jsonl'\n",
        "k =0\n",
        "with jsonlines.open(train_file) as reader:\n",
        "    for obj in reader:\n",
        "        print(obj['img'])\n",
        "        if k==9:\n",
        "            break\n",
        "        k+=1"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:06:47.781027Z",
          "iopub.execute_input": "2024-09-20T20:06:47.781584Z",
          "iopub.status.idle": "2024-09-20T20:06:47.816516Z",
          "shell.execute_reply.started": "2024-09-20T20:06:47.781551Z",
          "shell.execute_reply": "2024-09-20T20:06:47.815694Z"
        },
        "trusted": true,
        "id": "-Pj53Fc_XaqC",
        "outputId": "ab3703b1-8efe-448a-b6d9-6c043a177735"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "img/42953.png\nimg/23058.png\nimg/13894.png\nimg/37408.png\nimg/82403.png\nimg/16952.png\nimg/76932.png\nimg/70914.png\nimg/02973.png\nimg/58306.png\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "visual_feats = {}"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:06:47.817527Z",
          "iopub.execute_input": "2024-09-20T20:06:47.817806Z",
          "iopub.status.idle": "2024-09-20T20:06:47.821644Z",
          "shell.execute_reply.started": "2024-09-20T20:06:47.817783Z",
          "shell.execute_reply": "2024-09-20T20:06:47.820748Z"
        },
        "trusted": true,
        "id": "sMotq9ySXaqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "device = 'cuda'\n",
        "frcnn_cfg = Config.from_pretrained(\"unc-nlp/frcnn-vg-finetuned\")\n",
        "frcnn_cfg.MODEL.device = device\n",
        "frcnn = GeneralizedRCNN.from_pretrained(\"unc-nlp/frcnn-vg-finetuned\", config=frcnn_cfg)\n",
        "image_preprocess = Preprocess(frcnn_cfg)\n",
        "frcnn.eval()\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:06:47.822797Z",
          "iopub.execute_input": "2024-09-20T20:06:47.823057Z",
          "iopub.status.idle": "2024-09-20T20:07:06.859202Z",
          "shell.execute_reply.started": "2024-09-20T20:06:47.823035Z",
          "shell.execute_reply": "2024-09-20T20:07:06.858376Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "e1981743fa5040f09224ffeade883c80",
            "520d46f637744c83b614547de8285762"
          ]
        },
        "id": "4jO7kkOgXaqD",
        "outputId": "96421d4f-0be2-4ad4-e104-b46b7bf33c61"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "%s not found in cache or force_download set to True, downloading to %s https://s3.amazonaws.com/models.huggingface.co/bert/unc-nlp/frcnn-vg-finetuned/config.yaml /root/.cache/torch/transformers/tmpta0ya4hq\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/2.13k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e1981743fa5040f09224ffeade883c80"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "loading configuration file cache\n%s not found in cache or force_download set to True, downloading to %s https://cdn.huggingface.co/unc-nlp/frcnn-vg-finetuned/pytorch_model.bin /root/.cache/torch/transformers/tmp8o42v8rd\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/262M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "520d46f637744c83b614547de8285762"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "loading weights file https://cdn.huggingface.co/unc-nlp/frcnn-vg-finetuned/pytorch_model.bin from cache at /root/.cache/torch/transformers/57f6df6abe353be2773f2700159c65615babf39ab5b48114d2b49267672ae10f.77b59256a4cf8343ae0f923246a81489fc8d82f98d082edc2d2037c977c0d9d0\nAll model checkpoint weights were used when initializing GeneralizedRCNN.\n\nAll the weights of GeneralizedRCNN were initialized from the model checkpoint at unc-nlp/frcnn-vg-finetuned.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use GeneralizedRCNN for predictions without further training.\n",
          "output_type": "stream"
        },
        {
          "execution_count": 11,
          "output_type": "execute_result",
          "data": {
            "text/plain": "GeneralizedRCNN(\n  (backbone): ResNet(\n    (stem): BasicStem(\n      (conv1): Conv2d(\n        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n        (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (res2): Sequential(\n      (0): BottleneckBlock(\n        (shortcut): Conv2d(\n          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv1): Conv2d(\n          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BottleneckBlock(\n        (conv1): Conv2d(\n          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (2): BottleneckBlock(\n        (conv1): Conv2d(\n          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (res3): Sequential(\n      (0): BottleneckBlock(\n        (shortcut): Conv2d(\n          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv1): Conv2d(\n          256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BottleneckBlock(\n        (conv1): Conv2d(\n          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (2): BottleneckBlock(\n        (conv1): Conv2d(\n          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (3): BottleneckBlock(\n        (conv1): Conv2d(\n          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (res4): Sequential(\n      (0): BottleneckBlock(\n        (shortcut): Conv2d(\n          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv1): Conv2d(\n          512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (2): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (3): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (4): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (5): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (6): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (7): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (8): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (9): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (10): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (11): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (12): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (13): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (14): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (15): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (16): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (17): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (18): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (19): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (20): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (21): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (22): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n    )\n  )\n  (proposal_generator): RPN(\n    (anchor_generator): AnchorGenerator(\n      (cell_anchors): ParameterList(  (0): Parameter containing: [torch.float32 of size 12x4 (cuda:0)])\n    )\n    (rpn_head): RPNHead(\n      (conv): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (objectness_logits): Conv2d(512, 12, kernel_size=(1, 1), stride=(1, 1))\n      (anchor_deltas): Conv2d(512, 48, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n  (roi_heads): Res5ROIHeads(\n    (pooler): ROIPooler(\n      (level_poolers): ModuleList(\n        (0): RoIPool(output_size=(14, 14), spatial_scale=0.0625)\n      )\n    )\n    (res5): Sequential(\n      (0): BottleneckBlock(\n        (shortcut): Conv2d(\n          1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv1): Conv2d(\n          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False\n          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BottleneckBlock(\n        (conv1): Conv2d(\n          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False\n          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (2): BottleneckBlock(\n        (conv1): Conv2d(\n          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False\n          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (box_predictor): FastRCNNOutputLayers(\n      (cls_score): Linear(in_features=2048, out_features=1601, bias=True)\n      (bbox_pred): Linear(in_features=2048, out_features=6400, bias=True)\n      (cls_embedding): Embedding(1601, 256)\n      (fc_attr): Linear(in_features=2304, out_features=512, bias=True)\n      (attr_score): Linear(in_features=512, out_features=401, bias=True)\n    )\n  )\n)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "def get_visual_embedding(img_paths):\n",
        "    images, sizes, scales_yx = image_preprocess(img_paths) # img_paths -> list of image paths\n",
        "    output_dict = frcnn(\n",
        "      images,\n",
        "      sizes,\n",
        "      scales_yx=scales_yx,\n",
        "      padding=\"max_detections\",\n",
        "      max_detections=frcnn_cfg.max_detections,\n",
        "      return_tensors=\"pt\",\n",
        "    )\n",
        "    features = output_dict.get(\"roi_features\")\n",
        "    visual_embeds = features\n",
        "    visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\n",
        "    visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n",
        "    return visual_embeds,visual_token_type_ids,visual_attention_mask"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:07:06.862003Z",
          "iopub.execute_input": "2024-09-20T20:07:06.862289Z",
          "iopub.status.idle": "2024-09-20T20:07:06.868578Z",
          "shell.execute_reply.started": "2024-09-20T20:07:06.862265Z",
          "shell.execute_reply": "2024-09-20T20:07:06.867602Z"
        },
        "trusted": true,
        "id": "KeMbpxruXaqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "visual_feats = torch.load('./vs_tensors.pt')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:07:06.869807Z",
          "iopub.execute_input": "2024-09-20T20:07:06.870143Z",
          "iopub.status.idle": "2024-09-20T20:07:09.993678Z",
          "shell.execute_reply.started": "2024-09-20T20:07:06.870120Z",
          "shell.execute_reply": "2024-09-20T20:07:09.992847Z"
        },
        "trusted": true,
        "id": "EJrs42l_XaqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "len(visual_feats)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:07:09.994836Z",
          "iopub.execute_input": "2024-09-20T20:07:09.995135Z",
          "iopub.status.idle": "2024-09-20T20:07:10.001398Z",
          "shell.execute_reply.started": "2024-09-20T20:07:09.995110Z",
          "shell.execute_reply": "2024-09-20T20:07:10.000336Z"
        },
        "trusted": true,
        "id": "oiSukJqcXaqE",
        "outputId": "8620f2ef-f157-459e-db44-54d9eecf51de"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 14,
          "output_type": "execute_result",
          "data": {
            "text/plain": "8500"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# visual_feats"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:07:10.002632Z",
          "iopub.execute_input": "2024-09-20T20:07:10.002954Z",
          "iopub.status.idle": "2024-09-20T20:07:10.907137Z",
          "shell.execute_reply.started": "2024-09-20T20:07:10.002930Z",
          "shell.execute_reply": "2024-09-20T20:07:10.905953Z"
        },
        "trusted": true,
        "id": "zU4xi7-_XaqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "from utils import Config\n",
        "import utils\n",
        "from transformers import VisualBertModel, BertTokenizer\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.metrics import accuracy_score,f1_score,roc_auc_score\n",
        "from scipy.special import softmax"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:07:10.908526Z",
          "iopub.execute_input": "2024-09-20T20:07:10.909425Z",
          "iopub.status.idle": "2024-09-20T20:07:10.915952Z",
          "shell.execute_reply.started": "2024-09-20T20:07:10.909397Z",
          "shell.execute_reply": "2024-09-20T20:07:10.914977Z"
        },
        "trusted": true,
        "id": "oMkkEs4yXaqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "train_set = []\n",
        "train_file = '/kaggle/input/facebook-hateful-meme-dataset/data/train.jsonl'\n",
        "k =0\n",
        "with jsonlines.open(train_file) as reader:\n",
        "    for obj in reader:\n",
        "    #         print(obj['img'])\n",
        "\n",
        "        train_set.append({'text': obj['text'],\n",
        "\n",
        "                       'img': obj['img'],\n",
        "\n",
        "                       'label': int(obj['label'])})\n",
        "    #         print(k)\n",
        "    #         if k==1500:\n",
        "    #             break\n",
        "        k+=1\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:07:10.917405Z",
          "iopub.execute_input": "2024-09-20T20:07:10.917675Z",
          "iopub.status.idle": "2024-09-20T20:07:10.959382Z",
          "shell.execute_reply.started": "2024-09-20T20:07:10.917653Z",
          "shell.execute_reply": "2024-09-20T20:07:10.958501Z"
        },
        "trusted": true,
        "id": "pKytYPurXaqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "len(train_set)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:07:10.960525Z",
          "iopub.execute_input": "2024-09-20T20:07:10.961398Z",
          "iopub.status.idle": "2024-09-20T20:07:10.966999Z",
          "shell.execute_reply.started": "2024-09-20T20:07:10.961365Z",
          "shell.execute_reply": "2024-09-20T20:07:10.966004Z"
        },
        "trusted": true,
        "id": "BTgoDA7PXaqF",
        "outputId": "f1739a9f-422a-415d-ddfa-43bbce193268"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 18,
          "output_type": "execute_result",
          "data": {
            "text/plain": "8500"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, ds, max_len=64):\n",
        "        self.ds = ds\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "        self.model = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "\n",
        "\n",
        "        self.max_len = max_len\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        ds_idx = self.ds[index]\n",
        "        inputs = self.tokenizer(ds_idx['text'], padding=\"max_length\", truncation=True, max_length=self.max_len, return_tensors='pt')\n",
        "\n",
        "\n",
        "        inputs['input_ids'] = inputs['input_ids'].squeeze(0)\n",
        "        inputs['token_type_ids'] = inputs['token_type_ids'].squeeze(0)\n",
        "        inputs['attention_mask'] = inputs['attention_mask'].squeeze(0)\n",
        "\n",
        "        # visual_embeds,visual_token_type_ids,visual_attention_mask = get_visual_embedding('./data/'+ds_idx['img'])\n",
        "        visual_embeds,visual_token_type_ids,visual_attention_mask = visual_feats[ds_idx['img']]\n",
        "\n",
        "        #print(ds_idx['img'])\n",
        "\n",
        "        inputs.update({\n",
        "          \"visual_embeds\": torch.squeeze(visual_embeds),\n",
        "          \"visual_token_type_ids\": torch.squeeze(visual_token_type_ids),\n",
        "          \"visual_attention_mask\": torch.squeeze(visual_attention_mask)\n",
        "        })\n",
        "\n",
        "\n",
        "\n",
        "        #inputs.update({\"path\": ds_idx['img']})\n",
        "\n",
        "        return inputs, int(ds_idx['label'])\n",
        "\n",
        "t = CustomDataset(train_set)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:09:22.805866Z",
          "iopub.execute_input": "2024-09-20T20:09:22.806286Z",
          "iopub.status.idle": "2024-09-20T20:09:27.511878Z",
          "shell.execute_reply.started": "2024-09-20T20:09:22.806253Z",
          "shell.execute_reply": "2024-09-20T20:09:27.511005Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "fe0a2f4e6b1e45ba94f73d8ffe5536a1",
            "03525e428db24a55a3c2055d3f43096c",
            "5231f56b3a194bcbb55ca739e9aa9a28",
            "82de137f7893491cbd6c80593300c993",
            "245cd965e1614eb68788cc8bd13bb780",
            "80ca6916942a44fe9839666cc7b189e2"
          ]
        },
        "id": "TiFXg5bmXaqF",
        "outputId": "207a972e-18fc-494b-ccd5-8fefd6e845cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fe0a2f4e6b1e45ba94f73d8ffe5536a1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "03525e428db24a55a3c2055d3f43096c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5231f56b3a194bcbb55ca739e9aa9a28"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "82de137f7893491cbd6c80593300c993"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/631 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "245cd965e1614eb68788cc8bd13bb780"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "pytorch_model.bin:   0%|          | 0.00/448M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "80ca6916942a44fe9839666cc7b189e2"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!export CUBLAS_WORKSPACE_CONFIG=:16:8"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:09:27.513547Z",
          "iopub.execute_input": "2024-09-20T20:09:27.513869Z",
          "iopub.status.idle": "2024-09-20T20:09:27.518087Z",
          "shell.execute_reply.started": "2024-09-20T20:09:27.513843Z",
          "shell.execute_reply": "2024-09-20T20:09:27.516955Z"
        },
        "trusted": true,
        "id": "LWkMzYC_XaqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "seed=42\n",
        "random.seed(seed)     # python random generator\n",
        "np.random.seed(seed)  # numpy random generator\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "#torch.use_deterministic_algorithms(True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:09:27.519239Z",
          "iopub.execute_input": "2024-09-20T20:09:27.519560Z",
          "iopub.status.idle": "2024-09-20T20:09:27.532407Z",
          "shell.execute_reply.started": "2024-09-20T20:09:27.519526Z",
          "shell.execute_reply": "2024-09-20T20:09:27.531603Z"
        },
        "trusted": true,
        "id": "TZ2Cp7FEXaqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(t)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:09:27.534361Z",
          "iopub.execute_input": "2024-09-20T20:09:27.534642Z",
          "iopub.status.idle": "2024-09-20T20:09:27.544726Z",
          "shell.execute_reply.started": "2024-09-20T20:09:27.534619Z",
          "shell.execute_reply": "2024-09-20T20:09:27.543768Z"
        },
        "trusted": true,
        "id": "HRdyb7sPXaqG",
        "outputId": "90999bdd-9e47-44e4-a6d2-81fdd795fa2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 22,
          "output_type": "execute_result",
          "data": {
            "text/plain": "8500"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "torch.manual_seed(42)\n",
        "t_p,te_p = torch.utils.data.random_split(t,[7840,660])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:09:30.110787Z",
          "iopub.execute_input": "2024-09-20T20:09:30.111661Z",
          "iopub.status.idle": "2024-09-20T20:09:30.119265Z",
          "shell.execute_reply.started": "2024-09-20T20:09:30.111631Z",
          "shell.execute_reply": "2024-09-20T20:09:30.118385Z"
        },
        "trusted": true,
        "id": "u_2V1kjYXaqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(t_p, shuffle=True, batch_size=32)\n",
        "eval_dataloader = DataLoader(te_p, batch_size=1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:09:30.224392Z",
          "iopub.execute_input": "2024-09-20T20:09:30.224646Z",
          "iopub.status.idle": "2024-09-20T20:09:30.229413Z",
          "shell.execute_reply.started": "2024-09-20T20:09:30.224625Z",
          "shell.execute_reply": "2024-09-20T20:09:30.228441Z"
        },
        "trusted": true,
        "id": "FOKv5U2tXaqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:09:33.414240Z",
          "iopub.execute_input": "2024-09-20T20:09:33.414599Z",
          "iopub.status.idle": "2024-09-20T20:09:33.419351Z",
          "shell.execute_reply.started": "2024-09-20T20:09:33.414574Z",
          "shell.execute_reply": "2024-09-20T20:09:33.418190Z"
        },
        "trusted": true,
        "id": "rt2Vm8L7XaqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('h')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:09:33.647539Z",
          "iopub.execute_input": "2024-09-20T20:09:33.648388Z",
          "iopub.status.idle": "2024-09-20T20:09:34.518511Z",
          "shell.execute_reply.started": "2024-09-20T20:09:33.648358Z",
          "shell.execute_reply": "2024-09-20T20:09:34.517564Z"
        },
        "trusted": true,
        "id": "M8_XBTUFXaqH",
        "outputId": "097ac4c7-3283-4398-8cec-524329706e6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "h\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "concept_classes = [\"holocaust\", \"nazism\", \"genocide\", \"funny\", \"anti muslim\", \"terrorism\", \"violence\", \"politics\", \"racism\", \"international relation\", \"adult\", \"gore\", \"misogynistic\", \"immigration\", \"extremism\", \"immoral\", \"white supremacy\", \"indecency\"]\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:09:34.520660Z",
          "iopub.execute_input": "2024-09-20T20:09:34.521045Z",
          "iopub.status.idle": "2024-09-20T20:09:34.589888Z",
          "shell.execute_reply.started": "2024-09-20T20:09:34.521012Z",
          "shell.execute_reply": "2024-09-20T20:09:34.588912Z"
        },
        "trusted": true,
        "id": "a6H7aPyWXaqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(concept_classes)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:09:36.091740Z",
          "iopub.execute_input": "2024-09-20T20:09:36.092381Z",
          "iopub.status.idle": "2024-09-20T20:09:36.098248Z",
          "shell.execute_reply.started": "2024-09-20T20:09:36.092351Z",
          "shell.execute_reply": "2024-09-20T20:09:36.097172Z"
        },
        "trusted": true,
        "id": "WpwUVc2RXaqI",
        "outputId": "bda4dd6d-fe08-4307-a801-7af0ec1c7d90"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 28,
          "output_type": "execute_result",
          "data": {
            "text/plain": "18"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnt"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:09:36.181005Z",
          "iopub.execute_input": "2024-09-20T20:09:36.181556Z",
          "iopub.status.idle": "2024-09-20T20:09:36.868875Z",
          "shell.execute_reply.started": "2024-09-20T20:09:36.181531Z",
          "shell.execute_reply": "2024-09-20T20:09:36.867758Z"
        },
        "trusted": true,
        "id": "jsmAd8RPXaqJ",
        "outputId": "f03381d2-c968-41bf-a99f-cffe98307148"
      },
      "execution_count": null,
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcnt\u001b[49m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cnt' is not defined"
          ],
          "ename": "NameError",
          "evalue": "name 'cnt' is not defined",
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GlIqP5k_XaqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-JTSR71IXaqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rznrnbtFXaqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zmd3EbchXaqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xXTCOQQQXaqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-07T13:26:10.133975Z",
          "iopub.execute_input": "2024-04-07T13:26:10.134397Z",
          "iopub.status.idle": "2024-04-07T13:26:10.139811Z",
          "shell.execute_reply.started": "2024-04-07T13:26:10.134362Z",
          "shell.execute_reply": "2024-04-07T13:26:10.138850Z"
        },
        "trusted": true,
        "id": "EBFHS_xiXaqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "with open('/kaggle/working/full_annotation.pkl', 'rb') as f:\n",
        "    mynewlist = pickle.load(f)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:09:40.607191Z",
          "iopub.execute_input": "2024-09-20T20:09:40.607550Z",
          "iopub.status.idle": "2024-09-20T20:09:40.612595Z",
          "shell.execute_reply.started": "2024-09-20T20:09:40.607521Z",
          "shell.execute_reply": "2024-09-20T20:09:40.611680Z"
        },
        "trusted": true,
        "id": "S9fHmOY1XaqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(mynewlist)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:09:40.867081Z",
          "iopub.execute_input": "2024-09-20T20:09:40.867377Z",
          "iopub.status.idle": "2024-09-20T20:09:40.873417Z",
          "shell.execute_reply.started": "2024-09-20T20:09:40.867352Z",
          "shell.execute_reply": "2024-09-20T20:09:40.872457Z"
        },
        "trusted": true,
        "id": "u4imtJQVXaqL",
        "outputId": "15554630-51fd-45e3-c9b4-adbcfab3e40e"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 31,
          "output_type": "execute_result",
          "data": {
            "text/plain": "208"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "awU2ttRxXaqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "gc=mynewlist"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:09:42.756806Z",
          "iopub.execute_input": "2024-09-20T20:09:42.757168Z",
          "iopub.status.idle": "2024-09-20T20:09:42.761529Z",
          "shell.execute_reply.started": "2024-09-20T20:09:42.757141Z",
          "shell.execute_reply": "2024-09-20T20:09:42.760519Z"
        },
        "trusted": true,
        "id": "b6qVY_3yXaqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "device='cuda'"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:09:43.926845Z",
          "iopub.execute_input": "2024-09-20T20:09:43.927212Z",
          "iopub.status.idle": "2024-09-20T20:09:43.931432Z",
          "shell.execute_reply.started": "2024-09-20T20:09:43.927185Z",
          "shell.execute_reply": "2024-09-20T20:09:43.930416Z"
        },
        "trusted": true,
        "id": "8xbhcuN2XaqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9X0DsEhUXaqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:09:46.068462Z",
          "iopub.execute_input": "2024-09-20T20:09:46.068850Z",
          "iopub.status.idle": "2024-09-20T20:09:46.074854Z",
          "shell.execute_reply.started": "2024-09-20T20:09:46.068819Z",
          "shell.execute_reply": "2024-09-20T20:09:46.073891Z"
        },
        "trusted": true,
        "id": "4KoFuha9XaqN",
        "outputId": "ae0b3123-f98e-44bd-ce83-d7827ca4dac7"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 34,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'cuda'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:09:47.773357Z",
          "iopub.execute_input": "2024-09-20T20:09:47.773752Z",
          "iopub.status.idle": "2024-09-20T20:09:47.935226Z",
          "shell.execute_reply.started": "2024-09-20T20:09:47.773719Z",
          "shell.execute_reply": "2024-09-20T20:09:47.934498Z"
        },
        "trusted": true,
        "id": "sWIsLT_9XaqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(\"jew\")[1:-1]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:09:49.638363Z",
          "iopub.execute_input": "2024-09-20T20:09:49.638761Z",
          "iopub.status.idle": "2024-09-20T20:09:49.645918Z",
          "shell.execute_reply.started": "2024-09-20T20:09:49.638700Z",
          "shell.execute_reply": "2024-09-20T20:09:49.644928Z"
        },
        "trusted": true,
        "id": "0LeU7RzzXaqN",
        "outputId": "fde75225-d2bf-4482-913c-f55f9e439b7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 36,
          "output_type": "execute_result",
          "data": {
            "text/plain": "[16522]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "DYN_ROUTE = False\n",
        "DECONFOUND = True"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:09:50.169514Z",
          "iopub.execute_input": "2024-09-20T20:09:50.170260Z",
          "iopub.status.idle": "2024-09-20T20:09:50.174248Z",
          "shell.execute_reply.started": "2024-09-20T20:09:50.170229Z",
          "shell.execute_reply": "2024-09-20T20:09:50.173377Z"
        },
        "trusted": true,
        "id": "FzSMauPrXaqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "!pip install pytorch_revgrad\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:09:52.343715Z",
          "iopub.execute_input": "2024-09-20T20:09:52.344102Z",
          "iopub.status.idle": "2024-09-20T20:10:04.623651Z",
          "shell.execute_reply.started": "2024-09-20T20:09:52.344071Z",
          "shell.execute_reply": "2024-09-20T20:10:04.622644Z"
        },
        "trusted": true,
        "id": "OwhgLArZXaqO",
        "outputId": "ae8ec212-3ffb-47a5-e0ce-9b44d36882bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting pytorch_revgrad\n  Downloading pytorch_revgrad-0.2.0-py3-none-any.whl.metadata (1.7 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pytorch_revgrad) (1.24.4)\nRequirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_revgrad) (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->pytorch_revgrad) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->pytorch_revgrad) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->pytorch_revgrad) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->pytorch_revgrad) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->pytorch_revgrad) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->pytorch_revgrad) (2023.12.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->pytorch_revgrad) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->pytorch_revgrad) (1.3.0)\nDownloading pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\nInstalling collected packages: pytorch_revgrad\nSuccessfully installed pytorch_revgrad-0.2.0\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.nn.Parameter(torch.tensor(0.1))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:10:04.625764Z",
          "iopub.execute_input": "2024-09-20T20:10:04.626152Z",
          "iopub.status.idle": "2024-09-20T20:10:04.630721Z",
          "shell.execute_reply.started": "2024-09-20T20:10:04.626115Z",
          "shell.execute_reply": "2024-09-20T20:10:04.629921Z"
        },
        "trusted": true,
        "id": "Dt0_zxYFXaqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "num_concepts = 18\n",
        "#BS = 8\n",
        "import torch\n",
        "from torch import nn\n",
        "from pytorch_revgrad import RevGrad\n",
        "class VB(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VB, self).__init__()\n",
        "        self.vb = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(768, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 2),\n",
        "        )\n",
        "\n",
        "\n",
        "        self.linear_relu_stack_latent = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(768, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 2),\n",
        "        )\n",
        "\n",
        "\n",
        "        self.linear_relu_stack_hidden = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(768, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 2),\n",
        "        )\n",
        "\n",
        "        #self.w = torch.nn.Parameter(torch.ones(11,1)/11)\n",
        "        self.w = torch.nn.Parameter(torch.ones(num_concepts,1))\n",
        "\n",
        "        self.project_latent = nn.Linear(768,2048)\n",
        "\n",
        "        #self.W_ij = [[nn.Linear(768,768) for i in range(64)] for j in range(17)]\n",
        "\n",
        "        self.W_ij = torch.nn.Parameter(torch.randn(18,64,28,28))\n",
        "\n",
        "        self.proj_conc = nn.Linear(768,28)\n",
        "        self.proj_embed = nn.Linear(768,28)\n",
        "\n",
        "\n",
        "\n",
        "        self.grad_rev1 = RevGrad()\n",
        "        self.grad_rev2 = RevGrad()\n",
        "\n",
        "        self.w1 = torch.nn.Parameter(torch.tensor(0.2))\n",
        "\n",
        "\n",
        "    def forward(self,\n",
        "        all_concepts_,\n",
        "        input_ids = None,\n",
        "        attention_mask = None,\n",
        "        token_type_ids = None,\n",
        "        position_ids = None,\n",
        "        head_mask = None,\n",
        "        inputs_embeds = None,\n",
        "        visual_embeds = None,\n",
        "        visual_attention_mask = None,\n",
        "        visual_token_type_ids = None,\n",
        "        image_text_alignment = None,\n",
        "        output_attentions = None,\n",
        "        output_hidden_states = None,\n",
        "        return_dict = None):\n",
        "\n",
        "        #         print(all_concepts.shape)\n",
        "\n",
        "        B = visual_embeds.size(0)\n",
        "\n",
        "        #         print(B)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ##############################################\n",
        "\n",
        "        all_concepts = self.proj_conc(all_concepts_)\n",
        "\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs_embeds = self.vb.embeddings.word_embeddings(input_ids)\n",
        "\n",
        "        #print(0/0)\n",
        "\n",
        "        inputs_embeds = self.proj_embed(inputs_embeds)\n",
        "\n",
        "\n",
        "        #####################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #b1 = torch.bmm(inputs_embeds,all_concepts.repeat(B,1,1).transpose(1,2))\n",
        "\n",
        "        #print(inputs_embeds.shape)\n",
        "        #print(all_concepts.repeat(B,1,1).transpose(1,2).shape)\n",
        "\n",
        "        #b,64,28\n",
        "        #b,28,17\n",
        "\n",
        "\n",
        "        b = torch.einsum('blx,bxn->bln',inputs_embeds,all_concepts.repeat(B,1,1).transpose(1,2))\n",
        "\n",
        "        #         print(b1)\n",
        "\n",
        "        #         print('********')\n",
        "\n",
        "        #         print(b2)\n",
        "\n",
        "        #         print(torch.equal(b1,b2))\n",
        "\n",
        "\n",
        "\n",
        "        b = b.masked_fill(~attention_mask.unsqueeze(dim=2).bool(), -np.inf)\n",
        "        c = F.softmax(b,dim=1)\n",
        "\n",
        "\n",
        "\n",
        "        #inter = torch.einsum('bcsn,csnn->bcsn',inputs_embeds.unsqueeze(1).repeat(1,17,1,1), self.W_ij)\n",
        "\n",
        "        #print(inter[:,0,0,:])\n",
        "\n",
        "        einsum_expression = 'bln,clnx->bclx'\n",
        "        inter = torch.einsum(einsum_expression, inputs_embeds, self.W_ij)\n",
        "\n",
        "        k = torch.einsum('bcsn,bsc->bcn',inter,c)\n",
        "        norm = torch.norm(k,dim=2)\n",
        "\n",
        "\n",
        "\n",
        "        kk1 = ((norm**2)/(1+norm**2)).unsqueeze(dim=2) * (k/norm.unsqueeze(dim=2))\n",
        "\n",
        "        w_dynamic = kk1.norm(dim=2).unsqueeze(2)\n",
        "\n",
        "\n",
        "        raw_latent = (1-self.w1) * all_concepts_.unsqueeze(0).repeat(B,1,1) * self.w.unsqueeze(dim=0).repeat(B,1,1) + all_concepts_.unsqueeze(0).repeat(B,1,1) * w_dynamic * self.w1\n",
        "\n",
        "        raw_latent = torch.mean(raw_latent, dim=1)\n",
        "\n",
        "        #raw_latent = torch.mean(all_concepts*self.w,dim=0)\n",
        "        #raw_latent = raw_latent.unsqueeze(dim=0)\n",
        "\n",
        "        latent = self.project_latent(raw_latent)\n",
        "\n",
        "\n",
        "\n",
        "        latent = latent.unsqueeze(dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        visual_embeds_latent = torch.cat((visual_embeds,latent),dim=1)\n",
        "\n",
        "\n",
        "        ################################################################\n",
        "\n",
        "\n",
        "        #print(b)\n",
        "\n",
        "        #######################################################\n",
        "        #         print(b.shape)\n",
        "\n",
        "        #         b = []\n",
        "        #         for i in range(64):\n",
        "        #             bi = []\n",
        "        #             for j in range(17):\n",
        "        #                 #print(inputs_embeds[:,i,:].shape)\n",
        "        #                 ci = all_concepts[j].unsqueeze(0).repeat(32,1,1).squeeze()\n",
        "        #                 uj = inputs_embeds[:,i,:]\n",
        "        #                 #print(ci.shape)\n",
        "        #                 bij = torch.bmm(uj.unsqueeze(dim=1), ci.unsqueeze(dim=2))\n",
        "        #                 #print(bij.shape)\n",
        "        #                 #print(bij)\n",
        "        #                 bi.append(bij.squeeze())\n",
        "        #             bi = torch.stack(bi)\n",
        "\n",
        "        #             bi = bi.T\n",
        "        #             #print(bi.shape)\n",
        "        #             #print(bi)\n",
        "        #             b.append(bi)\n",
        "\n",
        "\n",
        "\n",
        "        #         b = torch.stack(b)\n",
        "        #         b = b.transpose(0,1)\n",
        "\n",
        "        #         print(b)\n",
        "        #         print(b.shape)\n",
        "\n",
        "\n",
        "        #print(0/0)\n",
        "\n",
        "        ####################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #c = F.softmax(b,dim=1)\n",
        "\n",
        "        #print(c)\n",
        "        #print(c.shape)\n",
        "        #print(0/0)\n",
        "        ############################################################\n",
        "\n",
        "        #print\n",
        "\n",
        "        #c = 32*64*17\n",
        "\n",
        "        #u_j_i = 32*768\n",
        "\n",
        "\n",
        "        #cij = 32*1\n",
        "\n",
        "\n",
        "        #         kk = []\n",
        "        #         for i in range(17):\n",
        "        #             si = 0\n",
        "        #             for j in range(64):\n",
        "        #                 #print(inputs_embeds[:,j,:].shape)\n",
        "        #                 #print(self.W_ij)\n",
        "        #                 #print(self.W_ij[i,j,:,:].shape)\n",
        "\n",
        "\n",
        "\n",
        "        #                 u_j_i = torch.mm(inputs_embeds[:,j,:],self.W_ij[i,j,:,:])\n",
        "\n",
        "\n",
        "        #                 #print(j,i,u_j_i)\n",
        "        #                 #print(0/0)\n",
        "\n",
        "        #                 cij = c[:,j,i].unsqueeze(dim=1)\n",
        "        #                 #print('cij ',cij)\n",
        "        #                 #print(cij.shape)\n",
        "        #                 #print(u_j_i)\n",
        "        #                 #print(u_j_i.shape)\n",
        "        #                 si+= u_j_i*cij\n",
        "\n",
        "        #                 #print(u_j_i*cij)\n",
        "        #             #print(si)\n",
        "        #             #print(0/0)\n",
        "\n",
        "        #             norm = torch.norm(si, dim=1)\n",
        "\n",
        "        #             #print(norm)\n",
        "        #             #print(norm.shape)\n",
        "\n",
        "        #             vi = ((norm**2)/(1+norm**2)).unsqueeze(dim=1) * (si/norm.unsqueeze(dim=1))\n",
        "\n",
        "        #             #print(vi)\n",
        "        #             kk.append(torch.norm(vi,dim=1))\n",
        "        #             #kk.append(vi)\n",
        "        #             #print(kk)\n",
        "\n",
        "\n",
        "        #         kk = torch.stack(kk)\n",
        "\n",
        "        #         print('kk', kk)\n",
        "        #         print('kk1', kk1)\n",
        "        #         print(kk.shape)\n",
        "        #         print(kk1.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Create a tensor of ones with the same number of columns as the original tensor\n",
        "        ones_column_vam = torch.ones((visual_attention_mask.size(0), 1), dtype=visual_attention_mask.dtype)\n",
        "\n",
        "        # Concatenate the original tensor with the tensor of ones along the second dimension (columns)\n",
        "        visual_attention_mask_latent = torch.cat((visual_attention_mask, ones_column_vam.to('cuda')), dim=1)\n",
        "\n",
        "        # Create a tensor of ones with the same number of columns as the original tensor\n",
        "        ones_column_vtt = torch.ones((visual_token_type_ids.size(0), 1), dtype=visual_token_type_ids.dtype)\n",
        "\n",
        "        # Concatenate the original tensor with the tensor of ones along the second dimension (columns)\n",
        "        visual_token_type_ids_latent = torch.cat((visual_token_type_ids, ones_column_vtt.to('cuda')), dim=1)\n",
        "\n",
        "        #         print(latent.shape)\n",
        "\n",
        "        #         print(visual_embeds.shape)\n",
        "        #         print(visual_attention_mask.shape)\n",
        "        #         print(visual_token_type_ids.shape)\n",
        "\n",
        "        #         print(self.w)\n",
        "\n",
        "        x = self.vb(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask, visual_embeds=visual_embeds_latent, visual_attention_mask=visual_attention_mask_latent, visual_token_type_ids=visual_token_type_ids_latent)\n",
        "\n",
        "        p = self.vb(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask, visual_embeds=visual_embeds, visual_attention_mask=visual_attention_mask, visual_token_type_ids=visual_token_type_ids)\n",
        "        p = self.grad_rev1(p.pooler_output)\n",
        "        #q = self.grad_rev2(raw_latent.repeat(B,1))\n",
        "        q = self.grad_rev2(raw_latent)\n",
        "\n",
        "        logits_p = self.linear_relu_stack_hidden(p)\n",
        "        logits_q = self.linear_relu_stack_latent(q)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        logits = self.linear_relu_stack(x.pooler_output)\n",
        "\n",
        "\n",
        "        if_logits_p = self.linear_relu_stack(p)\n",
        "        if_logits_q = self.linear_relu_stack(q)\n",
        "\n",
        "        return logits, logits_p, logits_q, if_logits_p, if_logits_q\n",
        "        #return logits, logits_p, logits_q, if_logits_p, if_logits_p\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:10:14.917920Z",
          "iopub.execute_input": "2024-09-20T20:10:14.918253Z",
          "iopub.status.idle": "2024-09-20T20:10:14.955560Z",
          "shell.execute_reply.started": "2024-09-20T20:10:14.918229Z",
          "shell.execute_reply": "2024-09-20T20:10:14.954815Z"
        },
        "trusted": true,
        "id": "SVNWmsT4XaqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3pZcXWLpXaqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EB1qkB0RXaqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vTEl3TMEXaqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7-g7mOemXaqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g8tXrzOIXaqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "vb = VB()\n",
        "\n",
        "# vb = nn.DataParallel(vb)\n",
        "vb.to(device)\n",
        "concept_classes = [\"holocaust\", \"nazism\", \"genocide\", \"funny\", \"anti muslim\", \"terrorism\", \"violence\", \"politics\", \"racism\", \"international relation\", \"adult\", \"gore\", \"misogynistic\", \"immigration\", \"extremism\", \"immoral\", \"white supremacy\", \"indecency\"]\n",
        "\n",
        "\n",
        "concept_embedding_map = {}\n",
        "\n",
        "all_concepts = []\n",
        "\n",
        "\n",
        "for c in concept_classes:\n",
        "    conc = torch.tensor(tokenizer.encode(c)[1:-1])\n",
        "    e = vb.vb.embeddings.word_embeddings(conc.to('cuda'))\n",
        "    x = e.detach().cpu().mean(dim=0)\n",
        "\n",
        "\n",
        "\n",
        "    all_concepts.append(x)\n",
        "\n",
        "    concept_embedding_map[c] = x\n",
        "\n",
        "\n",
        "all_concepts = torch.stack(all_concepts)\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:10:18.331157Z",
          "iopub.execute_input": "2024-09-20T20:10:18.331845Z",
          "iopub.status.idle": "2024-09-20T20:10:19.296279Z",
          "shell.execute_reply.started": "2024-09-20T20:10:18.331812Z",
          "shell.execute_reply": "2024-09-20T20:10:19.295499Z"
        },
        "trusted": true,
        "id": "OM5LpT6QXaqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "all_concepts.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:10:19.671087Z",
          "iopub.execute_input": "2024-09-20T20:10:19.671966Z",
          "iopub.status.idle": "2024-09-20T20:10:19.678116Z",
          "shell.execute_reply.started": "2024-09-20T20:10:19.671931Z",
          "shell.execute_reply": "2024-09-20T20:10:19.677084Z"
        },
        "trusted": true,
        "id": "8a10BMFfXaqS",
        "outputId": "f76a05a7-336b-4873-94cb-b4c4abf822ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 42,
          "output_type": "execute_result",
          "data": {
            "text/plain": "torch.Size([18, 768])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "vb = VB()\n",
        "vb.load_state_dict(torch.load('/kaggle/working/final_model_concept.pt'))\n",
        "vb.to('cuda')\n",
        "vb.eval()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:10:22.200471Z",
          "iopub.execute_input": "2024-09-20T20:10:22.201144Z",
          "iopub.status.idle": "2024-09-20T20:10:23.276446Z",
          "shell.execute_reply.started": "2024-09-20T20:10:22.201100Z",
          "shell.execute_reply": "2024-09-20T20:10:23.275435Z"
        },
        "trusted": true,
        "id": "GpX-Cu-8XaqS",
        "outputId": "3a48bbcf-7ea0-485a-b275-a9348186363f"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 43,
          "output_type": "execute_result",
          "data": {
            "text/plain": "VB(\n  (vb): VisualBertModel(\n    (embeddings): VisualBertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=1)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n      (visual_token_type_embeddings): Embedding(2, 768)\n      (visual_position_embeddings): Embedding(512, 768)\n      (visual_projection): Linear(in_features=2048, out_features=768, bias=True)\n    )\n    (encoder): VisualBertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x VisualBertLayer(\n          (attention): VisualBertAttention(\n            (self): VisualBertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): VisualBertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): VisualBertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): VisualBertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): VisualBertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (linear_relu_stack): Sequential(\n    (0): Dropout(p=0.1, inplace=False)\n    (1): Linear(in_features=768, out_features=512, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=512, out_features=128, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=128, out_features=2, bias=True)\n  )\n  (linear_relu_stack_latent): Sequential(\n    (0): Dropout(p=0.1, inplace=False)\n    (1): Linear(in_features=768, out_features=512, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=512, out_features=128, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=128, out_features=2, bias=True)\n  )\n  (linear_relu_stack_hidden): Sequential(\n    (0): Dropout(p=0.1, inplace=False)\n    (1): Linear(in_features=768, out_features=512, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=512, out_features=128, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=128, out_features=2, bias=True)\n  )\n  (project_latent): Linear(in_features=768, out_features=2048, bias=True)\n  (proj_conc): Linear(in_features=768, out_features=28, bias=True)\n  (proj_embed): Linear(in_features=768, out_features=28, bias=True)\n  (grad_rev1): RevGrad()\n  (grad_rev2): RevGrad()\n)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# all_concepts.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-08T19:16:02.452605Z",
          "iopub.execute_input": "2024-04-08T19:16:02.453217Z",
          "iopub.status.idle": "2024-04-08T19:16:02.456971Z",
          "shell.execute_reply.started": "2024-04-08T19:16:02.453184Z",
          "shell.execute_reply": "2024-04-08T19:16:02.456066Z"
        },
        "trusted": true,
        "id": "TSP5x69QXaqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all_concepts[0].unsqueeze(0).shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-08T19:16:02.804764Z",
          "iopub.execute_input": "2024-04-08T19:16:02.805412Z",
          "iopub.status.idle": "2024-04-08T19:16:02.809242Z",
          "shell.execute_reply.started": "2024-04-08T19:16:02.805380Z",
          "shell.execute_reply": "2024-04-08T19:16:02.808252Z"
        },
        "trusted": true,
        "id": "M_Ep8axQXaqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all_concepts[0].unsqueeze(0).repeat(32,1,1).squeeze().shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-08T19:16:03.142450Z",
          "iopub.execute_input": "2024-04-08T19:16:03.143322Z",
          "iopub.status.idle": "2024-04-08T19:16:03.147015Z",
          "shell.execute_reply.started": "2024-04-08T19:16:03.143275Z",
          "shell.execute_reply": "2024-04-08T19:16:03.146068Z"
        },
        "trusted": true,
        "id": "q6e_tuF_XaqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "# check evaluation performance separately on p and q using their classifier: should be less\n",
        "pred_r1, pred_r2 = [],[]\n",
        "pred_both = []\n",
        "act = []\n",
        "for ii, batch in enumerate(eval_dataloader):\n",
        "    with torch.no_grad():\n",
        "        batch_inp = {k: v.to(device) for k, v in batch[0].items()}\n",
        "        outputs, r1, r2,_,_ = vb(all_concepts.to('cuda'),**batch_inp)\n",
        "        act.append(batch[1].detach().cpu().numpy()[0])\n",
        "\n",
        "        pred_r1.append(np.argmax(r1.detach().cpu().numpy()[0]))\n",
        "        pred_r2.append(np.argmax(r2.detach().cpu().numpy()[0]))\n",
        "        pred_both.append(np.argmax(outputs.detach().cpu().numpy()[0]))\n",
        "\n",
        "print('*')\n",
        "print(f1_score(act,pred_r1,average='macro'))\n",
        "print(f1_score(act,pred_r1,average='weighted'))\n",
        "print(accuracy_score(act,pred_r1))\n",
        "print('*'*10)\n",
        "print(f1_score(act,pred_r2,average='macro'))\n",
        "print(f1_score(act,pred_r2,average='weighted'))\n",
        "print(accuracy_score(act,pred_r2))\n",
        "print('*'*10)\n",
        "print(f1_score(act,pred_both,average='macro'))\n",
        "print(f1_score(act,pred_both,average='weighted'))\n",
        "print(accuracy_score(act,pred_both))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:10:27.450147Z",
          "iopub.execute_input": "2024-09-20T20:10:27.450794Z",
          "iopub.status.idle": "2024-09-20T20:10:42.943916Z",
          "shell.execute_reply.started": "2024-09-20T20:10:27.450761Z",
          "shell.execute_reply": "2024-09-20T20:10:42.942892Z"
        },
        "trusted": true,
        "id": "KADOePdKXaqU",
        "outputId": "2d821be0-e11d-45f8-c240-ee3c6af203cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "*\n0.40647482014388486\n0.5567473294091998\n0.6848484848484848\n**********\n0.40647482014388486\n0.5567473294091998\n0.6848484848484848\n**********\n0.7036565354754966\n0.7469814601650707\n0.75\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check evaluation performance separately on p and q using parent classifier: should be also less\n",
        "pred_r1, pred_r2 = [],[]\n",
        "pred_both = []\n",
        "act = []\n",
        "for ii, batch in enumerate(eval_dataloader):\n",
        "    with torch.no_grad():\n",
        "        batch_inp = {k: v.to(device) for k, v in batch[0].items()}\n",
        "        outputs, _,_, r1, r2 = vb(all_concepts.to('cuda'),**batch_inp)\n",
        "        act.append(batch[1].detach().cpu().numpy()[0])\n",
        "\n",
        "        pred_r1.append(np.argmax(r1.detach().cpu().numpy()[0]))\n",
        "        pred_r2.append(np.argmax(r2.detach().cpu().numpy()[0]))\n",
        "        pred_both.append(np.argmax(outputs.detach().cpu().numpy()[0]))\n",
        "\n",
        "print('*'*10)\n",
        "print(f1_score(act,pred_r1,average='macro'))\n",
        "print(f1_score(act,pred_r1,average='weighted'))\n",
        "print(accuracy_score(act,pred_r1))\n",
        "print('*'*10)\n",
        "print(f1_score(act,pred_r2,average='macro'))\n",
        "print(f1_score(act,pred_r2,average='weighted'))\n",
        "print(accuracy_score(act,pred_r2))\n",
        "print('*'*10)\n",
        "print(f1_score(act,pred_both,average='macro'))\n",
        "print(f1_score(act,pred_both,average='weighted'))\n",
        "print(accuracy_score(act,pred_both))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:12:51.902121Z",
          "iopub.execute_input": "2024-09-20T20:12:51.902840Z",
          "iopub.status.idle": "2024-09-20T20:13:07.206806Z",
          "shell.execute_reply.started": "2024-09-20T20:12:51.902805Z",
          "shell.execute_reply": "2024-09-20T20:13:07.205840Z"
        },
        "trusted": true,
        "id": "B-MCA0HWXaqU",
        "outputId": "d5f3d6cb-35eb-48d8-bcc1-858e99be139c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "**********\n0.23963133640552994\n0.1510403574919704\n0.3151515151515151\n**********\n0.23963133640552994\n0.1510403574919704\n0.3151515151515151\n**********\n0.7036565354754966\n0.7469814601650707\n0.75\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.save(vb.state_dict(), '/kaggle/working/final_model_wo_adversarial.pt')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:13:07.208605Z",
          "iopub.execute_input": "2024-09-20T20:13:07.209001Z",
          "iopub.status.idle": "2024-09-20T20:13:07.213349Z",
          "shell.execute_reply.started": "2024-09-20T20:13:07.208968Z",
          "shell.execute_reply": "2024-09-20T20:13:07.212466Z"
        },
        "trusted": true,
        "id": "WRq6eULRXaqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "from typing import Dict\n",
        "\n",
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "\n",
        "\n",
        "# import svm_classifier\n",
        "\n",
        "REGRESSION = False\n",
        "from typing import List\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "def get_weights(model) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        :return: final weights of the model, as np array\n",
        "        \"\"\"\n",
        "\n",
        "        w = model.coef_\n",
        "        if len(w.shape) == 1:\n",
        "                w = np.expand_dims(w, 0)\n",
        "\n",
        "        return w\n",
        "\n",
        "\n",
        "def train_network(model, X_train: np.ndarray, Y_train: np.ndarray, X_dev: np.ndarray, Y_dev: np.ndarray) -> float:\n",
        "\n",
        "        \"\"\"\n",
        "        :param X_train:\n",
        "        :param Y_train:\n",
        "        :param X_dev:\n",
        "        :param Y_dev:\n",
        "        :return: accuracy score on the dev set / Person's R in the case of regression\n",
        "        \"\"\"\n",
        "\n",
        "        model.fit(X_train, Y_train)\n",
        "        score = model.score(X_dev, Y_dev)\n",
        "        return score\n",
        "\n",
        "\n",
        "from typing import Dict\n",
        "\n",
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "\n",
        "# import svm_classifier\n",
        "\n",
        "REGRESSION = False\n",
        "from typing import List\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "\n",
        "def get_nullspace_projection(W: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    :param W: the matrix over its nullspace to project\n",
        "    :return: the projection matrix\n",
        "    \"\"\"\n",
        "    nullspace_basis = scipy.linalg.null_space(W)  # orthogonal basis\n",
        "    nullspace_basis = nullspace_basis * np.sign(nullspace_basis[0][0])  # handle sign ambiguity\n",
        "    projection_matrix = nullspace_basis.dot(nullspace_basis.T)\n",
        "\n",
        "    return projection_matrix\n",
        "\n",
        "from sklearn import svm"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:13:07.214285Z",
          "iopub.execute_input": "2024-09-20T20:13:07.214567Z",
          "iopub.status.idle": "2024-09-20T20:13:07.224946Z",
          "shell.execute_reply.started": "2024-09-20T20:13:07.214543Z",
          "shell.execute_reply": "2024-09-20T20:13:07.224175Z"
        },
        "trusted": true,
        "id": "0QJBIj3yXaqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#R\n",
        "num_concepts = 18\n",
        "BS = 8\n",
        "import torch\n",
        "from torch import nn\n",
        "from pytorch_revgrad import RevGrad\n",
        "class VB(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VB, self).__init__()\n",
        "        self.vb = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(768, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 2),\n",
        "        )\n",
        "\n",
        "\n",
        "        self.linear_relu_stack_latent = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(768, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 2),\n",
        "        )\n",
        "\n",
        "\n",
        "        self.linear_relu_stack_hidden = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(768, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 2),\n",
        "        )\n",
        "\n",
        "        #self.w = torch.nn.Parameter(torch.ones(11,1)/11)\n",
        "        self.w = torch.nn.Parameter(torch.ones(num_concepts,1))\n",
        "\n",
        "        self.project_latent = nn.Linear(768,2048)\n",
        "\n",
        "        #self.W_ij = [[nn.Linear(768,768) for i in range(64)] for j in range(17)]\n",
        "\n",
        "        self.W_ij = torch.nn.Parameter(torch.randn(18,64,28,28))\n",
        "\n",
        "        self.proj_conc = nn.Linear(768,28)\n",
        "        self.proj_embed = nn.Linear(768,28)\n",
        "\n",
        "\n",
        "\n",
        "        self.grad_rev1 = RevGrad()\n",
        "        self.grad_rev2 = RevGrad()\n",
        "\n",
        "        self.w1 = torch.nn.Parameter(torch.tensor(0.2))\n",
        "\n",
        "\n",
        "    def forward(self,\n",
        "        all_concepts_,\n",
        "        input_ids = None,\n",
        "        attention_mask = None,\n",
        "        token_type_ids = None,\n",
        "        position_ids = None,\n",
        "        head_mask = None,\n",
        "        inputs_embeds = None,\n",
        "        visual_embeds = None,\n",
        "        visual_attention_mask = None,\n",
        "        visual_token_type_ids = None,\n",
        "        image_text_alignment = None,\n",
        "        output_attentions = None,\n",
        "        output_hidden_states = None,\n",
        "        return_dict = None):\n",
        "\n",
        "        #         print(all_concepts.shape)\n",
        "\n",
        "        B = visual_embeds.size(0)\n",
        "\n",
        "        #         print(B)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ##############################################\n",
        "\n",
        "        all_concepts = self.proj_conc(all_concepts_)\n",
        "\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs_embeds = self.vb.embeddings.word_embeddings(input_ids)\n",
        "\n",
        "        #print(0/0)\n",
        "\n",
        "        inputs_embeds = self.proj_embed(inputs_embeds)\n",
        "\n",
        "\n",
        "        #####################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #b1 = torch.bmm(inputs_embeds,all_concepts.repeat(B,1,1).transpose(1,2))\n",
        "\n",
        "        #print(inputs_embeds.shape)\n",
        "        #print(all_concepts.repeat(B,1,1).transpose(1,2).shape)\n",
        "\n",
        "        #b,64,28\n",
        "        #b,28,17\n",
        "\n",
        "\n",
        "        b = torch.einsum('blx,bxn->bln',inputs_embeds,all_concepts.repeat(B,1,1).transpose(1,2))\n",
        "\n",
        "        #         print(b1)\n",
        "\n",
        "        #         print('********')\n",
        "\n",
        "        #         print(b2)\n",
        "\n",
        "        #         print(torch.equal(b1,b2))\n",
        "\n",
        "\n",
        "\n",
        "        b = b.masked_fill(~attention_mask.unsqueeze(dim=2).bool(), -np.inf)\n",
        "        c = F.softmax(b,dim=1)\n",
        "\n",
        "\n",
        "\n",
        "        #inter = torch.einsum('bcsn,csnn->bcsn',inputs_embeds.unsqueeze(1).repeat(1,17,1,1), self.W_ij)\n",
        "\n",
        "        #print(inter[:,0,0,:])\n",
        "\n",
        "        einsum_expression = 'bln,clnx->bclx'\n",
        "        inter = torch.einsum(einsum_expression, inputs_embeds, self.W_ij)\n",
        "\n",
        "        k = torch.einsum('bcsn,bsc->bcn',inter,c)\n",
        "        norm = torch.norm(k,dim=2)\n",
        "\n",
        "\n",
        "\n",
        "        kk1 = ((norm**2)/(1+norm**2)).unsqueeze(dim=2) * (k/norm.unsqueeze(dim=2))\n",
        "\n",
        "        w_dynamic = kk1.norm(dim=2).unsqueeze(2)\n",
        "\n",
        "\n",
        "\n",
        "        raw_latent = (1-self.w1) * all_concepts_.unsqueeze(0).repeat(B,1,1) * self.w.unsqueeze(dim=0).repeat(B,1,1) + all_concepts_.unsqueeze(0).repeat(B,1,1) * w_dynamic * self.w1\n",
        "\n",
        "\n",
        "        #raw_latent = 0 * all_concepts_.unsqueeze(0).repeat(B,1,1) * self.w.unsqueeze(dim=0).repeat(B,1,1) + all_concepts_.unsqueeze(0).repeat(B,1,1) * w_dynamic *1\n",
        "\n",
        "\n",
        "        raw_latent = torch.mean(raw_latent, dim=1)\n",
        "\n",
        "        #raw_latent = torch.mean(all_concepts*self.w,dim=0)\n",
        "        #raw_latent = raw_latent.unsqueeze(dim=0)\n",
        "\n",
        "        latent = self.project_latent(raw_latent)\n",
        "\n",
        "\n",
        "\n",
        "        latent = latent.unsqueeze(dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        visual_embeds_latent = torch.cat((visual_embeds,latent),dim=1)\n",
        "\n",
        "\n",
        "        ################################################################\n",
        "\n",
        "\n",
        "        #print(b)\n",
        "\n",
        "        #######################################################\n",
        "        #         print(b.shape)\n",
        "\n",
        "        #         b = []\n",
        "        #         for i in range(64):\n",
        "        #             bi = []\n",
        "        #             for j in range(17):\n",
        "        #                 #print(inputs_embeds[:,i,:].shape)\n",
        "        #                 ci = all_concepts[j].unsqueeze(0).repeat(32,1,1).squeeze()\n",
        "        #                 uj = inputs_embeds[:,i,:]\n",
        "        #                 #print(ci.shape)\n",
        "        #                 bij = torch.bmm(uj.unsqueeze(dim=1), ci.unsqueeze(dim=2))\n",
        "        #                 #print(bij.shape)\n",
        "        #                 #print(bij)\n",
        "        #                 bi.append(bij.squeeze())\n",
        "        #             bi = torch.stack(bi)\n",
        "\n",
        "        #             bi = bi.T\n",
        "        #             #print(bi.shape)\n",
        "        #             #print(bi)\n",
        "        #             b.append(bi)\n",
        "\n",
        "\n",
        "\n",
        "        #         b = torch.stack(b)\n",
        "        #         b = b.transpose(0,1)\n",
        "\n",
        "        #         print(b)\n",
        "        #         print(b.shape)\n",
        "\n",
        "\n",
        "        #print(0/0)\n",
        "\n",
        "        ####################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #c = F.softmax(b,dim=1)\n",
        "\n",
        "        #print(c)\n",
        "        #print(c.shape)\n",
        "        #print(0/0)\n",
        "        ############################################################\n",
        "\n",
        "        #print\n",
        "\n",
        "        #c = 32*64*17\n",
        "\n",
        "        #u_j_i = 32*768\n",
        "\n",
        "\n",
        "        #cij = 32*1\n",
        "\n",
        "\n",
        "        #         kk = []\n",
        "        #         for i in range(17):\n",
        "        #             si = 0\n",
        "        #             for j in range(64):\n",
        "        #                 #print(inputs_embeds[:,j,:].shape)\n",
        "        #                 #print(self.W_ij)\n",
        "        #                 #print(self.W_ij[i,j,:,:].shape)\n",
        "\n",
        "\n",
        "\n",
        "        #                 u_j_i = torch.mm(inputs_embeds[:,j,:],self.W_ij[i,j,:,:])\n",
        "\n",
        "\n",
        "        #                 #print(j,i,u_j_i)\n",
        "        #                 #print(0/0)\n",
        "\n",
        "        #                 cij = c[:,j,i].unsqueeze(dim=1)\n",
        "        #                 #print('cij ',cij)\n",
        "        #                 #print(cij.shape)\n",
        "        #                 #print(u_j_i)\n",
        "        #                 #print(u_j_i.shape)\n",
        "        #                 si+= u_j_i*cij\n",
        "\n",
        "        #                 #print(u_j_i*cij)\n",
        "        #             #print(si)\n",
        "        #             #print(0/0)\n",
        "\n",
        "        #             norm = torch.norm(si, dim=1)\n",
        "\n",
        "        #             #print(norm)\n",
        "        #             #print(norm.shape)\n",
        "\n",
        "        #             vi = ((norm**2)/(1+norm**2)).unsqueeze(dim=1) * (si/norm.unsqueeze(dim=1))\n",
        "\n",
        "        #             #print(vi)\n",
        "        #             kk.append(torch.norm(vi,dim=1))\n",
        "        #             #kk.append(vi)\n",
        "        #             #print(kk)\n",
        "\n",
        "\n",
        "        #         kk = torch.stack(kk)\n",
        "\n",
        "        #         print('kk', kk)\n",
        "        #         print('kk1', kk1)\n",
        "        #         print(kk.shape)\n",
        "        #         print(kk1.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Create a tensor of ones with the same number of columns as the original tensor\n",
        "        ones_column_vam = torch.ones((visual_attention_mask.size(0), 1), dtype=visual_attention_mask.dtype)\n",
        "\n",
        "        # Concatenate the original tensor with the tensor of ones along the second dimension (columns)\n",
        "        visual_attention_mask_latent = torch.cat((visual_attention_mask, ones_column_vam.to('cuda')), dim=1)\n",
        "\n",
        "        # Create a tensor of ones with the same number of columns as the original tensor\n",
        "        ones_column_vtt = torch.ones((visual_token_type_ids.size(0), 1), dtype=visual_token_type_ids.dtype)\n",
        "\n",
        "        # Concatenate the original tensor with the tensor of ones along the second dimension (columns)\n",
        "        visual_token_type_ids_latent = torch.cat((visual_token_type_ids, ones_column_vtt.to('cuda')), dim=1)\n",
        "\n",
        "        #         print(latent.shape)\n",
        "\n",
        "        #         print(visual_embeds.shape)\n",
        "        #         print(visual_attention_mask.shape)\n",
        "        #         print(visual_token_type_ids.shape)\n",
        "\n",
        "        #         print(self.w)\n",
        "\n",
        "        x = self.vb(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask, visual_embeds=visual_embeds_latent, visual_attention_mask=visual_attention_mask_latent, visual_token_type_ids=visual_token_type_ids_latent)\n",
        "\n",
        "        p = self.vb(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask, visual_embeds=visual_embeds, visual_attention_mask=visual_attention_mask, visual_token_type_ids=visual_token_type_ids)\n",
        "        p = self.grad_rev1(p.pooler_output)\n",
        "        #q = self.grad_rev2(raw_latent.repeat(B,1))\n",
        "        q = self.grad_rev2(raw_latent)\n",
        "\n",
        "        logits_p = self.linear_relu_stack_hidden(p)\n",
        "        logits_q = self.linear_relu_stack_latent(q)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        logits = self.linear_relu_stack(x.pooler_output)\n",
        "\n",
        "\n",
        "        if_logits_p = self.linear_relu_stack(p)\n",
        "        if_logits_q = self.linear_relu_stack(q)\n",
        "\n",
        "        return logits, logits_p, logits_q, if_logits_p, if_logits_q\n",
        "        #return logits, logits_p, logits_q, if_logits_p, if_logits_p\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:13:07.354281Z",
          "iopub.execute_input": "2024-09-20T20:13:07.354552Z",
          "iopub.status.idle": "2024-09-20T20:13:07.383238Z",
          "shell.execute_reply.started": "2024-09-20T20:13:07.354531Z",
          "shell.execute_reply": "2024-09-20T20:13:07.382330Z"
        },
        "trusted": true,
        "id": "fzPmaGX_XaqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "def get_inlp(all_concepts):\n",
        "    print('INLP step')\n",
        "    num_concepts = 18\n",
        "    all_concepts_ = all_concepts.numpy()\n",
        "    multiplier = np.logical_xor(1, np.eye(num_concepts)).astype(np.int32)/ (num_concepts-1)\n",
        "    X = np.matmul(multiplier,all_concepts_)\n",
        "    assert X.shape[0]==num_concepts\n",
        "    Y = np.arange(num_concepts)\n",
        "\n",
        "    #             x_train_cp = x_train\n",
        "    #             x_test_cp = x_test\n",
        "    x_train_cp = X\n",
        "    x_test_cp = X\n",
        "    y_train = Y\n",
        "    y_test = Y\n",
        "    print('******')\n",
        "    for i in range(1):\n",
        "\n",
        "        clf = svm.SVC(kernel=\"linear\")\n",
        "        scr = train_network(clf, x_train_cp, y_train, x_test_cp, y_test)\n",
        "        print('initial {}'.format(scr))\n",
        "        W = get_weights(clf)\n",
        "        P_i = get_nullspace_projection(W)\n",
        "        x_train_cp = x_train_cp.dot(P_i)\n",
        "        x_test_cp = x_test_cp.dot(P_i)\n",
        "        print(clf.score(x_test_cp, y_test))\n",
        "        print(clf.score(x_train_cp, y_train))\n",
        "        clf = svm.SVC(kernel=\"linear\")\n",
        "        train_network(clf, x_train_cp, y_train, x_test_cp, y_test)\n",
        "    print('******')\n",
        "    all_concepts_ = torch.tensor(all_concepts_)\n",
        "    all_concepts_ = all_concepts_.mm(torch.tensor(P_i, dtype = all_concepts_.dtype))\n",
        "\n",
        "    return all_concepts_"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:13:11.303068Z",
          "iopub.execute_input": "2024-09-20T20:13:11.303939Z",
          "iopub.status.idle": "2024-09-20T20:13:11.313310Z",
          "shell.execute_reply.started": "2024-09-20T20:13:11.303899Z",
          "shell.execute_reply": "2024-09-20T20:13:11.312297Z"
        },
        "trusted": true,
        "id": "uyaha6MnXaqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "import torch\n",
        "from torch import nn\n",
        "from pytorch_revgrad import RevGrad\n",
        "class VB_wrapper(nn.Module):\n",
        "    def __init__(self, vb):\n",
        "        super(VB_wrapper, self).__init__()\n",
        "\n",
        "        self.vb = vb\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,\n",
        "        all_concepts_,\n",
        "        input_embeds = None,\n",
        "        visual_embeds = None,\n",
        "        attention_mask = None,\n",
        "        token_type_ids = None,\n",
        "        position_ids = None,\n",
        "        head_mask = None,\n",
        "        inputs_embeds = None,\n",
        "\n",
        "        visual_attention_mask = None,\n",
        "        visual_token_type_ids = None,\n",
        "        image_text_alignment = None,\n",
        "        output_attentions = None,\n",
        "        output_hidden_states = None,\n",
        "        return_dict = None):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #         print('ac', all_concepts_.shape)\n",
        "        #         print('ii', input_embeds.shape)\n",
        "        #         print('am', attention_mask)\n",
        "        #         print('tti', token_type_ids)\n",
        "        #         print('pi', position_ids)\n",
        "        #         print('hm', head_mask)\n",
        "        #         print('ie', inputs_embeds)\n",
        "        #         print('ve', visual_embeds)\n",
        "        #         print('vam', visual_attention_mask)\n",
        "        #         print('vtti', visual_token_type_ids)\n",
        "        #         print(image_text_alignment)\n",
        "        #         print(output_attentions)\n",
        "        #         print(output_hidden_states)\n",
        "        #         print(return_dict)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        B = visual_embeds.size(0)\n",
        "\n",
        "\n",
        "        all_concepts = self.vb.proj_conc(all_concepts_)\n",
        "\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            inputs_embeds = self.vb.vb.embeddings.word_embeddings(input_ids.to('cuda'))\n",
        "        \"\"\"\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs_embeds = self.vb.vb.embeddings.word_embeddings(input_ids.to('cuda'))\n",
        "\n",
        "        #print(0/0)\n",
        "\n",
        "        inputs_embeds = self.vb.proj_embed(inputs_embeds)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        b = torch.einsum('blx,bxn->bln',inputs_embeds,all_concepts.transpose(1,2))\n",
        "\n",
        "\n",
        "        #print(b.shape)\n",
        "\n",
        "\n",
        "\n",
        "        b = b.masked_fill(~attention_mask.unsqueeze(dim=2).bool(), -np.inf)\n",
        "        c = F.softmax(b,dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        einsum_expression = 'bln,clnx->bclx'\n",
        "        inter = torch.einsum(einsum_expression, inputs_embeds, self.vb.W_ij)\n",
        "\n",
        "        k = torch.einsum('bcsn,bsc->bcn',inter,c)\n",
        "        norm = torch.norm(k,dim=2)\n",
        "\n",
        "\n",
        "\n",
        "        kk1 = ((norm**2)/(1+norm**2)).unsqueeze(dim=2) * (k/norm.unsqueeze(dim=2))\n",
        "\n",
        "        w_dynamic = kk1.norm(dim=2).unsqueeze(2)\n",
        "\n",
        "        num_reps = int(all_concepts.shape[0])\n",
        "        w_static = self.vb.w.unsqueeze(0).repeat(num_reps,1,1)\n",
        "\n",
        "\n",
        "        #         print('*'*10)\n",
        "\n",
        "        #         print(self.vb.w1.shape)\n",
        "        #         print(all_concepts_.shape)\n",
        "        #         print(w_static.shape)\n",
        "        #         print(w_dynamic.shape)\n",
        "\n",
        "\n",
        "\n",
        "        if DYN_ROUTE:\n",
        "            raw_latent = (1-self.vb.w1) * all_concepts_ * w_static + all_concepts_ * w_dynamic * self.vb.w1\n",
        "\n",
        "        else:\n",
        "            raw_latent = (1-self.vb.w1) * all_concepts_ * w_static\n",
        "\n",
        "\n",
        "\n",
        "        #raw_latent = (1-self.vb.w1) * all_concepts_ * w_static\n",
        "        raw_latent = torch.mean(raw_latent, dim=1)\n",
        "\n",
        "\n",
        "        latent = self.vb.project_latent(raw_latent)\n",
        "\n",
        "\n",
        "\n",
        "        latent = latent.unsqueeze(dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        visual_embeds_latent = torch.cat((visual_embeds,latent),dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #         num_reps = int(all_concepts.shape[0]/17)\n",
        "        #         w_static = self.vb.w.unsqueeze(0).repeat(num_reps,1,1).reshape(17*num_reps,1)\n",
        "        #         raw_latent = torch.mean(all_concepts*w_static,dim=0)\n",
        "        #         raw_latent = raw_latent.unsqueeze(dim=0)\n",
        "\n",
        "        #         latent = self.vb.project_latent(raw_latent)\n",
        "\n",
        "\n",
        "\n",
        "        #         latent = latent.repeat(B,1).unsqueeze(dim=1)\n",
        "\n",
        "\n",
        "        #         visual_embeds_latent = torch.cat((visual_embeds,latent),dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Create a tensor of ones with the same number of columns as the original tensor\n",
        "        ones_column_vam = torch.ones((visual_attention_mask.size(0), 1), dtype=visual_attention_mask.dtype)\n",
        "\n",
        "        # Concatenate the original tensor with the tensor of ones along the second dimension (columns)\n",
        "        visual_attention_mask_latent = torch.cat((visual_attention_mask, ones_column_vam.to('cuda')), dim=1)\n",
        "\n",
        "        # Create a tensor of ones with the same number of columns as the original tensor\n",
        "        ones_column_vtt = torch.ones((visual_token_type_ids.size(0), 1), dtype=visual_token_type_ids.dtype)\n",
        "\n",
        "        # Concatenate the original tensor with the tensor of ones along the second dimension (columns)\n",
        "        visual_token_type_ids_latent = torch.cat((visual_token_type_ids, ones_column_vtt.to('cuda')), dim=1)\n",
        "\n",
        "        #         print(latent.shape)\n",
        "\n",
        "        #         print(visual_embeds.shape)\n",
        "        #         print(visual_attention_mask.shape)\n",
        "        #         print(visual_token_type_ids.shape)\n",
        "\n",
        "        #         print(self.w)\n",
        "\n",
        "        x = self.vb.vb(inputs_embeds = input_embeds, token_type_ids = token_type_ids, attention_mask = attention_mask, visual_embeds=visual_embeds_latent, visual_attention_mask=visual_attention_mask_latent, visual_token_type_ids=visual_token_type_ids_latent)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        logits = self.vb.linear_relu_stack(x.pooler_output)\n",
        "        #print('softmax logit', torch.softmax(logits, dim=1))\n",
        "        #print(0/0)\n",
        "\n",
        "        #logits = torch.softmax(logits, dim=1)[:, 1].unsqueeze(1)\n",
        "\n",
        "        #print('previous',torch.softmax(logits, dim=1)[:, 1].unsqueeze(1) )\n",
        "\n",
        "        logits = torch.softmax(logits, dim=1).max(dim=1).values.unsqueeze(1)\n",
        "\n",
        "        #print('logits', logits)\n",
        "\n",
        "        #print(0/0)\n",
        "\n",
        "        #print(logits)\n",
        "        #print(logits.shape)\n",
        "\n",
        "\n",
        "\n",
        "        return logits\n",
        "\n",
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:13:13.373600Z",
          "iopub.execute_input": "2024-09-20T20:13:13.374104Z",
          "iopub.status.idle": "2024-09-20T20:13:13.394856Z",
          "shell.execute_reply.started": "2024-09-20T20:13:13.374071Z",
          "shell.execute_reply": "2024-09-20T20:13:13.393887Z"
        },
        "trusted": true,
        "id": "Q_zfEYwXXaqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "vb = VB()\n",
        "\n",
        "vb.load_state_dict(torch.load('/kaggle/working/final_model_concept.pt'))\n",
        "vb.to('cuda')\n",
        "vb.eval()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:13:15.550926Z",
          "iopub.execute_input": "2024-09-20T20:13:15.551788Z",
          "iopub.status.idle": "2024-09-20T20:13:16.636075Z",
          "shell.execute_reply.started": "2024-09-20T20:13:15.551750Z",
          "shell.execute_reply": "2024-09-20T20:13:16.635123Z"
        },
        "trusted": true,
        "id": "cqjqJeWlXaqW",
        "outputId": "ca61de04-dce0-44c3-fa33-b8e7028a9fa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 51,
          "output_type": "execute_result",
          "data": {
            "text/plain": "VB(\n  (vb): VisualBertModel(\n    (embeddings): VisualBertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=1)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n      (visual_token_type_embeddings): Embedding(2, 768)\n      (visual_position_embeddings): Embedding(512, 768)\n      (visual_projection): Linear(in_features=2048, out_features=768, bias=True)\n    )\n    (encoder): VisualBertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x VisualBertLayer(\n          (attention): VisualBertAttention(\n            (self): VisualBertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): VisualBertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): VisualBertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): VisualBertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): VisualBertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (linear_relu_stack): Sequential(\n    (0): Dropout(p=0.1, inplace=False)\n    (1): Linear(in_features=768, out_features=512, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=512, out_features=128, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=128, out_features=2, bias=True)\n  )\n  (linear_relu_stack_latent): Sequential(\n    (0): Dropout(p=0.1, inplace=False)\n    (1): Linear(in_features=768, out_features=512, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=512, out_features=128, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=128, out_features=2, bias=True)\n  )\n  (linear_relu_stack_hidden): Sequential(\n    (0): Dropout(p=0.1, inplace=False)\n    (1): Linear(in_features=768, out_features=512, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=512, out_features=128, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=128, out_features=2, bias=True)\n  )\n  (project_latent): Linear(in_features=768, out_features=2048, bias=True)\n  (proj_conc): Linear(in_features=768, out_features=28, bias=True)\n  (proj_embed): Linear(in_features=768, out_features=28, bias=True)\n  (grad_rev1): RevGrad()\n  (grad_rev2): RevGrad()\n)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "vb_c = VB()\n",
        "vb_c.to(device)\n",
        "# concept_classes = [\"jew\", \"holocaust\", \"hitler\", \"genocide\", \"funny\", \"muslim\", \"terrorism\", \"violence\", \"politics\", \"naive\", \"international relation\"]\n",
        "\n",
        "\n",
        "# concept_embedding_map = {}\n",
        "\n",
        "# all_concepts = []\n",
        "\n",
        "\n",
        "# for c in concept_classes:\n",
        "#     conc = torch.tensor(tokenizer.encode(c)[1:-1])\n",
        "#     e = vb_c.vb.embeddings.word_embeddings(conc.to('cuda'))\n",
        "#     x = e.detach().cpu().mean(dim=0)\n",
        "\n",
        "#     all_concepts.append(x)\n",
        "\n",
        "#     concept_embedding_map[c] = x\n",
        "\n",
        "\n",
        "# all_concepts = torch.stack(all_concepts)\n",
        "\n",
        "# concept_classes = [\"jew\", \"holocaust\", \"hitler\", \"genocide\", \"funny\", \"muslim\", \"terrorism\", \"violence\", \"politics\", \"racism\", \"international relation\", \"sex\", \"gore\", \"violence\", \"immigration\", \"extremism\", \"immoral\"]\n",
        "\n",
        "concept_classes = [\"holocaust\", \"nazism\", \"genocide\", \"funny\", \"anti muslim\", \"terrorism\", \"violence\", \"politics\", \"racism\", \"international relation\", \"adult\", \"gore\", \"misogynistic\", \"immigration\", \"extremism\", \"immoral\", \"white supremacy\", \"indecency\"]\n",
        "\n",
        "\n",
        "concept_embedding_map = {}\n",
        "\n",
        "all_concepts = []\n",
        "\n",
        "\n",
        "for c in concept_classes:\n",
        "    conc = torch.tensor(tokenizer.encode(c)[1:-1])\n",
        "    e = vb_c.vb.embeddings.word_embeddings(conc.to('cuda'))\n",
        "    x = e.detach().cpu().mean(dim=0)\n",
        "\n",
        "\n",
        "    all_concepts.append(x)\n",
        "\n",
        "    concept_embedding_map[c] = x\n",
        "\n",
        "\n",
        "all_concepts = torch.stack(all_concepts)\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:13:18.455176Z",
          "iopub.execute_input": "2024-09-20T20:13:18.456092Z",
          "iopub.status.idle": "2024-09-20T20:13:19.157868Z",
          "shell.execute_reply.started": "2024-09-20T20:13:18.456061Z",
          "shell.execute_reply": "2024-09-20T20:13:19.157022Z"
        },
        "trusted": true,
        "id": "JSnz-AxAXaqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "def get_inlp(all_concepts):\n",
        "\n",
        "    num_concepts = 18\n",
        "    all_concepts_ = all_concepts.cpu().numpy()\n",
        "    multiplier = np.logical_xor(1, np.eye(num_concepts)).astype(np.int32)/ (num_concepts-1)\n",
        "    X = np.matmul(multiplier,all_concepts_)\n",
        "    assert X.shape[0]==num_concepts\n",
        "    Y = np.arange(num_concepts)\n",
        "\n",
        "    #             x_train_cp = x_train\n",
        "    #             x_test_cp = x_test\n",
        "    x_train_cp = X\n",
        "    x_test_cp = X\n",
        "    y_train = Y\n",
        "    y_test = Y\n",
        "\n",
        "    for i in range(1):\n",
        "\n",
        "        clf = svm.SVC(kernel=\"linear\")\n",
        "        scr = train_network(clf, x_train_cp, y_train, x_test_cp, y_test)\n",
        "\n",
        "        W = get_weights(clf)\n",
        "        P_i = get_nullspace_projection(W)\n",
        "        x_train_cp = x_train_cp.dot(P_i)\n",
        "        x_test_cp = x_test_cp.dot(P_i)\n",
        "\n",
        "        clf = svm.SVC(kernel=\"linear\")\n",
        "        train_network(clf, x_train_cp, y_train, x_test_cp, y_test)\n",
        "\n",
        "    all_concepts_ = torch.tensor(all_concepts_)\n",
        "    all_concepts_ = all_concepts_.mm(torch.tensor(P_i, dtype = all_concepts_.dtype))\n",
        "\n",
        "    return all_concepts_"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:13:20.504461Z",
          "iopub.execute_input": "2024-09-20T20:13:20.504844Z",
          "iopub.status.idle": "2024-09-20T20:13:20.513670Z",
          "shell.execute_reply.started": "2024-09-20T20:13:20.504814Z",
          "shell.execute_reply": "2024-09-20T20:13:20.512775Z"
        },
        "trusted": true,
        "id": "2lvoBiAXXaqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "#torch.save(vb.state_dict(), '/kaggle/working/final_model_concept_inlp.pt')\n",
        "# check evaluation performance separately on p and q using their classifier: should be less\n",
        "pred_r1, pred_r2 = [],[]\n",
        "pred_both = []\n",
        "act = []\n",
        "if DECONFOUND:\n",
        "    all_concepts_ = get_inlp(all_concepts).to('cuda')\n",
        "else:\n",
        "    all_concepts_ = all_concepts.to('cuda')\n",
        "\n",
        "for ii, batch in enumerate(eval_dataloader):\n",
        "    with torch.no_grad():\n",
        "        batch_inp = {k: v.to(device) for k, v in batch[0].items()}\n",
        "        outputs, r1, r2,_,_ = vb(all_concepts_.to('cuda'),**batch_inp)\n",
        "        act.append(batch[1].detach().cpu().numpy()[0])\n",
        "\n",
        "        pred_r1.append(np.argmax(r1.detach().cpu().numpy()[0]))\n",
        "        pred_r2.append(np.argmax(r2.detach().cpu().numpy()[0]))\n",
        "        pred_both.append(np.argmax(outputs.detach().cpu().numpy()[0]))\n",
        "\n",
        "print('*')\n",
        "print(f1_score(act,pred_r1,average='macro'))\n",
        "print(f1_score(act,pred_r1,average='weighted'))\n",
        "print(accuracy_score(act,pred_r1))\n",
        "print('*'*10)\n",
        "print(f1_score(act,pred_r2,average='macro'))\n",
        "print(f1_score(act,pred_r2,average='weighted'))\n",
        "print(accuracy_score(act,pred_r2))\n",
        "print('*'*10)\n",
        "print(f1_score(act,pred_both,average='macro'))\n",
        "print(f1_score(act,pred_both,average='weighted'))\n",
        "print(accuracy_score(act,pred_both))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:13:21.903369Z",
          "iopub.execute_input": "2024-09-20T20:13:21.904205Z",
          "iopub.status.idle": "2024-09-20T20:13:37.434181Z",
          "shell.execute_reply.started": "2024-09-20T20:13:21.904175Z",
          "shell.execute_reply": "2024-09-20T20:13:37.433113Z"
        },
        "trusted": true,
        "id": "n2toWYR1XaqY",
        "outputId": "3c6bf25f-a947-4673-b192-aa3e41c23eb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "*\n0.40647482014388486\n0.5567473294091998\n0.6848484848484848\n**********\n0.40647482014388486\n0.5567473294091998\n0.6848484848484848\n**********\n0.7036565354754966\n0.7469814601650707\n0.75\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('h')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:13:37.436045Z",
          "iopub.execute_input": "2024-09-20T20:13:37.436376Z",
          "iopub.status.idle": "2024-09-20T20:13:37.441195Z",
          "shell.execute_reply.started": "2024-09-20T20:13:37.436349Z",
          "shell.execute_reply": "2024-09-20T20:13:37.440224Z"
        },
        "trusted": true,
        "id": "JhvUsrnMXaqZ",
        "outputId": "c268437f-9e0d-451c-f0a7-79d5d7ef8af1"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "h\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "def model_pred(input_ids, attention_mask, token_type_ids, visual_embeds, visual_attention_mask, visual_token_type_ids, all_concepts, concept_no=None):\n",
        "\n",
        "\n",
        "\n",
        "    B = visual_embeds.size(0)\n",
        "\n",
        "\n",
        "    #######################################################\n",
        "    input_ids = input_ids.to('cuda')\n",
        "    attention_mask = attention_mask.to('cuda')\n",
        "    token_type_ids = token_type_ids.to('cuda')\n",
        "    visual_embeds = visual_embeds.to('cuda')\n",
        "    visual_attention_mask = visual_attention_mask.to('cuda')\n",
        "    visual_token_type_ids = visual_token_type_ids.to('cuda')\n",
        "\n",
        "    #####################################################################\n",
        "\n",
        "    ##############################################\n",
        "\n",
        "    all_concepts = all_concepts.to('cuda')\n",
        "\n",
        "    # deconfounding step by passing the concepts through INLP (1)\n",
        "    if DECONFOUND:\n",
        "        all_concepts_ = get_inlp(all_concepts).to('cuda')\n",
        "    else:\n",
        "        all_concepts_ = all_concepts.to('cuda')\n",
        "\n",
        "    # this is without deconfounding while testing (2)\n",
        "    # either use (1) or (2) or mean of (1) and (2)\n",
        "\n",
        "    # all_concepts_ = all_concepts.to('cuda')\n",
        "\n",
        "    #all_concepts_ = (get_inlp(all_concepts).to('cuda') + all_concepts.to('cuda')) / 2\n",
        "\n",
        "    all_concepts = vb.proj_conc(all_concepts_)\n",
        "\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        inputs_embeds = vb.vb.embeddings.word_embeddings(input_ids.to('cuda'))\n",
        "\n",
        "    #print(0/0)\n",
        "\n",
        "    inputs_embeds = vb.proj_embed(inputs_embeds)\n",
        "\n",
        "\n",
        "    #####################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #B, 64, 28, #B, 28, 17\n",
        "\n",
        "    b = torch.einsum('blx,bxn->bln',inputs_embeds,all_concepts.repeat(B,1,1).transpose(1,2))\n",
        "\n",
        "\n",
        "\n",
        "    b = b.masked_fill(~attention_mask.unsqueeze(dim=2).bool(), -np.inf)\n",
        "    c = F.softmax(b,dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    einsum_expression = 'bln,clnx->bclx'\n",
        "    inter = torch.einsum(einsum_expression, inputs_embeds, vb.W_ij)\n",
        "\n",
        "    k = torch.einsum('bcsn,bsc->bcn',inter,c)\n",
        "    norm = torch.norm(k,dim=2)\n",
        "\n",
        "\n",
        "\n",
        "    kk1 = ((norm**2)/(1+norm**2)).unsqueeze(dim=2) * (k/norm.unsqueeze(dim=2))\n",
        "\n",
        "    w_dynamic = kk1.norm(dim=2).unsqueeze(2)\n",
        "\n",
        "    #     print(w_dynamic.shape)\n",
        "    #     print(vb.w.shape)\n",
        "\n",
        "\n",
        "    #     print(w_dynamic)\n",
        "    #     print(vb.w)\n",
        "    #     print('*'*10)\n",
        "\n",
        "\n",
        "    #print(concept_no)\n",
        "    if concept_no==None:\n",
        "        k_static = vb.w.data.clone()\n",
        "        k_dynamic = w_dynamic.data.clone()\n",
        "    else:\n",
        "        k_static = vb.w.data.clone()\n",
        "        k_dynamic = w_dynamic.data.clone()\n",
        "        k_static[concept_no][0] = 0\n",
        "        k_dynamic[0][concept_no][0] = 0\n",
        "\n",
        "    #     print(k_dynamic)\n",
        "    #     print(k_static)\n",
        "    #     print('*'*10)\n",
        "\n",
        "    #     print(0/0)\n",
        "\n",
        "    #either use the following line or raw_latent = (1-vb.w1) * all_concepts_.unsqueeze(0).repeat(B,1,1) * k_static.unsqueeze(dim=0).repeat(B,1,1)\n",
        "\n",
        "    if DYN_ROUTE:\n",
        "        raw_latent = (1-vb.w1) * all_concepts_.unsqueeze(0).repeat(B,1,1) * k_static.unsqueeze(dim=0).repeat(B,1,1) + all_concepts_.unsqueeze(0).repeat(B,1,1) * k_dynamic * vb.w1\n",
        "    else:\n",
        "        raw_latent = (1-vb.w1) * all_concepts_.unsqueeze(0).repeat(B,1,1) * k_static.unsqueeze(dim=0).repeat(B,1,1)\n",
        "\n",
        "    #raw_latent = 0.0 * all_concepts_.unsqueeze(0).repeat(B,1,1) * k_static.unsqueeze(dim=0).repeat(B,1,1) + all_concepts_.unsqueeze(0).repeat(B,1,1) * k_dynamic * 1.0\n",
        "\n",
        "    #raw_latent = (1-vb.w1) * all_concepts_.unsqueeze(0).repeat(B,1,1) * k_static.unsqueeze(dim=0).repeat(B,1,1)\n",
        "\n",
        "    raw_latent = torch.mean(raw_latent, dim=1)\n",
        "\n",
        "\n",
        "\n",
        "    latent = vb.project_latent(raw_latent)\n",
        "\n",
        "\n",
        "\n",
        "    latent = latent.unsqueeze(dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    visual_embeds_latent = torch.cat((visual_embeds,latent),dim=1)\n",
        "\n",
        "\n",
        "    ################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Create a tensor of ones with the same number of columns as the original tensor\n",
        "    ones_column_vam = torch.ones((visual_attention_mask.size(0), 1), dtype=visual_attention_mask.dtype)\n",
        "\n",
        "    # Concatenate the original tensor with the tensor of ones along the second dimension (columns)\n",
        "    visual_attention_mask_latent = torch.cat((visual_attention_mask.to('cuda'), ones_column_vam.to('cuda')), dim=1)\n",
        "\n",
        "    # Create a tensor of ones with the same number of columns as the original tensor\n",
        "    ones_column_vtt = torch.ones((visual_token_type_ids.size(0), 1), dtype=visual_token_type_ids.dtype)\n",
        "\n",
        "    # Concatenate the original tensor with the tensor of ones along the second dimension (columns)\n",
        "    visual_token_type_ids_latent = torch.cat((visual_token_type_ids.to('cuda'), ones_column_vtt.to('cuda')), dim=1)\n",
        "\n",
        "\n",
        "\n",
        "    x = vb.vb(input_ids = input_ids.to('cuda'), token_type_ids = token_type_ids.to('cuda'), attention_mask = attention_mask.to('cuda'), visual_embeds=visual_embeds_latent.to('cuda'), visual_attention_mask=visual_attention_mask_latent.to('cuda'), visual_token_type_ids=visual_token_type_ids_latent.to('cuda'))\n",
        "\n",
        "    p = vb.vb(input_ids = input_ids.to('cuda'), token_type_ids = token_type_ids.to('cuda'), attention_mask = attention_mask.to('cuda'), visual_embeds=visual_embeds.to('cuda'), visual_attention_mask=visual_attention_mask.to('cuda'), visual_token_type_ids=visual_token_type_ids.to('cuda'))\n",
        "    p = vb.grad_rev1(p.pooler_output)\n",
        "    q = vb.grad_rev2(raw_latent.repeat(B,1))\n",
        "\n",
        "    logits_p = vb.linear_relu_stack_hidden(p)\n",
        "    logits_q = vb.linear_relu_stack_latent(q.to('cuda'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    logits = vb.linear_relu_stack(x.pooler_output)\n",
        "\n",
        "\n",
        "    if_logits_p = vb.linear_relu_stack(p)\n",
        "    if_logits_q = vb.linear_relu_stack(q.to('cuda'))\n",
        "\n",
        "    return torch.softmax(logits,dim=-1), x.pooler_output"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:13:37.442354Z",
          "iopub.execute_input": "2024-09-20T20:13:37.442775Z",
          "iopub.status.idle": "2024-09-20T20:13:37.465972Z",
          "shell.execute_reply.started": "2024-09-20T20:13:37.442746Z",
          "shell.execute_reply": "2024-09-20T20:13:37.465033Z"
        },
        "trusted": true,
        "id": "24JJUJVAXaqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:13:37.468186Z",
          "iopub.execute_input": "2024-09-20T20:13:37.468538Z",
          "iopub.status.idle": "2024-09-20T20:13:37.480536Z",
          "shell.execute_reply.started": "2024-09-20T20:13:37.468505Z",
          "shell.execute_reply": "2024-09-20T20:13:37.479725Z"
        },
        "trusted": true,
        "id": "-QH2NhM2XaqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(50):\n",
        "    if train_set[i]['label']==1:\n",
        "        print(i, train_set[i])\n",
        "        Image.open('/kaggle/input/facebook-hateful-meme-dataset/data/'+train_set[i]['img'])\n",
        "        inputs = {}\n",
        "        inputs = tokenizer(train_set[i]['text'], padding=\"max_length\", truncation=True, max_length=64, return_tensors='pt')\n",
        "\n",
        "\n",
        "        inputs['input_ids'] = inputs['input_ids']\n",
        "        inputs['token_type_ids'] = inputs['token_type_ids']\n",
        "        inputs['attention_mask'] = inputs['attention_mask']\n",
        "\n",
        "        input_ids = inputs['input_ids']\n",
        "        token_type_ids = inputs['token_type_ids']\n",
        "        attention_mask = inputs['attention_mask']\n",
        "\n",
        "        visual_embeds,visual_token_type_ids,visual_attention_mask = get_visual_embedding('/kaggle/input/facebook-hateful-meme-dataset/data/'+train_set[i]['img'])\n",
        "        # visual_embeds,visual_token_type_ids,visual_attention_mask = visual_feats[train_set[874]['img']]\n",
        "\n",
        "\n",
        "\n",
        "        inputs.update({\n",
        "          \"visual_embeds\":  visual_embeds,\n",
        "          \"visual_token_type_ids\": visual_token_type_ids,\n",
        "          \"visual_attention_mask\": visual_attention_mask\n",
        "        })\n",
        "\n",
        "\n",
        "        class_map = {0:'non offensive', 1: 'offensive'}\n",
        "        #print('original model prediction {}'.format(model_pred(input_ids,attention_mask,token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids)))\n",
        "        orig,_ = model_pred(input_ids,attention_mask,token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids,all_concepts)\n",
        "        orig = orig[0]\n",
        "        print(orig)\n",
        "        print('predicted class {}'.format(class_map[np.argmax(orig.detach().cpu().numpy())]))\n",
        "        pred_class = np.argmax(orig.detach().cpu().numpy())\n",
        "        orig = orig[pred_class]\n",
        "\n",
        "        k = []\n",
        "        for i in range(18):\n",
        "            #print('Removing concept: {}'.format(concept_classes[i]))\n",
        "\n",
        "            after_intervention,_ = model_pred(input_ids,attention_mask,token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids,all_concepts,concept_no=i)\n",
        "            #print('hello')\n",
        "            after_intervention = after_intervention[0][pred_class]\n",
        "            #print(after_intervention, orig)\n",
        "            #print(0/0)\n",
        "            k.append(abs(after_intervention-orig).item())\n",
        "            #print('model prediction after removal {}'.format(model_pred(input_ids,attention_mask,token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids,concept_no=i)))\n",
        "        k = np.asarray(k)\n",
        "        print(k)\n",
        "        k = list(map(lambda x: ((x-np.mean(k))/np.mean(k))*100, k))\n",
        "\n",
        "        conc_scr = []\n",
        "        for i in range(17):\n",
        "            cc = concept_classes[i]\n",
        "            print('relative ICaCE scores for concept {} is {}%'.format(cc,k[i]))\n",
        "            conc_scr.append((cc,k[i]))\n",
        "        conc_scr.sort(key=lambda tup: tup[1], reverse=True)\n",
        "\n",
        "        for i in conc_scr:\n",
        "            if i[1]> 0:\n",
        "                print(i[0], end = \" \")\n",
        "        print(\"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:13:41.775153Z",
          "iopub.execute_input": "2024-09-20T20:13:41.775852Z",
          "iopub.status.idle": "2024-09-20T20:13:54.491630Z",
          "shell.execute_reply.started": "2024-09-20T20:13:41.775818Z",
          "shell.execute_reply": "2024-09-20T20:13:54.490433Z"
        },
        "trusted": true,
        "id": "Q5ytgx2hXaqa",
        "outputId": "478b22b1-bca8-4ed1-fd26-768d55858e04"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "10 {'text': 'jew mad? get fuhrerious!', 'img': 'img/79351.png', 'label': 1}\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:3526.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "tensor([0.2247, 0.7753], device='cuda:0', grad_fn=<SelectBackward0>)\npredicted class offensive\n[0.00029367 0.00029379 0.00029379 0.00029361 0.00029379 0.00029367\n 0.00029379 0.00029367 0.00029367 0.00029367 0.00029367 0.00029385\n 0.00029385 0.00029385 0.00029361 0.00029379 0.00029379 0.00029367]\nrelative ICaCE scores for concept holocaust is -0.021419311200047142%\nrelative ICaCE scores for concept nazism is 0.01916464686319619%\nrelative ICaCE scores for concept genocide is 0.01916464686319619%\nrelative ICaCE scores for concept funny is -0.04171129023166881%\nrelative ICaCE scores for concept anti muslim is 0.01916464686319619%\nrelative ICaCE scores for concept terrorism is -0.021419311200047142%\nrelative ICaCE scores for concept violence is 0.01916464686319619%\nrelative ICaCE scores for concept politics is -0.021419311200047142%\nrelative ICaCE scores for concept racism is -0.021419311200047142%\nrelative ICaCE scores for concept international relation is -0.021419311200047142%\nrelative ICaCE scores for concept adult is -0.021419311200047142%\nrelative ICaCE scores for concept gore is 0.039456625894817854%\nrelative ICaCE scores for concept misogynistic is 0.039456625894817854%\nrelative ICaCE scores for concept immigration is 0.039456625894817854%\nrelative ICaCE scores for concept extremism is -0.04171129023166881%\nrelative ICaCE scores for concept immoral is 0.01916464686319619%\nrelative ICaCE scores for concept white supremacy is 0.01916464686319619%\ngore misogynistic immigration nazism genocide anti muslim violence immoral white supremacy \n\n12 {'text': 'brother... a day without a blast is a day wasted', 'img': 'img/25489.png', 'label': 1}\ntensor([0.8936, 0.1064], device='cuda:0', grad_fn=<SelectBackward0>)\npredicted class non offensive\n[0.00020969 0.00020975 0.00020969 0.00020969 0.00020975 0.00020969\n 0.00020969 0.00020969 0.00020969 0.00020969 0.00020975 0.00020987\n 0.00020987 0.00020975 0.00020969 0.00020975 0.00020975 0.00020969]\nrelative ICaCE scores for concept holocaust is -0.018946570670704294%\nrelative ICaCE scores for concept nazism is 0.009473285335358609%\nrelative ICaCE scores for concept genocide is -0.018946570670704294%\nrelative ICaCE scores for concept funny is -0.018946570670704294%\nrelative ICaCE scores for concept anti muslim is 0.009473285335358609%\nrelative ICaCE scores for concept terrorism is -0.018946570670704294%\nrelative ICaCE scores for concept violence is -0.018946570670704294%\nrelative ICaCE scores for concept politics is -0.018946570670704294%\nrelative ICaCE scores for concept racism is -0.018946570670704294%\nrelative ICaCE scores for concept international relation is -0.018946570670704294%\nrelative ICaCE scores for concept adult is 0.009473285335358609%\nrelative ICaCE scores for concept gore is 0.06631299734748441%\nrelative ICaCE scores for concept misogynistic is 0.06631299734748441%\nrelative ICaCE scores for concept immigration is 0.009473285335358609%\nrelative ICaCE scores for concept extremism is -0.018946570670704294%\nrelative ICaCE scores for concept immoral is 0.009473285335358609%\nrelative ICaCE scores for concept white supremacy is 0.009473285335358609%\ngore misogynistic nazism anti muslim adult immigration immoral white supremacy \n\n27 {'text': \"is bribing muslims for liberal votes justin trudeau's only skill? why does justin trudeau love foreigners so much while openly disrespecting canadians, canadian values, our history and traditions, our seniors and veterans? sharia law has no place in canada! never has... never will.\", 'img': 'img/72640.png', 'label': 1}\ntensor([0.1694, 0.8306], device='cuda:0', grad_fn=<SelectBackward0>)\npredicted class offensive\n[5.96046448e-07 5.96046448e-07 5.96046448e-07 5.96046448e-07\n 6.55651093e-07 6.55651093e-07 6.55651093e-07 5.96046448e-07\n 6.55651093e-07 6.55651093e-07 5.96046448e-07 5.96046448e-07\n 5.96046448e-07 5.96046448e-07 5.96046448e-07 5.96046448e-07\n 5.96046448e-07 6.55651093e-07]\nrelative ICaCE scores for concept holocaust is -3.225806451612909%\nrelative ICaCE scores for concept nazism is -3.225806451612909%\nrelative ICaCE scores for concept genocide is -3.225806451612909%\nrelative ICaCE scores for concept funny is -3.225806451612909%\nrelative ICaCE scores for concept anti muslim is 6.451612903225801%\nrelative ICaCE scores for concept terrorism is 6.451612903225801%\nrelative ICaCE scores for concept violence is 6.451612903225801%\nrelative ICaCE scores for concept politics is -3.225806451612909%\nrelative ICaCE scores for concept racism is 6.451612903225801%\nrelative ICaCE scores for concept international relation is 6.451612903225801%\nrelative ICaCE scores for concept adult is -3.225806451612909%\nrelative ICaCE scores for concept gore is -3.225806451612909%\nrelative ICaCE scores for concept misogynistic is -3.225806451612909%\nrelative ICaCE scores for concept immigration is -3.225806451612909%\nrelative ICaCE scores for concept extremism is -3.225806451612909%\nrelative ICaCE scores for concept immoral is -3.225806451612909%\nrelative ICaCE scores for concept white supremacy is -3.225806451612909%\nanti muslim terrorism violence racism international relation \n\n30 {'text': \"d.j. osama spin laden droppin' beats like the twin towers\", 'img': 'img/93547.png', 'label': 1}\ntensor([0.2857, 0.7143], device='cuda:0', grad_fn=<SelectBackward0>)\npredicted class offensive\n[0.00068933 0.00068957 0.00068951 0.00068933 0.00068945 0.00068945\n 0.00068945 0.00068939 0.00068927 0.00068945 0.00068951 0.00068969\n 0.00068986 0.00068957 0.00068921 0.00068969 0.00068945 0.00068933]\nrelative ICaCE scores for concept holocaust is -0.020651928553931005%\nrelative ICaCE scores for concept nazism is 0.013928044838700582%\nrelative ICaCE scores for concept genocide is 0.005283051490542683%\nrelative ICaCE scores for concept funny is -0.020651928553931005%\nrelative ICaCE scores for concept anti muslim is -0.0033619418576152126%\nrelative ICaCE scores for concept terrorism is -0.0033619418576152126%\nrelative ICaCE scores for concept violence is -0.0033619418576152126%\nrelative ICaCE scores for concept politics is -0.012006935205773108%\nrelative ICaCE scores for concept racism is -0.0292969219020889%\nrelative ICaCE scores for concept international relation is -0.0033619418576152126%\nrelative ICaCE scores for concept adult is 0.005283051490542683%\nrelative ICaCE scores for concept gore is 0.031218031535016375%\nrelative ICaCE scores for concept misogynistic is 0.05715301157949006%\nrelative ICaCE scores for concept immigration is 0.013928044838700582%\nrelative ICaCE scores for concept extremism is -0.0379419152502468%\nrelative ICaCE scores for concept immoral is 0.031218031535016375%\nrelative ICaCE scores for concept white supremacy is -0.0033619418576152126%\nmisogynistic gore immoral nazism immigration genocide adult \n\n48 {'text': 'we said we would never forget why are you voting them into our government?', 'img': 'img/74386.png', 'label': 1}\ntensor([0.7733, 0.2267], device='cuda:0', grad_fn=<SelectBackward0>)\npredicted class non offensive\n[0.00178379 0.00178438 0.00178403 0.00178367 0.00178438 0.00178409\n 0.00178409 0.00178403 0.00178367 0.00178397 0.00178421 0.00178486\n 0.00178522 0.00178444 0.00178379 0.0017845  0.00178409 0.00178409]\nrelative ICaCE scores for concept holocaust is -0.022085912343050063%\nrelative ICaCE scores for concept nazism is 0.01132135002459686%\nrelative ICaCE scores for concept genocide is -0.008723007395991297%\nrelative ICaCE scores for concept funny is -0.028767364816579447%\nrelative ICaCE scores for concept anti muslim is 0.01132135002459686%\nrelative ICaCE scores for concept terrorism is -0.005382281159226603%\nrelative ICaCE scores for concept violence is -0.005382281159226603%\nrelative ICaCE scores for concept politics is -0.008723007395991297%\nrelative ICaCE scores for concept racism is -0.028767364816579447%\nrelative ICaCE scores for concept international relation is -0.012063733632755988%\nrelative ICaCE scores for concept adult is 0.001299171314302782%\nrelative ICaCE scores for concept gore is 0.0380471599187144%\nrelative ICaCE scores for concept misogynistic is 0.058091517339302554%\nrelative ICaCE scores for concept immigration is 0.014662076261361552%\nrelative ICaCE scores for concept extremism is -0.022085912343050063%\nrelative ICaCE scores for concept immoral is 0.018002802498126243%\nrelative ICaCE scores for concept white supremacy is -0.005382281159226603%\nmisogynistic gore immoral immigration nazism anti muslim adult \n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from captum.attr import IntegratedGradients\n",
        "\n",
        "# # input1 = torch.tensor([[1.0, 3.0], [3.0, 5.0]], requires_grad=True)\n",
        "# # input2 = torch.tensor([[1.0, 4.0], [0.0, 2.0]], requires_grad=True)\n",
        "# # # Initializing our toy model\n",
        "# # model = ToyModel_With_Additional_Forward_Args()\n",
        "# # # Applying integrated gradients on the input\n",
        "# ig = IntegratedGradients(vb)\n",
        "# (input1_attr, input2_attr), delta = ig.attribute((input1, input2), n_steps=100,\n",
        "#                                     additional_forward_args=1, return_convergence_delta=True)\n",
        "# output\n",
        "# .........\n",
        "# input1_attr: tensor([[0.0000, 0.0000],\n",
        "#                      [0.0000, 3.3428]], grad_fn=<MulBackward0>)\n",
        "# input2_attr:  tensor([[ 0.0000,  0.0000],\n",
        "#                       [0.0000, -1.3371]], grad_fn=<MulBackward0>)\n",
        "# approximation_error (aka delta): 0.005693793296813965"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:13:55.417694Z",
          "iopub.execute_input": "2024-09-20T20:13:55.418332Z",
          "iopub.status.idle": "2024-09-20T20:13:55.423105Z",
          "shell.execute_reply.started": "2024-09-20T20:13:55.418303Z",
          "shell.execute_reply": "2024-09-20T20:13:55.421986Z"
        },
        "trusted": true,
        "id": "usp7Tk0rXaqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "concept_classes"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:13:55.751887Z",
          "iopub.execute_input": "2024-09-20T20:13:55.752204Z",
          "iopub.status.idle": "2024-09-20T20:13:55.758417Z",
          "shell.execute_reply.started": "2024-09-20T20:13:55.752178Z",
          "shell.execute_reply": "2024-09-20T20:13:55.757461Z"
        },
        "trusted": true,
        "id": "Y2ypI5iWXaqa",
        "outputId": "f30927ed-ad7a-4407-a588-fc3132665b80"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 60,
          "output_type": "execute_result",
          "data": {
            "text/plain": "['holocaust',\n 'nazism',\n 'genocide',\n 'funny',\n 'anti muslim',\n 'terrorism',\n 'violence',\n 'politics',\n 'racism',\n 'international relation',\n 'adult',\n 'gore',\n 'misogynistic',\n 'immigration',\n 'extremism',\n 'immoral',\n 'white supremacy',\n 'indecency']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vb"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:13:57.916657Z",
          "iopub.execute_input": "2024-09-20T20:13:57.917050Z",
          "iopub.status.idle": "2024-09-20T20:13:57.925061Z",
          "shell.execute_reply.started": "2024-09-20T20:13:57.917019Z",
          "shell.execute_reply": "2024-09-20T20:13:57.924142Z"
        },
        "trusted": true,
        "id": "WXh_UzKRXaqb",
        "outputId": "b01cbcfc-7dc3-4977-e8ae-bee1d3a5c12c"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 61,
          "output_type": "execute_result",
          "data": {
            "text/plain": "VB(\n  (vb): VisualBertModel(\n    (embeddings): VisualBertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=1)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n      (visual_token_type_embeddings): Embedding(2, 768)\n      (visual_position_embeddings): Embedding(512, 768)\n      (visual_projection): Linear(in_features=2048, out_features=768, bias=True)\n    )\n    (encoder): VisualBertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x VisualBertLayer(\n          (attention): VisualBertAttention(\n            (self): VisualBertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): VisualBertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): VisualBertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): VisualBertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): VisualBertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (linear_relu_stack): Sequential(\n    (0): Dropout(p=0.1, inplace=False)\n    (1): Linear(in_features=768, out_features=512, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=512, out_features=128, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=128, out_features=2, bias=True)\n  )\n  (linear_relu_stack_latent): Sequential(\n    (0): Dropout(p=0.1, inplace=False)\n    (1): Linear(in_features=768, out_features=512, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=512, out_features=128, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=128, out_features=2, bias=True)\n  )\n  (linear_relu_stack_hidden): Sequential(\n    (0): Dropout(p=0.1, inplace=False)\n    (1): Linear(in_features=768, out_features=512, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=512, out_features=128, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=128, out_features=2, bias=True)\n  )\n  (project_latent): Linear(in_features=768, out_features=2048, bias=True)\n  (proj_conc): Linear(in_features=768, out_features=28, bias=True)\n  (proj_embed): Linear(in_features=768, out_features=28, bias=True)\n  (grad_rev1): RevGrad()\n  (grad_rev2): RevGrad()\n)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "baselines = (torch.randn(20,18,768).to('cuda'), torch.randn(20,64,768).to('cuda'), torch.randn(20,36,2048).to('cuda'))\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:14:01.399606Z",
          "iopub.execute_input": "2024-09-20T20:14:01.400336Z",
          "iopub.status.idle": "2024-09-20T20:14:01.431744Z",
          "shell.execute_reply.started": "2024-09-20T20:14:01.400303Z",
          "shell.execute_reply": "2024-09-20T20:14:01.431016Z"
        },
        "trusted": true,
        "id": "DljB5xt3Xaqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "from captum.attr import IntegratedGradients,Saliency,DeepLift,DeepLiftShap, GradientShap, InputXGradient, KernelShap, FeatureAblation\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:14:02.457884Z",
          "iopub.execute_input": "2024-09-20T20:14:02.458251Z",
          "iopub.status.idle": "2024-09-20T20:14:02.537027Z",
          "shell.execute_reply.started": "2024-09-20T20:14:02.458223Z",
          "shell.execute_reply": "2024-09-20T20:14:02.536059Z"
        },
        "trusted": true,
        "id": "OowlHB8aXaqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.manual_seed(42)\n",
        "# print(torch.randn(2,3))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:14:02.797990Z",
          "iopub.execute_input": "2024-09-20T20:14:02.798900Z",
          "iopub.status.idle": "2024-09-20T20:14:02.803130Z",
          "shell.execute_reply.started": "2024-09-20T20:14:02.798859Z",
          "shell.execute_reply": "2024-09-20T20:14:02.801964Z"
        },
        "trusted": true,
        "id": "20WbdlpTXaqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "#vb_wrapper = VB_wrapper(vb)\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:14:04.473358Z",
          "iopub.execute_input": "2024-09-20T20:14:04.473759Z",
          "iopub.status.idle": "2024-09-20T20:14:04.478001Z",
          "shell.execute_reply.started": "2024-09-20T20:14:04.473721Z",
          "shell.execute_reply": "2024-09-20T20:14:04.477085Z"
        },
        "trusted": true,
        "id": "8hi1OSjEXaqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "!pip install shutup"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:14:05.332824Z",
          "iopub.execute_input": "2024-09-20T20:14:05.333946Z",
          "iopub.status.idle": "2024-09-20T20:14:17.754773Z",
          "shell.execute_reply.started": "2024-09-20T20:14:05.333901Z",
          "shell.execute_reply": "2024-09-20T20:14:17.753579Z"
        },
        "trusted": true,
        "id": "YslJ2zXZXaqd",
        "outputId": "2f3c911e-b839-4c88-e164-0d5e03ddcfef"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting shutup\n  Downloading shutup-0.2.0-py3-none-any.whl.metadata (530 bytes)\nDownloading shutup-0.2.0-py3-none-any.whl (1.5 kB)\nInstalling collected packages: shutup\nSuccessfully installed shutup-0.2.0\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(gc)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:14:17.757199Z",
          "iopub.execute_input": "2024-09-20T20:14:17.757990Z",
          "iopub.status.idle": "2024-09-20T20:14:17.764364Z",
          "shell.execute_reply.started": "2024-09-20T20:14:17.757951Z",
          "shell.execute_reply": "2024-09-20T20:14:17.763403Z"
        },
        "trusted": true,
        "id": "k7hSjA2vXaqd",
        "outputId": "64face7e-9df6-43bd-99a7-877995b7ff04"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 67,
          "output_type": "execute_result",
          "data": {
            "text/plain": "208"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def call_sim():\n",
        "\n",
        "    import math\n",
        "    # Quantitative case study (1) Faithfulness on concepts\n",
        "    # Quantitative case study (2) Are they explainable?\n",
        "    test_dataloader=DataLoader(t_p, shuffle=True, batch_size=1)\n",
        "\n",
        "    pred_r1, pred_r2 = [],[]\n",
        "    pred_both = []\n",
        "    act = []\n",
        "\n",
        "    multimodal_repr = []\n",
        "    concept_repr = []\n",
        "\n",
        "    concept_repr1 = []\n",
        "    concept_repr2 = []\n",
        "    y_p = []\n",
        "\n",
        "    cr, rr = [],[]\n",
        "    cr1,rr1 = [],[]\n",
        "\n",
        "\n",
        "    ours_icace = []\n",
        "    ia_icace = []\n",
        "\n",
        "    concept_cat = {}\n",
        "    for i,c in enumerate(concept_classes):\n",
        "        concept_cat[c] = all_concepts[i]\n",
        "\n",
        "    for ii, batch in tqdm(enumerate(eval_dataloader)):\n",
        "\n",
        "        #if batch[1].detach().cpu().numpy()[0] == 1:\n",
        "\n",
        "        ll = batch[1].detach().cpu().numpy()[0]\n",
        "        y_p.append(ll)\n",
        "\n",
        "        #print('*'*10)\n",
        "\n",
        "        input_ids = batch[0]['input_ids'].to('cuda')\n",
        "        #print(tokenizer.batch_decode(batch[0]['input_ids'], skip_special_tokens=True))\n",
        "        token_type_ids = batch[0]['token_type_ids'].to('cuda')\n",
        "        attention_mask = batch[0]['attention_mask'].to('cuda')\n",
        "        visual_embeds = batch[0]['visual_embeds'].to('cuda')\n",
        "        visual_token_type_ids = batch[0]['visual_token_type_ids'].to('cuda')\n",
        "        visual_attention_mask = batch[0]['visual_attention_mask'].to('cuda')\n",
        "\n",
        "        inputs_embeds = vb_wrapper.vb.vb.embeddings.word_embeddings(input_ids)\n",
        "\n",
        "        #     (input1_attr, input2_attr), delta = ig.attribute(inputs = (all_concepts.to('cuda'), inputs_embeds), additional_forward_args = (attention_mask,\n",
        "        #         token_type_ids,\n",
        "        #         None,\n",
        "        #         None,\n",
        "        #         None,\n",
        "        #         visual_embeds,\n",
        "        #         visual_attention_mask,\n",
        "        #         visual_token_type_ids), return_convergence_delta=True)\n",
        "\n",
        "        #print(visual_embeds)\n",
        "\n",
        "        #     (input1_attr, input2_attr, input3_attr) = ig.attribute(inputs = (all_concepts.unsqueeze(0).to('cuda'), inputs_embeds, visual_embeds), additional_forward_args = (attention_mask,\n",
        "        #         token_type_ids,\n",
        "        #         None,\n",
        "        #         None,\n",
        "        #         None,\n",
        "        #         visual_attention_mask,\n",
        "        #         visual_token_type_ids))\n",
        "        #vb(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask, visual_embeds=visual_embeds_latent, visual_attention_mask=visual_attention_mask_latent, visual_token_type_ids=visual_token_type_ids_latent)\n",
        "\n",
        "        if type(ig) in [captum.attr._core.kernel_shap.KernelShap, captum.attr._core.input_x_gradient.InputXGradient, captum.attr._core.deep_lift.DeepLift, captum.attr._core.saliency.Saliency, captum.attr._core.integrated_gradients.IntegratedGradients, captum.attr._core.feature_ablation.FeatureAblation]:\n",
        "                baseline = None\n",
        "                #print('none')\n",
        "        else:\n",
        "            baseline = baselines\n",
        "            #print('baseline')\n",
        "\n",
        "        all_conc = get_inlp(all_concepts).to('cuda')\n",
        "        if type(ig) in [captum.attr._core.input_x_gradient.InputXGradient,captum.attr._core.saliency.Saliency]:\n",
        "            #print('ip_grad')\n",
        "            (input1_attr, input2_attr, input3_attr) = ig.attribute(inputs = (all_conc.unsqueeze(0), inputs_embeds, visual_embeds),\n",
        "\n",
        "                                                                additional_forward_args = (attention_mask,\n",
        "            token_type_ids,\n",
        "            None,\n",
        "            None,\n",
        "            None,\n",
        "            visual_attention_mask,\n",
        "            visual_token_type_ids))\n",
        "\n",
        "        else:\n",
        "            #print('others')\n",
        "\n",
        "            (input1_attr, input2_attr, input3_attr) = ig.attribute(inputs = (all_conc.unsqueeze(0), inputs_embeds, visual_embeds),\n",
        "                                                                  baselines = baseline,\n",
        "                                                                    additional_forward_args = (attention_mask,\n",
        "                token_type_ids,\n",
        "                None,\n",
        "                None,\n",
        "                None,\n",
        "                visual_attention_mask,\n",
        "                visual_token_type_ids))\n",
        "\n",
        "\n",
        "        #print(input1_attr, input2_attr)\n",
        "        #print(input1_attr.shape, input2_attr.shape)\n",
        "        k1=input1_attr.sum(dim=-1).squeeze(0).detach().cpu().numpy()\n",
        "\n",
        "        conc_scr = []\n",
        "        attribution_score = {}\n",
        "        for i in range(18):\n",
        "            cc = concept_classes[i]\n",
        "            #print('relative attribution scores for concept {} is {}'.format(cc,k[i]))\n",
        "            conc_scr.append((cc,k1[i]))\n",
        "            attribution_score[cc] = k1[i]\n",
        "            #conc_scr[cc] = k1[i]\n",
        "        conc_scr.sort(key=lambda tup: tup[1], reverse=True)\n",
        "\n",
        "        d = {}\n",
        "\n",
        "        for cnt,i in enumerate(conc_scr):\n",
        "            d[i[0]] = cnt\n",
        "\n",
        "        #print(d)\n",
        "\n",
        "\n",
        "        orig,pooled_op = model_pred(input_ids,attention_mask,token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids,all_concepts)\n",
        "        orig = orig[0]\n",
        "        pooled_op = pooled_op.detach().cpu()\n",
        "        multimodal_repr.append(pooled_op)\n",
        "        #print('predicted class {}'.format(class_map[np.argmax(orig.detach().cpu().numpy())]))\n",
        "        #print('actual class {}'.format(ll))\n",
        "        #print(0/0)\n",
        "        #orig = orig[0]\n",
        "        k = []\n",
        "        pred_class = np.argmax(orig.detach().cpu().numpy())\n",
        "        causal_score = {}\n",
        "        #print(pred_class)\n",
        "        for i in range(18):\n",
        "            #print('Removing concept: {}'.format(concept_classes[i]))\n",
        "            cc = concept_classes[i]\n",
        "            after_intervention,_ = model_pred(input_ids,attention_mask,token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids,all_concepts,concept_no=i)\n",
        "            after_intervention = after_intervention[0][pred_class]\n",
        "            #print('hello')\n",
        "            #print(orig[pred_class], after_intervention)\n",
        "            #print(0/0)\n",
        "            k.append((cc,abs(after_intervention-orig[pred_class]).item()))\n",
        "            causal_score[cc] = abs(after_intervention-orig[pred_class]).item()\n",
        "            #print('model prediction after removal {}'.format(model_pred(input_ids,attention_mask,token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids,concept_no=i)))\n",
        "        #k = np.asarray(k)\n",
        "        #k = np.asarray(k)\n",
        "        #k = list(map(lambda x: ((x-np.mean(k))/np.mean(k))*100, k))\n",
        "        k.sort(key=lambda tup: tup[1], reverse=True)\n",
        "        #print(k)\n",
        "\n",
        "        #print(list(map(lambda x: x[0], list(k))))\n",
        "\n",
        "        gamma = 0.9\n",
        "        temp = []\n",
        "        for cnt,i in enumerate(list(map(lambda x: x[0], list(k)))):\n",
        "            temp.append((gamma**cnt) * concept_cat[i])\n",
        "            #temp.append((math.e**(-cnt)) * concept_cat[i])\n",
        "            #temp.append((1.1**(-cnt)) * concept_cat[i])\n",
        "\n",
        "        temp = torch.stack(temp).mean(dim=0).unsqueeze(dim=0)\n",
        "        #print(temp)\n",
        "        #print(temp.shape)\n",
        "\n",
        "        concept_repr.append(temp)\n",
        "\n",
        "\n",
        "\n",
        "        temp = []\n",
        "        for cnt,i in enumerate(list(map(lambda x: x[0], list(conc_scr)))):\n",
        "            temp.append((gamma**cnt) * concept_cat[i])\n",
        "            #temp.append((math.e**(-cnt)) * concept_cat[i])\n",
        "            #temp.append((1.1**(-cnt)) * concept_cat[i])\n",
        "\n",
        "        temp = torch.stack(temp).mean(dim=0).unsqueeze(dim=0)\n",
        "        #print(temp)\n",
        "        #print(temp.shape)\n",
        "\n",
        "        concept_repr1.append(temp)\n",
        "\n",
        "\n",
        "        #print(list(map(lambda x: x[0], list(conc_scr))))\n",
        "        #print(0/0)\n",
        "\n",
        "        #considering the k to be the causally sorted rank, we estimate ranks of\n",
        "\n",
        "        ip_attrib_causal = []\n",
        "\n",
        "\n",
        "\n",
        "        for i in range(18):\n",
        "            cc = concept_classes[i]\n",
        "            ip_attrib_causal.append((cc,((2*attribution_score[cc]*causal_score[cc]) / (attribution_score[cc]+causal_score[cc]))))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ip_attrib_causal.sort(key=lambda tup: tup[1], reverse=True)\n",
        "\n",
        "\n",
        "\n",
        "        ra_icace_ours = []\n",
        "        for cnt,i in enumerate(list(map(lambda x: x[0], list(k)))):\n",
        "            ra_icace_ours.append((gamma**cnt) * causal_score[i])\n",
        "            #ra_icace_ours.append((math.e**(-cnt)) * causal_score[i])\n",
        "            #ra_icace_ours.append((1.1**(-cnt)) * causal_score[i])\n",
        "\n",
        "        ra_icace_ia = []\n",
        "        for cnt,i in enumerate(list(map(lambda x: x[0], list(conc_scr)))):\n",
        "            ra_icace_ia.append((gamma**cnt) * causal_score[i])\n",
        "            #ra_icace_ia.append((math.e**(-cnt)) * causal_score[i])\n",
        "            #ra_icace_ia.append((1.1**(-cnt)) * causal_score[i])\n",
        "\n",
        "\n",
        "\n",
        "        ours_icace.append(np.mean(ra_icace_ours))\n",
        "        ia_icace.append(np.mean(ra_icace_ia))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        temp = []\n",
        "        for cnt,i in enumerate(list(map(lambda x: x[0], list(ip_attrib_causal)))):\n",
        "            temp.append((gamma**cnt) * concept_cat[i])\n",
        "            #temp.append((math.e**(-cnt)) * concept_cat[i])\n",
        "            #temp.append((1.1**(-cnt)) * concept_cat[i])\n",
        "\n",
        "        temp = torch.stack(temp).mean(dim=0).unsqueeze(dim=0)\n",
        "\n",
        "        concept_repr2.append(temp)\n",
        "\n",
        "\n",
        "\n",
        "        d1 = {}\n",
        "\n",
        "        for cnt,i in enumerate(ip_attrib_causal):\n",
        "            d1[i[0]] = cnt\n",
        "\n",
        "        r1,r2,r3 = [],[],[]\n",
        "        for cnt,i in enumerate(list(map(lambda x: x[0], list(k)))):\n",
        "            r1.append(cnt)\n",
        "            r2.append(d[i])\n",
        "            r3.append(d1[i])\n",
        "\n",
        "        #print(r1,r2)\n",
        "        corr, _ = stats.kendalltau(r1, r2)\n",
        "        res,_ = stats.spearmanr(r1, r2)\n",
        "\n",
        "        corr1, _ = stats.kendalltau(r1, r3)\n",
        "        res1,_ = stats.spearmanr(r1, r3)\n",
        "\n",
        "        #print('kendall tau', corr)\n",
        "        cr.append(corr)\n",
        "        #print('spearmans rho', res)\n",
        "        rr.append(res)\n",
        "\n",
        "        cr1.append(corr1)\n",
        "        rr1.append(res1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    multimodal_repr = torch.stack(multimodal_repr).squeeze(1)\n",
        "\n",
        "    #causal\n",
        "    concept_repr = torch.stack(concept_repr).squeeze(1)\n",
        "\n",
        "\n",
        "    #attribution\n",
        "    concept_repr1 = torch.stack(concept_repr1).squeeze(1)\n",
        "\n",
        "    #HM\n",
        "    concept_repr2 = torch.stack(concept_repr2).squeeze(1)\n",
        "\n",
        "\n",
        "    return np.mean(cr), np.mean(rr), multimodal_repr, concept_repr, concept_repr1, y_p\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:14:21.281544Z",
          "iopub.execute_input": "2024-09-20T20:14:21.282033Z",
          "iopub.status.idle": "2024-09-20T20:14:21.323544Z",
          "shell.execute_reply.started": "2024-09-20T20:14:21.281998Z",
          "shell.execute_reply": "2024-09-20T20:14:21.322605Z"
        },
        "trusted": true,
        "id": "FZWNX_BDXaqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /kaggle/working"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:14:28.951878Z",
          "iopub.execute_input": "2024-09-20T20:14:28.952306Z",
          "iopub.status.idle": "2024-09-20T20:14:30.038215Z",
          "shell.execute_reply.started": "2024-09-20T20:14:28.952274Z",
          "shell.execute_reply": "2024-09-20T20:14:30.037040Z"
        },
        "trusted": true,
        "id": "yxZIGrjqXaqe",
        "outputId": "78badbba-c353-4f06-b14d-6e39091a3b73"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "25461.png\t   cr1.pt\t\t\t  mean_comparison_plot.png\n28076.png\t   cr2.pt\t\t\t  mr.pt\n94162.png\t   final_model_concept.pt\t  rr.npy\n96704.png\t   final_model_concept_inlp.pt\t  rr1.npy\ncausal_vb.pt\t   final_model_wo_adversarial.pt  transformers\ncausal_vb_inlp.pt  foo1.png\t\t\t  vs_tensors.pt\ncr.npy\t\t   full_annotation.pkl\t\t  y_p.pt\ncr.pt\t\t   image1.jpg\ncr1.npy\t\t   image2.jpg\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#clf = svm.SVC(kernel='poly', C=2, random_state=42)\n",
        "\n",
        "def get_sim(multimodal_repr, concept_repr1, y_p):\n",
        "\n",
        "    X = np.concatenate([multimodal_repr.numpy(),concept_repr1.numpy()],axis=-1) # inp w/ clip representation w/ exp -> w/ both\n",
        "    #X = (multimodal_repr.numpy()*concept_repr.numpy())\n",
        "    X_ = multimodal_repr.numpy() # inp w/ clip representation w/o exp -> w/ only input\n",
        "    X__ = concept_repr1.numpy() # inp w/o clip representation w/ exp -> w/ only exp\n",
        "\n",
        "    print(X.shape)\n",
        "    print(X_.shape)\n",
        "    print(X__.shape)\n",
        "    print(y_p)\n",
        "    #print(0/0)\n",
        "\n",
        "\n",
        "\n",
        "    kf = KFold(n_splits=5)\n",
        "    y = np.array(y_p)\n",
        "    vals = []\n",
        "    vals_lus = []\n",
        "    k1,k2,k3 = [],[],[]\n",
        "    C,S = [],[]\n",
        "    for train, test in kf.split(X):\n",
        "\n",
        "        X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]\n",
        "        X_train_, X_test_, y_train, y_test = X_[train], X_[test], y[train], y[test]\n",
        "        X_train__, X_test__, y_train, y_test = X__[train], X__[test], y[train], y[test]\n",
        "\n",
        "\n",
        "        clf1 = make_pipeline(StandardScaler(),svm.SVC(probability=True, kernel='rbf', C=2, random_state=42))\n",
        "        clf1.fit(X_train, y_train)\n",
        "        y_pred = clf1.predict(X_test)\n",
        "\n",
        "        clf2 = make_pipeline(StandardScaler(),svm.SVC(probability=True,kernel='rbf', C=2, random_state=42))\n",
        "        clf2.fit(X_train_, y_train)\n",
        "        y_pred_ = clf2.predict(X_test_)\n",
        "\n",
        "        clf3 = make_pipeline(StandardScaler(), svm.SVC(probability=True,kernel='rbf', C=2, random_state=42))\n",
        "        clf3.fit(X_train__, y_train)\n",
        "        y_pred__ = clf3.predict(X_test__)\n",
        "\n",
        "\n",
        "        y_pred_proba_w_both = clf1.predict_proba(X_test)\n",
        "        y_pred_proba_w_inp = clf2.predict_proba(X_test_)\n",
        "        y_pred_proba_w_exp = clf3.predict_proba(X_test__)\n",
        "\n",
        "\n",
        "\n",
        "        comprehensiveness, sufficiency = 0,0\n",
        "\n",
        "        counter = 0\n",
        "        for pred_proba_w_both, pred_proba_w_inp, pred_proba_w_exp, pred_class in zip(y_pred_proba_w_both,y_pred_proba_w_inp,y_pred_proba_w_exp,y_pred):\n",
        "            comprehensiveness += (pred_proba_w_both[pred_class] - pred_proba_w_inp[pred_class])\n",
        "            sufficiency += (pred_proba_w_both[pred_class] - pred_proba_w_exp[pred_class])\n",
        "            counter+=1\n",
        "\n",
        "\n",
        "        comprehensiveness = comprehensiveness / counter\n",
        "        sufficiency = sufficiency / counter\n",
        "\n",
        "        C.append(comprehensiveness)\n",
        "        S.append(sufficiency)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        l, nl = 0,0\n",
        "        us = 0\n",
        "        for i ,j in zip(y_pred__,y_test):\n",
        "            if i==j:\n",
        "                l+=1\n",
        "            else:\n",
        "                nl+=1\n",
        "        a,b=0,0\n",
        "        for i,j,k,x in zip(y_pred,y_pred_,y_test,y_pred__):\n",
        "            us+=int(i==k) - int(j==k)\n",
        "            if k==x:\n",
        "                a += int(i==k) - int(j==k)\n",
        "            elif k!=x:\n",
        "                b += int(i==k) - int(j==k)\n",
        "\n",
        "        #print('LAS ',.5*(a/l)+.5*(b/nl))\n",
        "        #print('Leakage Unadjusted Simulatability (LUS)', (us/(l+nl)))\n",
        "        vals_lus.append(us/(l+nl))\n",
        "        #print('Comprehensiveness ', comprehensiveness)\n",
        "        #print('Sufficiency ', sufficiency)\n",
        "        #print('f1 between prediction of proposed model and SVM w/ both inp and exp {}'.format(f1_score(y_test, y_pred, average='macro')))\n",
        "        #print('f1 between prediction of proposed model and SVM w/ only inp {}'.format(f1_score(y_test, y_pred_, average='macro')))\n",
        "        #print('f1 between prediction of proposed model and SVM w/ only exp {}'.format(f1_score(y_test, y_pred__, average='macro')))\n",
        "        vals.append(.5*(a/l)+.5*(b/nl))\n",
        "        k1.append(f1_score(y_test, y_pred, average='macro'))\n",
        "        k2.append(f1_score(y_test, y_pred_, average='macro'))\n",
        "        k3.append(f1_score(y_test, y_pred__, average='macro'))\n",
        "\n",
        "\n",
        "    #print('Intra mean ', np.mean(ID), 'Intra std ', np.std(ID))\n",
        "    #print('Inter mean ', np.mean(ID1), 'Inter std ', np.std(ID1))\n",
        "    print('LAS mean ', np.mean(vals), 'LAS std ', np.std(vals))\n",
        "    print('LUS mean ', np.mean(vals_lus), 'LUS std ', np.std(vals_lus))\n",
        "    print('Comprehensiveness mean ', np.mean(C), 'Comprehensiveness std ', np.std(C))\n",
        "    print('Sufficiency mean ', np.mean(S), 'Sufficiency std ', np.std(S))\n",
        "    print('Avg. f1 between prediction of proposed model and SVM w/ both inp and exp {}'.format(np.mean(k1)))\n",
        "    print('Avg. f1 between prediction of proposed model and SVM w/ only inp {}'.format(np.mean(k2)))\n",
        "    print('Avg. f1 between prediction of proposed model and SVM w/ only exp {}'.format(np.mean(k3)))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:14:33.041525Z",
          "iopub.execute_input": "2024-09-20T20:14:33.041936Z",
          "iopub.status.idle": "2024-09-20T20:14:33.063796Z",
          "shell.execute_reply.started": "2024-09-20T20:14:33.041901Z",
          "shell.execute_reply": "2024-09-20T20:14:33.062876Z"
        },
        "trusted": true,
        "id": "1e_GES-hXaqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UQUgLw2xXaqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "cd = {i:[] for i in concept_classes}\n",
        "import shutup; shutup.please()\n",
        "def call():\n",
        "    import math\n",
        "    # Quantitative case study (1) Faithfulness on concepts\n",
        "    # Quantitative case study (2) Are they explainable?\n",
        "    test_dataloader=DataLoader(t_p, shuffle=True, batch_size=1)\n",
        "\n",
        "    pred_r1, pred_r2 = [],[]\n",
        "    pred_both = []\n",
        "    act = []\n",
        "\n",
        "    multimodal_repr = []\n",
        "    concept_repr = []\n",
        "\n",
        "    concept_repr1 = []\n",
        "    concept_repr2 = []\n",
        "    y_p = []\n",
        "\n",
        "    cr, rr = [],[]\n",
        "    cr1,rr1 = [],[]\n",
        "\n",
        "\n",
        "    ours_icace = []\n",
        "    ia_icace = []\n",
        "\n",
        "    counter = 0\n",
        "\n",
        "\n",
        "    c_p=0\n",
        "    c_r=0\n",
        "\n",
        "    a_p=0\n",
        "    a_r=0\n",
        "\n",
        "    average_precision_causal = 0\n",
        "    average_precision_attrib = 0\n",
        "\n",
        "    concept_cat = {}\n",
        "    for i,c in enumerate(concept_classes):\n",
        "        concept_cat[c] = all_concepts[i]\n",
        "\n",
        "    for ii, batch in tqdm(enumerate(eval_dataloader)):\n",
        "\n",
        "        if ii==0:\n",
        "            print(ig)\n",
        "\n",
        "        #if batch[1].detach().cpu().numpy()[0] == 1:\n",
        "\n",
        "        ll = batch[1].detach().cpu().numpy()[0]\n",
        "        if ll==1:\n",
        "\n",
        "            #ll = batch[1].detach().cpu().numpy()[0]\n",
        "            #print(ll, ii)\n",
        "            y_p.append(ll)\n",
        "\n",
        "            #print('*'*10)\n",
        "\n",
        "            input_ids = batch[0]['input_ids'].to('cuda')\n",
        "            #print(tokenizer.batch_decode(batch[0]['input_ids'], skip_special_tokens=True))\n",
        "            token_type_ids = batch[0]['token_type_ids'].to('cuda')\n",
        "            attention_mask = batch[0]['attention_mask'].to('cuda')\n",
        "            visual_embeds = batch[0]['visual_embeds'].to('cuda')\n",
        "            visual_token_type_ids = batch[0]['visual_token_type_ids'].to('cuda')\n",
        "            visual_attention_mask = batch[0]['visual_attention_mask'].to('cuda')\n",
        "\n",
        "            inputs_embeds = vb_wrapper.vb.vb.embeddings.word_embeddings(input_ids)\n",
        "\n",
        "            # print(inputs_embeds)\n",
        "            #     (input1_attr, input2_attr), delta = ig.attribute(inputs = (all_concepts.to('cuda'), inputs_embeds), additional_forward_args = (attention_mask,\n",
        "            #         token_type_ids,\n",
        "            #         None,\n",
        "            #         None,\n",
        "            #         None,\n",
        "            #         visual_embeds,\n",
        "            #         visual_attention_mask,\n",
        "            #         visual_token_type_ids), return_convergence_delta=True)\n",
        "\n",
        "            #print(visual_embeds)\n",
        "\n",
        "            #     (input1_attr, input2_attr, input3_attr) = ig.attribute(inputs = (all_concepts.unsqueeze(0).to('cuda'), inputs_embeds, visual_embeds), additional_forward_args = (attention_mask,\n",
        "            #         token_type_ids,\n",
        "            #         None,\n",
        "            #         None,\n",
        "            #         None,\n",
        "            #         visual_attention_mask,\n",
        "            #         visual_token_type_ids))\n",
        "            #vb(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask, visual_embeds=visual_embeds_latent, visual_attention_mask=visual_attention_mask_latent, visual_token_type_ids=visual_token_type_ids_latent)\n",
        "\n",
        "            if type(ig) in [captum.attr._core.kernel_shap.KernelShap, captum.attr._core.input_x_gradient.InputXGradient, captum.attr._core.deep_lift.DeepLift, captum.attr._core.saliency.Saliency, captum.attr._core.integrated_gradients.IntegratedGradients, captum.attr._core.feature_ablation.FeatureAblation]:\n",
        "                    baseline = None\n",
        "                    #print('none')\n",
        "            else:\n",
        "                baseline = baselines\n",
        "                #print('baseline')\n",
        "\n",
        "            if DECONFOUND:\n",
        "                all_conc = get_inlp(all_concepts).to('cuda')\n",
        "            else:\n",
        "                all_conc = all_concepts.to('cuda')\n",
        "\n",
        "            #print(all_conc)\n",
        "            #print(all_concepts)\n",
        "\n",
        "            #print(0/0)\n",
        "\n",
        "            #all_conc = all_concepts.to('cuda')\n",
        "\n",
        "            if type(ig) in [captum.attr._core.input_x_gradient.InputXGradient,captum.attr._core.saliency.Saliency]:\n",
        "                #print('ip_grad')\n",
        "                (input1_attr, input2_attr, input3_attr) = ig.attribute(inputs = (all_conc.unsqueeze(0).to('cuda'), inputs_embeds, visual_embeds),\n",
        "\n",
        "                                                                    additional_forward_args = (attention_mask,\n",
        "                token_type_ids,\n",
        "                None,\n",
        "                None,\n",
        "                None,\n",
        "                visual_attention_mask,\n",
        "                visual_token_type_ids))\n",
        "\n",
        "            else:\n",
        "                #print('others')\n",
        "\n",
        "                (input1_attr, input2_attr, input3_attr) = ig.attribute(inputs = (all_conc.unsqueeze(0).to('cuda'), inputs_embeds, visual_embeds),\n",
        "                                                                      baselines = baseline,\n",
        "                                                                        additional_forward_args = (attention_mask,\n",
        "                    token_type_ids,\n",
        "                    None,\n",
        "                    None,\n",
        "                    None,\n",
        "                    visual_attention_mask,\n",
        "                    visual_token_type_ids))\n",
        "\n",
        "\n",
        "            #print(input1_attr, input2_attr)\n",
        "            #print(input1_attr.shape, input2_attr.shape)\n",
        "            k1=input1_attr.sum(dim=-1).squeeze(0).detach().cpu().numpy()\n",
        "\n",
        "            conc_scr = []\n",
        "            attribution_score = {}\n",
        "            for i in range(18):\n",
        "                cc = concept_classes[i]\n",
        "                #print('relative attribution scores for concept {} is {}'.format(cc,k[i]))\n",
        "                conc_scr.append((cc,k1[i]))\n",
        "                attribution_score[cc] = k1[i]\n",
        "                #conc_scr[cc] = k1[i]\n",
        "            conc_scr.sort(key=lambda tup: tup[1], reverse=True)\n",
        "\n",
        "            d = {}\n",
        "\n",
        "            for cnt,i in enumerate(conc_scr):\n",
        "                d[i[0]] = cnt\n",
        "\n",
        "            #print(d)\n",
        "\n",
        "\n",
        "            orig,pooled_op = model_pred(input_ids,attention_mask,token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids,all_concepts)\n",
        "            orig = orig[0]\n",
        "            pooled_op = pooled_op.detach().cpu()\n",
        "            multimodal_repr.append(pooled_op)\n",
        "            #print('predicted class {}'.format(class_map[np.argmax(orig.detach().cpu().numpy())]))\n",
        "            #print('actual class {}'.format(ll))\n",
        "            #print(0/0)\n",
        "            #orig = orig[0]\n",
        "            k = []\n",
        "            pred_class = np.argmax(orig.detach().cpu().numpy())\n",
        "            causal_score = {}\n",
        "            #print(pred_class)\n",
        "            for i in range(18):\n",
        "                #print('Removing concept: {}'.format(concept_classes[i]))\n",
        "                cc = concept_classes[i]\n",
        "                after_intervention,_ = model_pred(input_ids,attention_mask,token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids,all_concepts,concept_no=i)\n",
        "                after_intervention = after_intervention[0][pred_class]\n",
        "                #print('hello')\n",
        "                #print(orig[pred_class], after_intervention)\n",
        "                #print(0/0)\n",
        "                k.append((cc,abs(after_intervention-orig[pred_class]).item()))\n",
        "                causal_score[cc] = abs(after_intervention-orig[pred_class]).item()\n",
        "                #print('model prediction after removal {}'.format(model_pred(input_ids,attention_mask,token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids,concept_no=i)))\n",
        "            #k = np.asarray(k)\n",
        "            #k = np.asarray(k)\n",
        "            #k = list(map(lambda x: ((x-np.mean(k))/np.mean(k))*100, k))\n",
        "            k.sort(key=lambda tup: tup[1], reverse=True)\n",
        "\n",
        "            #print(k)\n",
        "\n",
        "            #random.seed(counter)\n",
        "            #random.shuffle(k)\n",
        "\n",
        "            #print(k)\n",
        "\n",
        "            #print(0/0)\n",
        "\n",
        "\n",
        "            for i in k:\n",
        "                cd[i[0]].append(i[1])\n",
        "\n",
        "\n",
        "\n",
        "            #print(list(map(lambda x: x[0], list(k))))\n",
        "\n",
        "            gamma = 0.9\n",
        "            temp = []\n",
        "            for cnt,i in enumerate(list(map(lambda x: x[0], list(k)))):\n",
        "                temp.append((gamma**cnt) * concept_cat[i])\n",
        "                #temp.append((math.e**(-cnt)) * concept_cat[i])\n",
        "                #temp.append((1.1**(-cnt)) * concept_cat[i])\n",
        "\n",
        "            temp = torch.stack(temp).mean(dim=0).unsqueeze(dim=0)\n",
        "            #print(temp)\n",
        "            #print(temp.shape)\n",
        "\n",
        "            concept_repr.append(temp)\n",
        "\n",
        "\n",
        "\n",
        "            temp = []\n",
        "            for cnt,i in enumerate(list(map(lambda x: x[0], list(conc_scr)))):\n",
        "                temp.append((gamma**cnt) * concept_cat[i])\n",
        "                #temp.append((math.e**(-cnt)) * concept_cat[i])\n",
        "                #temp.append((1.1**(-cnt)) * concept_cat[i])\n",
        "\n",
        "            temp = torch.stack(temp).mean(dim=0).unsqueeze(dim=0)\n",
        "            #print(temp)\n",
        "            #print(temp.shape)\n",
        "\n",
        "            concept_repr1.append(temp)\n",
        "\n",
        "\n",
        "            #print(list(map(lambda x: x[0], list(conc_scr))))\n",
        "            #print(0/0)\n",
        "\n",
        "            #considering the k to be the causally sorted rank, we estimate ranks of\n",
        "\n",
        "            ip_attrib_causal = []\n",
        "\n",
        "\n",
        "\n",
        "            for i in range(18):\n",
        "                cc = concept_classes[i]\n",
        "                ip_attrib_causal.append((cc,((2*attribution_score[cc]*causal_score[cc]) / (attribution_score[cc]+causal_score[cc]))))\n",
        "\n",
        "\n",
        "\n",
        "            ip_attrib = []\n",
        "            for i in range(18):\n",
        "                cc = concept_classes[i]\n",
        "                ip_attrib.append((cc,attribution_score[cc]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            ip_attrib_causal.sort(key=lambda tup: tup[1], reverse=True)\n",
        "            ip_attrib.sort(key=lambda tup: tup[1], reverse=True)\n",
        "\n",
        "\n",
        "\n",
        "            ra_icace_ours = []\n",
        "            for cnt,i in enumerate(list(map(lambda x: x[0], list(k)))):\n",
        "                ra_icace_ours.append((gamma**cnt) * causal_score[i])\n",
        "                #ra_icace_ours.append((math.e**(-cnt)) * causal_score[i])\n",
        "                #ra_icace_ours.append((1.1**(-cnt)) * causal_score[i])\n",
        "\n",
        "            ra_icace_ia = []\n",
        "            for cnt,i in enumerate(list(map(lambda x: x[0], list(conc_scr)))):\n",
        "                ra_icace_ia.append((gamma**cnt) * causal_score[i])\n",
        "                #ra_icace_ia.append((math.e**(-cnt)) * causal_score[i])\n",
        "                #ra_icace_ia.append((1.1**(-cnt)) * causal_score[i])\n",
        "\n",
        "\n",
        "\n",
        "            ours_icace.append(np.mean(ra_icace_ours))\n",
        "            ia_icace.append(np.mean(ra_icace_ia))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            temp = []\n",
        "            for cnt,i in enumerate(list(map(lambda x: x[0], list(ip_attrib_causal)))):\n",
        "                temp.append((gamma**cnt) * concept_cat[i])\n",
        "                #temp.append((math.e**(-cnt)) * concept_cat[i])\n",
        "                #temp.append((1.1**(-cnt)) * concept_cat[i])\n",
        "\n",
        "            temp = torch.stack(temp).mean(dim=0).unsqueeze(dim=0)\n",
        "\n",
        "            concept_repr2.append(temp)\n",
        "\n",
        "\n",
        "\n",
        "            d1 = {}\n",
        "\n",
        "            for cnt,i in enumerate(ip_attrib_causal):\n",
        "                d1[i[0]] = cnt\n",
        "\n",
        "            r1,r2,r3 = [],[],[]\n",
        "            for cnt,i in enumerate(list(map(lambda x: x[0], list(k)))):\n",
        "                r1.append(cnt)\n",
        "                r2.append(d[i])\n",
        "                r3.append(d1[i])\n",
        "\n",
        "            #print(r1,r2)\n",
        "            corr, _ = stats.kendalltau(r1, r2)\n",
        "            res,_ = stats.spearmanr(r1, r2)\n",
        "\n",
        "            corr1, _ = stats.kendalltau(r1, r3)\n",
        "            res1,_ = stats.spearmanr(r1, r3)\n",
        "\n",
        "            #print('kendall tau', corr)\n",
        "            cr.append(corr)\n",
        "            #print('spearmans rho', res)\n",
        "            rr.append(res)\n",
        "\n",
        "            cr1.append(corr1)\n",
        "            rr1.append(res1)\n",
        "\n",
        "\n",
        "            rel_items = gc[counter]\n",
        "            causal = [i[0] for i in k]\n",
        "            #causal = random.sample(concept_classes,5)\n",
        "            attrib = [i[0] for i in ip_attrib]\n",
        "\n",
        "\n",
        "            #         mrr_causal = []\n",
        "            #         for i in range(len(causal)):\n",
        "            #             item = causal[i]\n",
        "            #             if item in rel_items:\n",
        "            #                 mrr_causal.append(1/(i+1))\n",
        "\n",
        "            #         mrr_attrib = []\n",
        "            #         for i in range(len(attrib)):\n",
        "            #             item = attrib[i]\n",
        "            #             if item in rel_items:\n",
        "            #                 mrr_attrib.append(1/(i+1))\n",
        "\n",
        "\n",
        "\n",
        "            #         mrr_causal = np.mean(np.asarray(mrr_causal))\n",
        "\n",
        "            #print('gc', rel_items)\n",
        "            #print('causal', causal)\n",
        "            #print('attrib', attrib)\n",
        "\n",
        "            K = 5\n",
        "\n",
        "            rel_rec_items_at_K = set(causal[:K]) & set(rel_items)\n",
        "            rec_items_at_K = causal[:K]\n",
        "            tot_rel_items = len(rel_items)\n",
        "\n",
        "            #print('Causal P@{} '.format(K), len(rel_rec_items_at_K) / len(rec_items_at_K) )\n",
        "            #print('Causal R@{} '.format(K), len(rel_rec_items_at_K) / tot_rel_items )\n",
        "\n",
        "            c_p+=len(rel_rec_items_at_K) / len(rec_items_at_K)\n",
        "            c_r+=len(rel_rec_items_at_K) / tot_rel_items\n",
        "\n",
        "\n",
        "            rel_rec_items_at_K = set(attrib[:K]) & set(rel_items)\n",
        "            rec_items_at_K = attrib[:K]\n",
        "            tot_rel_items = len(rel_items)\n",
        "\n",
        "            #print('Attrib P@{} '.format(K), len(rel_rec_items_at_K) / len(rec_items_at_K) )\n",
        "            #print('Attrib R@{} '.format(K), len(rel_rec_items_at_K) / tot_rel_items )\n",
        "\n",
        "            a_p+=len(rel_rec_items_at_K) / len(rec_items_at_K)\n",
        "            a_r+=len(rel_rec_items_at_K) / tot_rel_items\n",
        "\n",
        "\n",
        "            tot_precision_causal = []\n",
        "            for k in range(1,K+1):\n",
        "                tot_precision_causal.append(len(set(causal[:k]) & set(rel_items)) / len(causal[:k]))\n",
        "\n",
        "            tot_precision_attrib = []\n",
        "            for k in range(1,K+1):\n",
        "                tot_precision_attrib.append(len(set(attrib[:k]) & set(rel_items)) / len(attrib[:k]))\n",
        "\n",
        "\n",
        "\n",
        "            average_precision_causal += np.mean(np.asarray(tot_precision_causal))\n",
        "            average_precision_attrib += np.mean(np.asarray(tot_precision_attrib))\n",
        "\n",
        "            counter+=1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    multimodal_repr = torch.stack(multimodal_repr).squeeze(1)\n",
        "\n",
        "    #causal\n",
        "    concept_repr = torch.stack(concept_repr).squeeze(1)\n",
        "\n",
        "\n",
        "    #attribution\n",
        "    concept_repr1 = torch.stack(concept_repr1).squeeze(1)\n",
        "\n",
        "    #HM\n",
        "    concept_repr2 = torch.stack(concept_repr2).squeeze(1)\n",
        "\n",
        "    print('mean precision causal@5',c_p/counter)\n",
        "    print('mean recall causal@5', c_r/counter)\n",
        "\n",
        "    print('mean precision attrb@5',a_p/counter)\n",
        "    print('mean recall attrb@5', a_r/counter)\n",
        "\n",
        "    print('MAP@5 Causal ', average_precision_causal/counter)\n",
        "    print('MAP@5 Attrib ', average_precision_attrib/counter)\n",
        "\n",
        "    #     print('kendall tau', np.mean(cr))\n",
        "\n",
        "    #     print('spearmans rho', np.mean(rr))\n",
        "\n",
        "\n",
        "    #     print('-----------CAUSAL----------')\n",
        "    #     get_sim(multimodal_repr, concept_repr)\n",
        "\n",
        "    #     print('-----------Attribution----------')\n",
        "    #     get_sim(multimodal_repr, concept_repr1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:14:37.794716Z",
          "iopub.execute_input": "2024-09-20T20:14:37.795081Z",
          "iopub.status.idle": "2024-09-20T20:14:37.867415Z",
          "shell.execute_reply.started": "2024-09-20T20:14:37.795052Z",
          "shell.execute_reply": "2024-09-20T20:14:37.866536Z"
        },
        "trusted": true,
        "id": "zaggW4x2Xaqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baire = ['ig', 'saliency', 'deeplift', 'deepliftshap', 'gradshap', 'ipxgrad']\n",
        "#R\n",
        "#baire = ['ig', 'saliency']\n",
        "import captum\n",
        "from scipy import stats\n",
        "for fu in range(len(baire)):\n",
        "\n",
        "\n",
        "    vb_wrapper = VB_wrapper(vb)\n",
        "    vb_wrapper.eval()\n",
        "    vb_wrapper.zero_grad()\n",
        "\n",
        "    if baire[fu]=='ig':\n",
        "        ig = IntegratedGradients(vb_wrapper)\n",
        "    elif baire[fu]=='saliency':\n",
        "        ig = Saliency(vb_wrapper)\n",
        "    elif baire[fu]=='deeplift':\n",
        "        ig = DeepLift(vb_wrapper)\n",
        "    elif baire[fu]=='deepliftshap':\n",
        "        ig = DeepLiftShap(vb_wrapper)\n",
        "    elif baire[fu]=='gradshap':\n",
        "        ig = GradientShap(vb_wrapper)\n",
        "    elif baire[fu]=='ipxgrad':\n",
        "        ig = InputXGradient(vb_wrapper)\n",
        "\n",
        "    call()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T20:14:41.900817Z",
          "iopub.execute_input": "2024-09-20T20:14:41.901979Z",
          "iopub.status.idle": "2024-09-20T21:04:46.831718Z",
          "shell.execute_reply.started": "2024-09-20T20:14:41.901936Z",
          "shell.execute_reply": "2024-09-20T21:04:46.830479Z"
        },
        "trusted": true,
        "id": "hqb00gWhXaqg",
        "outputId": "951b1339-63a0-4bc2-c9f8-44445e8fa19e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "0it [00:00, ?it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "<captum.attr._core.integrated_gradients.IntegratedGradients object at 0x7db1308651e0>\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "660it [09:03,  1.21it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "mean precision causal@5 0.13942307692307662\nmean recall causal@5 0.20763507326007324\nmean precision attrb@5 0.17403846153846145\nmean recall attrb@5 0.2590430402930403\nMAP@5 Causal  0.16249999999999995\nMAP@5 Attrib  0.18873397435897438\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "0it [00:00, ?it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "<captum.attr._core.saliency.Saliency object at 0x7db1309f3250>\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "660it [08:01,  1.37it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "mean precision causal@5 0.13942307692307662\nmean recall causal@5 0.20763507326007324\nmean precision attrb@5 0.12499999999999975\nmean recall attrb@5 0.189320054945055\nMAP@5 Causal  0.16249999999999995\nMAP@5 Attrib  0.16442307692307687\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "0it [00:00, ?it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "<captum.attr._core.deep_lift.DeepLift object at 0x7db1308651e0>\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "660it [08:12,  1.34it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "mean precision causal@5 0.13942307692307662\nmean recall causal@5 0.20763507326007324\nmean precision attrb@5 0.17788461538461528\nmean recall attrb@5 0.26864697802197807\nMAP@5 Causal  0.16249999999999995\nMAP@5 Attrib  0.17187500000000003\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "0it [00:00, ?it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "<captum.attr._core.deep_lift.DeepLiftShap object at 0x7db1309f3250>\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "660it [08:37,  1.28it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "mean precision causal@5 0.13942307692307662\nmean recall causal@5 0.20763507326007324\nmean precision attrb@5 0.21730769230769245\nmean recall attrb@5 0.31462339743589757\nMAP@5 Causal  0.16249999999999995\nMAP@5 Attrib  0.22110576923076922\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "0it [00:00, ?it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "<captum.attr._core.gradient_shap.GradientShap object at 0x7db1307ae9b0>\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "660it [08:07,  1.35it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "mean precision causal@5 0.13942307692307662\nmean recall causal@5 0.20763507326007324\nmean precision attrb@5 0.2\nmean recall attrb@5 0.3031078296703299\nMAP@5 Causal  0.16249999999999995\nMAP@5 Attrib  0.2206891025641025\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "0it [00:00, ?it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "<captum.attr._core.input_x_gradient.InputXGradient object at 0x7db131194040>\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "660it [08:02,  1.37it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "mean precision causal@5 0.13942307692307662\nmean recall causal@5 0.20763507326007324\nmean precision attrb@5 0.17403846153846142\nmean recall attrb@5 0.26464056776556777\nMAP@5 Causal  0.16249999999999995\nMAP@5 Attrib  0.1685416666666667\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NR5TWP2bXarO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hi810ng4XarO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JG_QIzCaXarO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_t6c-tlkXarO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
