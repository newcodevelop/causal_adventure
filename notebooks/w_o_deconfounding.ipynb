{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 1246182,
          "sourceType": "datasetVersion",
          "datasetId": 715500
        }
      ],
      "dockerImageVersionId": 30648,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!ls /kaggle/working/"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:14:16.440104Z",
          "iopub.execute_input": "2024-09-20T21:14:16.440867Z",
          "iopub.status.idle": "2024-09-20T21:14:17.449228Z",
          "shell.execute_reply.started": "2024-09-20T21:14:16.440833Z",
          "shell.execute_reply": "2024-09-20T21:14:17.448292Z"
        },
        "trusted": true,
        "id": "JPs-6UKYYRaZ",
        "outputId": "77bee9f5-7815-42c3-efee-c1ab62821f44"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "25461.png\t   cr1.pt\t\t\t  mean_comparison_plot.png\n28076.png\t   cr2.pt\t\t\t  mr.pt\n94162.png\t   final_model_concept.pt\t  rr.npy\n96704.png\t   final_model_concept_inlp.pt\t  rr1.npy\ncausal_vb.pt\t   final_model_wo_adversarial.pt  transformers\ncausal_vb_inlp.pt  foo1.png\t\t\t  vs_tensors.pt\ncr.npy\t\t   full_annotation.pkl\t\t  y_p.pt\ncr.pt\t\t   image1.jpg\ncr1.npy\t\t   image2.jpg\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN\n",
        "\n",
        "import ipywidgets as widgets\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:14:21.136826Z",
          "iopub.execute_input": "2024-09-20T21:14:21.137248Z",
          "iopub.status.idle": "2024-09-20T21:14:21.218994Z",
          "shell.execute_reply.started": "2024-09-20T21:14:21.137214Z",
          "shell.execute_reply": "2024-09-20T21:14:21.218241Z"
        },
        "trusted": true,
        "id": "0weE7vQfYRaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "qqHVAo8hYRat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN\n",
        "ff = []"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:14:24.395020Z",
          "iopub.execute_input": "2024-09-20T21:14:24.395407Z",
          "iopub.status.idle": "2024-09-20T21:14:24.399750Z",
          "shell.execute_reply.started": "2024-09-20T21:14:24.395377Z",
          "shell.execute_reply": "2024-09-20T21:14:24.398756Z"
        },
        "trusted": true,
        "id": "5IZ0M7D_YRax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN\n",
        "!pip install jsonlines\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:14:24.830878Z",
          "iopub.execute_input": "2024-09-20T21:14:24.831242Z",
          "iopub.status.idle": "2024-09-20T21:14:38.179246Z",
          "shell.execute_reply.started": "2024-09-20T21:14:24.831214Z",
          "shell.execute_reply": "2024-09-20T21:14:38.178330Z"
        },
        "trusted": true,
        "id": "rYc3UgRyYRay",
        "outputId": "4c5452c6-c959-4ba4-fa36-4ae2150274d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting jsonlines\n  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\nRequirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonlines) (23.2.0)\nDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\nInstalling collected packages: jsonlines\nSuccessfully installed jsonlines-4.0.0\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN\n",
        "!pip install captum==0.7.0\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:14:42.519022Z",
          "iopub.execute_input": "2024-09-20T21:14:42.519381Z",
          "iopub.status.idle": "2024-09-20T21:14:55.625097Z",
          "shell.execute_reply.started": "2024-09-20T21:14:42.519354Z",
          "shell.execute_reply": "2024-09-20T21:14:55.624051Z"
        },
        "trusted": true,
        "id": "bufp_qgxYRaz",
        "outputId": "75ea6c8f-f737-4c37-af6e-c431e6646dca"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting captum==0.7.0\n  Downloading captum-0.7.0-py3-none-any.whl.metadata (26 kB)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from captum==0.7.0) (3.7.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from captum==0.7.0) (1.24.4)\nRequirement already satisfied: torch>=1.6 in /opt/conda/lib/python3.10/site-packages (from captum==0.7.0) (2.1.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from captum==0.7.0) (4.66.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum==0.7.0) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum==0.7.0) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum==0.7.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum==0.7.0) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum==0.7.0) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum==0.7.0) (2023.12.2)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum==0.7.0) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum==0.7.0) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum==0.7.0) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum==0.7.0) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum==0.7.0) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum==0.7.0) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum==0.7.0) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum==0.7.0) (2.8.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->captum==0.7.0) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6->captum==0.7.0) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6->captum==0.7.0) (1.3.0)\nDownloading captum-0.7.0-py3-none-any.whl (1.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: captum\nSuccessfully installed captum-0.7.0\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "import jsonlines"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:14:55.627046Z",
          "iopub.execute_input": "2024-09-20T21:14:55.627714Z",
          "iopub.status.idle": "2024-09-20T21:14:55.668956Z",
          "shell.execute_reply.started": "2024-09-20T21:14:55.627679Z",
          "shell.execute_reply": "2024-09-20T21:14:55.668251Z"
        },
        "trusted": true,
        "id": "7Ux9hH-cYRa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "!pip install wget\n",
        "#!pip install transformers"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:14:59.906642Z",
          "iopub.execute_input": "2024-09-20T21:14:59.906996Z",
          "iopub.status.idle": "2024-09-20T21:15:14.588604Z",
          "shell.execute_reply.started": "2024-09-20T21:14:59.906968Z",
          "shell.execute_reply": "2024-09-20T21:15:14.587676Z"
        },
        "trusted": true,
        "id": "48aGVmUmYRa1",
        "outputId": "a49a4bff-1a2a-4008-ac76-8d1a3e46d0f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting wget\n  Downloading wget-3.2.zip (10 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: wget\n  Building wheel for wget (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=0a303fd4be827abb72d2b2f126503511da224e0b3eadd98a53f64694b090d27b\n  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\nSuccessfully built wget\nInstalling collected packages: wget\nSuccessfully installed wget-3.2\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "import sys\n",
        "sys.path.append('/kaggle/working/transformers/examples/research_projects/visual_bert')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:15:14.590437Z",
          "iopub.execute_input": "2024-09-20T21:15:14.590736Z",
          "iopub.status.idle": "2024-09-20T21:15:14.595137Z",
          "shell.execute_reply.started": "2024-09-20T21:15:14.590711Z",
          "shell.execute_reply": "2024-09-20T21:15:14.594266Z"
        },
        "trusted": true,
        "id": "gD6s510VYRa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "import numpy as np\n",
        "import jsonlines\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import sys\n",
        "import os\n",
        "from IPython.display import Image, display\n",
        "import PIL.Image\n",
        "import io\n",
        "import torch\n",
        "import numpy as np\n",
        "from processing_image import Preprocess\n",
        "from visualizing_image import SingleImageViz\n",
        "from modeling_frcnn import GeneralizedRCNN\n",
        "from utils import Config\n",
        "import utils\n",
        "from transformers import VisualBertForQuestionAnswering, BertTokenizerFast\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
        "from sklearn import svm\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:15:29.444022Z",
          "iopub.execute_input": "2024-09-20T21:15:29.444899Z",
          "iopub.status.idle": "2024-09-20T21:15:39.147422Z",
          "shell.execute_reply.started": "2024-09-20T21:15:29.444870Z",
          "shell.execute_reply": "2024-09-20T21:15:39.146561Z"
        },
        "trusted": true,
        "id": "MT-ooHUKYRa5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "train_file = '/kaggle/input/facebook-hateful-meme-dataset/data/train.jsonl'\n",
        "k =0\n",
        "with jsonlines.open(train_file) as reader:\n",
        "    for obj in reader:\n",
        "        print(obj['img'])\n",
        "        if k==9:\n",
        "            break\n",
        "        k+=1"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:15:39.148904Z",
          "iopub.execute_input": "2024-09-20T21:15:39.149379Z",
          "iopub.status.idle": "2024-09-20T21:15:39.175865Z",
          "shell.execute_reply.started": "2024-09-20T21:15:39.149354Z",
          "shell.execute_reply": "2024-09-20T21:15:39.174965Z"
        },
        "trusted": true,
        "id": "pmBw2HIUYRa8",
        "outputId": "5c6defa1-a768-4ff3-a965-77782ad224a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "img/42953.png\nimg/23058.png\nimg/13894.png\nimg/37408.png\nimg/82403.png\nimg/16952.png\nimg/76932.png\nimg/70914.png\nimg/02973.png\nimg/58306.png\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "visual_feats = {}"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:15:39.176901Z",
          "iopub.execute_input": "2024-09-20T21:15:39.177309Z",
          "iopub.status.idle": "2024-09-20T21:15:39.181473Z",
          "shell.execute_reply.started": "2024-09-20T21:15:39.177268Z",
          "shell.execute_reply": "2024-09-20T21:15:39.180600Z"
        },
        "trusted": true,
        "id": "IrMIHjcwYRa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "device = 'cuda'\n",
        "frcnn_cfg = Config.from_pretrained(\"unc-nlp/frcnn-vg-finetuned\")\n",
        "frcnn_cfg.MODEL.device = device\n",
        "frcnn = GeneralizedRCNN.from_pretrained(\"unc-nlp/frcnn-vg-finetuned\", config=frcnn_cfg)\n",
        "image_preprocess = Preprocess(frcnn_cfg)\n",
        "frcnn.eval()\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:15:39.183272Z",
          "iopub.execute_input": "2024-09-20T21:15:39.183541Z",
          "iopub.status.idle": "2024-09-20T21:15:48.476136Z",
          "shell.execute_reply.started": "2024-09-20T21:15:39.183519Z",
          "shell.execute_reply": "2024-09-20T21:15:48.475159Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "549c484654d3458e97d5bffe10e17c27",
            "f06dacf7165a4cdba9fa1b5542aeaf49"
          ]
        },
        "id": "NDAA9bXoYRa9",
        "outputId": "d7c422c1-4b25-4b94-cc88-e0e5f2840d9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "%s not found in cache or force_download set to True, downloading to %s https://s3.amazonaws.com/models.huggingface.co/bert/unc-nlp/frcnn-vg-finetuned/config.yaml /root/.cache/torch/transformers/tmp2ivqyl5j\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/2.13k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "549c484654d3458e97d5bffe10e17c27"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "loading configuration file cache\n%s not found in cache or force_download set to True, downloading to %s https://cdn.huggingface.co/unc-nlp/frcnn-vg-finetuned/pytorch_model.bin /root/.cache/torch/transformers/tmpd1u7adq1\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/262M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f06dacf7165a4cdba9fa1b5542aeaf49"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "loading weights file https://cdn.huggingface.co/unc-nlp/frcnn-vg-finetuned/pytorch_model.bin from cache at /root/.cache/torch/transformers/57f6df6abe353be2773f2700159c65615babf39ab5b48114d2b49267672ae10f.77b59256a4cf8343ae0f923246a81489fc8d82f98d082edc2d2037c977c0d9d0\nAll model checkpoint weights were used when initializing GeneralizedRCNN.\n\nAll the weights of GeneralizedRCNN were initialized from the model checkpoint at unc-nlp/frcnn-vg-finetuned.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use GeneralizedRCNN for predictions without further training.\n",
          "output_type": "stream"
        },
        {
          "execution_count": 13,
          "output_type": "execute_result",
          "data": {
            "text/plain": "GeneralizedRCNN(\n  (backbone): ResNet(\n    (stem): BasicStem(\n      (conv1): Conv2d(\n        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n        (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (res2): Sequential(\n      (0): BottleneckBlock(\n        (shortcut): Conv2d(\n          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv1): Conv2d(\n          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BottleneckBlock(\n        (conv1): Conv2d(\n          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (2): BottleneckBlock(\n        (conv1): Conv2d(\n          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (res3): Sequential(\n      (0): BottleneckBlock(\n        (shortcut): Conv2d(\n          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv1): Conv2d(\n          256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BottleneckBlock(\n        (conv1): Conv2d(\n          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (2): BottleneckBlock(\n        (conv1): Conv2d(\n          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (3): BottleneckBlock(\n        (conv1): Conv2d(\n          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (res4): Sequential(\n      (0): BottleneckBlock(\n        (shortcut): Conv2d(\n          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv1): Conv2d(\n          512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (2): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (3): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (4): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (5): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (6): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (7): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (8): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (9): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (10): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (11): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (12): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (13): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (14): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (15): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (16): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (17): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (18): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (19): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (20): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (21): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (22): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n    )\n  )\n  (proposal_generator): RPN(\n    (anchor_generator): AnchorGenerator(\n      (cell_anchors): ParameterList(  (0): Parameter containing: [torch.float32 of size 12x4 (cuda:0)])\n    )\n    (rpn_head): RPNHead(\n      (conv): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (objectness_logits): Conv2d(512, 12, kernel_size=(1, 1), stride=(1, 1))\n      (anchor_deltas): Conv2d(512, 48, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n  (roi_heads): Res5ROIHeads(\n    (pooler): ROIPooler(\n      (level_poolers): ModuleList(\n        (0): RoIPool(output_size=(14, 14), spatial_scale=0.0625)\n      )\n    )\n    (res5): Sequential(\n      (0): BottleneckBlock(\n        (shortcut): Conv2d(\n          1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv1): Conv2d(\n          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False\n          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BottleneckBlock(\n        (conv1): Conv2d(\n          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False\n          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (2): BottleneckBlock(\n        (conv1): Conv2d(\n          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False\n          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (box_predictor): FastRCNNOutputLayers(\n      (cls_score): Linear(in_features=2048, out_features=1601, bias=True)\n      (bbox_pred): Linear(in_features=2048, out_features=6400, bias=True)\n      (cls_embedding): Embedding(1601, 256)\n      (fc_attr): Linear(in_features=2304, out_features=512, bias=True)\n      (attr_score): Linear(in_features=512, out_features=401, bias=True)\n    )\n  )\n)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "def get_visual_embedding(img_paths):\n",
        "    images, sizes, scales_yx = image_preprocess(img_paths) # img_paths -> list of image paths\n",
        "    output_dict = frcnn(\n",
        "      images,\n",
        "      sizes,\n",
        "      scales_yx=scales_yx,\n",
        "      padding=\"max_detections\",\n",
        "      max_detections=frcnn_cfg.max_detections,\n",
        "      return_tensors=\"pt\",\n",
        "    )\n",
        "    features = output_dict.get(\"roi_features\")\n",
        "    visual_embeds = features\n",
        "    visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\n",
        "    visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n",
        "    return visual_embeds,visual_token_type_ids,visual_attention_mask"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:15:48.477544Z",
          "iopub.execute_input": "2024-09-20T21:15:48.477906Z",
          "iopub.status.idle": "2024-09-20T21:15:48.484952Z",
          "shell.execute_reply.started": "2024-09-20T21:15:48.477870Z",
          "shell.execute_reply": "2024-09-20T21:15:48.483975Z"
        },
        "trusted": true,
        "id": "f2a2TbvCYRa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "visual_feats = torch.load('./vs_tensors.pt')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:15:48.487144Z",
          "iopub.execute_input": "2024-09-20T21:15:48.487666Z",
          "iopub.status.idle": "2024-09-20T21:15:51.588924Z",
          "shell.execute_reply.started": "2024-09-20T21:15:48.487632Z",
          "shell.execute_reply": "2024-09-20T21:15:51.588141Z"
        },
        "trusted": true,
        "id": "ulk9f4qNYRa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "len(visual_feats)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:15:54.973616Z",
          "iopub.execute_input": "2024-09-20T21:15:54.974260Z",
          "iopub.status.idle": "2024-09-20T21:15:54.979926Z",
          "shell.execute_reply.started": "2024-09-20T21:15:54.974231Z",
          "shell.execute_reply": "2024-09-20T21:15:54.979032Z"
        },
        "trusted": true,
        "id": "9aD6m81CYRa_",
        "outputId": "479af0a8-a1ef-4008-c8ef-716a97741dd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 16,
          "output_type": "execute_result",
          "data": {
            "text/plain": "8500"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# visual_feats"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:15:57.777053Z",
          "iopub.execute_input": "2024-09-20T21:15:57.777508Z",
          "iopub.status.idle": "2024-09-20T21:15:57.781468Z",
          "shell.execute_reply.started": "2024-09-20T21:15:57.777482Z",
          "shell.execute_reply": "2024-09-20T21:15:57.780543Z"
        },
        "trusted": true,
        "id": "3Y5jhy7HYRbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "from utils import Config\n",
        "import utils\n",
        "from transformers import VisualBertModel, BertTokenizer\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.metrics import accuracy_score,f1_score,roc_auc_score\n",
        "from scipy.special import softmax"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:16:00.571509Z",
          "iopub.execute_input": "2024-09-20T21:16:00.571896Z",
          "iopub.status.idle": "2024-09-20T21:16:00.577972Z",
          "shell.execute_reply.started": "2024-09-20T21:16:00.571866Z",
          "shell.execute_reply": "2024-09-20T21:16:00.576827Z"
        },
        "trusted": true,
        "id": "pzWu0kECYRbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "train_set = []\n",
        "train_file = '/kaggle/input/facebook-hateful-meme-dataset/data/train.jsonl'\n",
        "k =0\n",
        "with jsonlines.open(train_file) as reader:\n",
        "    for obj in reader:\n",
        "    #         print(obj['img'])\n",
        "\n",
        "        train_set.append({'text': obj['text'],\n",
        "\n",
        "                       'img': obj['img'],\n",
        "\n",
        "                       'label': int(obj['label'])})\n",
        "    #         print(k)\n",
        "    #         if k==1500:\n",
        "    #             break\n",
        "        k+=1\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:16:02.848381Z",
          "iopub.execute_input": "2024-09-20T21:16:02.848971Z",
          "iopub.status.idle": "2024-09-20T21:16:02.888920Z",
          "shell.execute_reply.started": "2024-09-20T21:16:02.848941Z",
          "shell.execute_reply": "2024-09-20T21:16:02.888123Z"
        },
        "trusted": true,
        "id": "zuF973gFYRbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "len(train_set)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:16:05.365376Z",
          "iopub.execute_input": "2024-09-20T21:16:05.366119Z",
          "iopub.status.idle": "2024-09-20T21:16:05.371517Z",
          "shell.execute_reply.started": "2024-09-20T21:16:05.366067Z",
          "shell.execute_reply": "2024-09-20T21:16:05.370606Z"
        },
        "trusted": true,
        "id": "Ypi8jKXwYRbD",
        "outputId": "339aabef-834c-4f17-c2f3-a08c74ce41af"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 20,
          "output_type": "execute_result",
          "data": {
            "text/plain": "8500"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, ds, max_len=64):\n",
        "        self.ds = ds\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "        self.model = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "\n",
        "\n",
        "        self.max_len = max_len\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        ds_idx = self.ds[index]\n",
        "        inputs = self.tokenizer(ds_idx['text'], padding=\"max_length\", truncation=True, max_length=self.max_len, return_tensors='pt')\n",
        "\n",
        "\n",
        "        inputs['input_ids'] = inputs['input_ids'].squeeze(0)\n",
        "        inputs['token_type_ids'] = inputs['token_type_ids'].squeeze(0)\n",
        "        inputs['attention_mask'] = inputs['attention_mask'].squeeze(0)\n",
        "\n",
        "        # visual_embeds,visual_token_type_ids,visual_attention_mask = get_visual_embedding('./data/'+ds_idx['img'])\n",
        "        visual_embeds,visual_token_type_ids,visual_attention_mask = visual_feats[ds_idx['img']]\n",
        "\n",
        "        #print(ds_idx['img'])\n",
        "\n",
        "        inputs.update({\n",
        "          \"visual_embeds\": torch.squeeze(visual_embeds),\n",
        "          \"visual_token_type_ids\": torch.squeeze(visual_token_type_ids),\n",
        "          \"visual_attention_mask\": torch.squeeze(visual_attention_mask)\n",
        "        })\n",
        "\n",
        "\n",
        "\n",
        "        #inputs.update({\"path\": ds_idx['img']})\n",
        "\n",
        "        return inputs, int(ds_idx['label'])\n",
        "\n",
        "t = CustomDataset(train_set)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:16:08.907332Z",
          "iopub.execute_input": "2024-09-20T21:16:08.908368Z",
          "iopub.status.idle": "2024-09-20T21:16:40.522797Z",
          "shell.execute_reply.started": "2024-09-20T21:16:08.908333Z",
          "shell.execute_reply": "2024-09-20T21:16:40.521822Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "14c0f64e488441e1bbf0e6bbf75fcfc5",
            "a37a4c4506f94397ad55ddf6d99727d5",
            "cb60ad9176ba464f883795f7d57cd3d2",
            "d90b0437ebaa4118808344111d564e51",
            "7a2beead80a5434d81a0124b80a1c31e",
            "f469598ba9fe4409b4d2dbc18e88c6c4"
          ]
        },
        "id": "bPNIwjXrYRbE",
        "outputId": "c05d2edf-adad-4629-ad8c-fb6c9641e9d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "14c0f64e488441e1bbf0e6bbf75fcfc5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a37a4c4506f94397ad55ddf6d99727d5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cb60ad9176ba464f883795f7d57cd3d2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d90b0437ebaa4118808344111d564e51"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/631 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7a2beead80a5434d81a0124b80a1c31e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "pytorch_model.bin:   0%|          | 0.00/448M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f469598ba9fe4409b4d2dbc18e88c6c4"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!export CUBLAS_WORKSPACE_CONFIG=:16:8"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:18:27.850112Z",
          "iopub.execute_input": "2024-09-20T21:18:27.850794Z",
          "iopub.status.idle": "2024-09-20T21:18:27.854949Z",
          "shell.execute_reply.started": "2024-09-20T21:18:27.850761Z",
          "shell.execute_reply": "2024-09-20T21:18:27.853906Z"
        },
        "trusted": true,
        "id": "LYs4Cdd0YRbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "seed=42\n",
        "random.seed(seed)     # python random generator\n",
        "np.random.seed(seed)  # numpy random generator\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "#torch.use_deterministic_algorithms(True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:18:28.605768Z",
          "iopub.execute_input": "2024-09-20T21:18:28.606704Z",
          "iopub.status.idle": "2024-09-20T21:18:28.612691Z",
          "shell.execute_reply.started": "2024-09-20T21:18:28.606667Z",
          "shell.execute_reply": "2024-09-20T21:18:28.611537Z"
        },
        "trusted": true,
        "id": "B8GqJVB8YRbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(t)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:18:30.873722Z",
          "iopub.execute_input": "2024-09-20T21:18:30.874600Z",
          "iopub.status.idle": "2024-09-20T21:18:30.880154Z",
          "shell.execute_reply.started": "2024-09-20T21:18:30.874556Z",
          "shell.execute_reply": "2024-09-20T21:18:30.879215Z"
        },
        "trusted": true,
        "id": "OTwInynUYRbF",
        "outputId": "5fac134a-d671-422f-f453-e45f12fb8f05"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 24,
          "output_type": "execute_result",
          "data": {
            "text/plain": "8500"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "torch.manual_seed(42)\n",
        "t_p,te_p = torch.utils.data.random_split(t,[7840,660])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:18:31.090025Z",
          "iopub.execute_input": "2024-09-20T21:18:31.090748Z",
          "iopub.status.idle": "2024-09-20T21:18:31.098956Z",
          "shell.execute_reply.started": "2024-09-20T21:18:31.090714Z",
          "shell.execute_reply": "2024-09-20T21:18:31.097762Z"
        },
        "trusted": true,
        "id": "uMnrd2m8YRbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(t_p, shuffle=True, batch_size=32)\n",
        "eval_dataloader = DataLoader(te_p, batch_size=1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:18:31.608997Z",
          "iopub.execute_input": "2024-09-20T21:18:31.609889Z",
          "iopub.status.idle": "2024-09-20T21:18:31.614559Z",
          "shell.execute_reply.started": "2024-09-20T21:18:31.609854Z",
          "shell.execute_reply": "2024-09-20T21:18:31.613600Z"
        },
        "trusted": true,
        "id": "u0NU868NYRbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:18:33.889572Z",
          "iopub.execute_input": "2024-09-20T21:18:33.890454Z",
          "iopub.status.idle": "2024-09-20T21:18:33.894409Z",
          "shell.execute_reply.started": "2024-09-20T21:18:33.890422Z",
          "shell.execute_reply": "2024-09-20T21:18:33.893449Z"
        },
        "trusted": true,
        "id": "xYzh-k6UYRbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('h')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:18:34.252860Z",
          "iopub.execute_input": "2024-09-20T21:18:34.253643Z",
          "iopub.status.idle": "2024-09-20T21:18:34.258263Z",
          "shell.execute_reply.started": "2024-09-20T21:18:34.253610Z",
          "shell.execute_reply": "2024-09-20T21:18:34.257337Z"
        },
        "trusted": true,
        "id": "CXKZILX8YRbG",
        "outputId": "1da9afc9-c265-4f40-a35d-84d42b8f3627"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "h\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "concept_classes = [\"holocaust\", \"nazism\", \"genocide\", \"funny\", \"anti muslim\", \"terrorism\", \"violence\", \"politics\", \"racism\", \"international relation\", \"adult\", \"gore\", \"misogynistic\", \"immigration\", \"extremism\", \"immoral\", \"white supremacy\", \"indecency\"]\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:18:34.634501Z",
          "iopub.execute_input": "2024-09-20T21:18:34.634870Z",
          "iopub.status.idle": "2024-09-20T21:18:34.639751Z",
          "shell.execute_reply.started": "2024-09-20T21:18:34.634842Z",
          "shell.execute_reply": "2024-09-20T21:18:34.638596Z"
        },
        "trusted": true,
        "id": "Omxz-psGYRbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(concept_classes)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:18:37.038056Z",
          "iopub.execute_input": "2024-09-20T21:18:37.038715Z",
          "iopub.status.idle": "2024-09-20T21:18:37.044278Z",
          "shell.execute_reply.started": "2024-09-20T21:18:37.038684Z",
          "shell.execute_reply": "2024-09-20T21:18:37.043344Z"
        },
        "trusted": true,
        "id": "Lx3jXZN9YRbG",
        "outputId": "c0ee22bf-57e3-492a-f2e2-7f29c868e5a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 30,
          "output_type": "execute_result",
          "data": {
            "text/plain": "18"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cnt"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:18:41.476100Z",
          "iopub.execute_input": "2024-09-20T21:18:41.476755Z",
          "iopub.status.idle": "2024-09-20T21:18:41.480661Z",
          "shell.execute_reply.started": "2024-09-20T21:18:41.476722Z",
          "shell.execute_reply": "2024-09-20T21:18:41.479753Z"
        },
        "trusted": true,
        "id": "yu93SyQLYRbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qK-0oysOYRbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XHupK7PqYRbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FFox-bfKYRbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S_Eu_IM2YRbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X_yFOm00YRbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-07T13:26:10.133975Z",
          "iopub.execute_input": "2024-04-07T13:26:10.134397Z",
          "iopub.status.idle": "2024-04-07T13:26:10.139811Z",
          "shell.execute_reply.started": "2024-04-07T13:26:10.134362Z",
          "shell.execute_reply": "2024-04-07T13:26:10.138850Z"
        },
        "trusted": true,
        "id": "E0MeMfSOYRbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "with open('/kaggle/working/full_annotation.pkl', 'rb') as f:\n",
        "    mynewlist = pickle.load(f)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:18:46.405650Z",
          "iopub.execute_input": "2024-09-20T21:18:46.406720Z",
          "iopub.status.idle": "2024-09-20T21:18:46.411276Z",
          "shell.execute_reply.started": "2024-09-20T21:18:46.406687Z",
          "shell.execute_reply": "2024-09-20T21:18:46.410363Z"
        },
        "trusted": true,
        "id": "X9EyFFhnYRbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(mynewlist)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:18:46.752755Z",
          "iopub.execute_input": "2024-09-20T21:18:46.753277Z",
          "iopub.status.idle": "2024-09-20T21:18:46.760859Z",
          "shell.execute_reply.started": "2024-09-20T21:18:46.753234Z",
          "shell.execute_reply": "2024-09-20T21:18:46.759412Z"
        },
        "trusted": true,
        "id": "vgZLakPKYRbM",
        "outputId": "ff9792e3-8268-4ae8-d43a-871c84eb6c53"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 33,
          "output_type": "execute_result",
          "data": {
            "text/plain": "208"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "70lzeVpFYRbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "gc=mynewlist"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:18:49.249227Z",
          "iopub.execute_input": "2024-09-20T21:18:49.249597Z",
          "iopub.status.idle": "2024-09-20T21:18:49.254075Z",
          "shell.execute_reply.started": "2024-09-20T21:18:49.249569Z",
          "shell.execute_reply": "2024-09-20T21:18:49.253157Z"
        },
        "trusted": true,
        "id": "N-11hkPOYRbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "device='cuda'"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:18:49.570949Z",
          "iopub.execute_input": "2024-09-20T21:18:49.571561Z",
          "iopub.status.idle": "2024-09-20T21:18:49.575752Z",
          "shell.execute_reply.started": "2024-09-20T21:18:49.571530Z",
          "shell.execute_reply": "2024-09-20T21:18:49.574562Z"
        },
        "trusted": true,
        "id": "13vo_KE7YRbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YikCJbNOYRbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:18:52.189222Z",
          "iopub.execute_input": "2024-09-20T21:18:52.189920Z",
          "iopub.status.idle": "2024-09-20T21:18:52.195800Z",
          "shell.execute_reply.started": "2024-09-20T21:18:52.189891Z",
          "shell.execute_reply": "2024-09-20T21:18:52.194763Z"
        },
        "trusted": true,
        "id": "T8wJ51b9YRbQ",
        "outputId": "695b1bab-8514-4fae-b9eb-699d4f721cda"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 36,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'cuda'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:18:52.620753Z",
          "iopub.execute_input": "2024-09-20T21:18:52.621414Z",
          "iopub.status.idle": "2024-09-20T21:18:52.955325Z",
          "shell.execute_reply.started": "2024-09-20T21:18:52.621381Z",
          "shell.execute_reply": "2024-09-20T21:18:52.954556Z"
        },
        "trusted": true,
        "id": "E_xO7IEZYRbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(\"jew\")[1:-1]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:18:53.507722Z",
          "iopub.execute_input": "2024-09-20T21:18:53.508075Z",
          "iopub.status.idle": "2024-09-20T21:18:53.514813Z",
          "shell.execute_reply.started": "2024-09-20T21:18:53.508047Z",
          "shell.execute_reply": "2024-09-20T21:18:53.513929Z"
        },
        "trusted": true,
        "id": "iK5qWLxsYRbR",
        "outputId": "4317cd37-9c27-48a5-ab74-99a592827f22"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 38,
          "output_type": "execute_result",
          "data": {
            "text/plain": "[16522]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "DYN_ROUTE = True\n",
        "DECONFOUND = False"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:19:04.988478Z",
          "iopub.execute_input": "2024-09-20T21:19:04.989146Z",
          "iopub.status.idle": "2024-09-20T21:19:04.993600Z",
          "shell.execute_reply.started": "2024-09-20T21:19:04.989115Z",
          "shell.execute_reply": "2024-09-20T21:19:04.992496Z"
        },
        "trusted": true,
        "id": "59iiyLp8YRbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "!pip install pytorch_revgrad\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:19:08.332899Z",
          "iopub.execute_input": "2024-09-20T21:19:08.333603Z",
          "iopub.status.idle": "2024-09-20T21:19:20.992355Z",
          "shell.execute_reply.started": "2024-09-20T21:19:08.333571Z",
          "shell.execute_reply": "2024-09-20T21:19:20.991350Z"
        },
        "trusted": true,
        "id": "EhNAcsy3YRbS",
        "outputId": "3dabda2c-1a19-4142-d421-9a5bca27208e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting pytorch_revgrad\n  Downloading pytorch_revgrad-0.2.0-py3-none-any.whl.metadata (1.7 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pytorch_revgrad) (1.24.4)\nRequirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_revgrad) (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->pytorch_revgrad) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->pytorch_revgrad) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->pytorch_revgrad) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->pytorch_revgrad) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->pytorch_revgrad) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->pytorch_revgrad) (2023.12.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->pytorch_revgrad) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->pytorch_revgrad) (1.3.0)\nDownloading pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\nInstalling collected packages: pytorch_revgrad\nSuccessfully installed pytorch_revgrad-0.2.0\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.nn.Parameter(torch.tensor(0.1))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:19:25.859395Z",
          "iopub.execute_input": "2024-09-20T21:19:25.860219Z",
          "iopub.status.idle": "2024-09-20T21:19:25.864508Z",
          "shell.execute_reply.started": "2024-09-20T21:19:25.860187Z",
          "shell.execute_reply": "2024-09-20T21:19:25.863555Z"
        },
        "trusted": true,
        "id": "41Uj4SgFYRbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "num_concepts = 18\n",
        "#BS = 8\n",
        "import torch\n",
        "from torch import nn\n",
        "from pytorch_revgrad import RevGrad\n",
        "class VB(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VB, self).__init__()\n",
        "        self.vb = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(768, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 2),\n",
        "        )\n",
        "\n",
        "\n",
        "        self.linear_relu_stack_latent = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(768, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 2),\n",
        "        )\n",
        "\n",
        "\n",
        "        self.linear_relu_stack_hidden = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(768, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 2),\n",
        "        )\n",
        "\n",
        "        #self.w = torch.nn.Parameter(torch.ones(11,1)/11)\n",
        "        self.w = torch.nn.Parameter(torch.ones(num_concepts,1))\n",
        "\n",
        "        self.project_latent = nn.Linear(768,2048)\n",
        "\n",
        "        #self.W_ij = [[nn.Linear(768,768) for i in range(64)] for j in range(17)]\n",
        "\n",
        "        self.W_ij = torch.nn.Parameter(torch.randn(18,64,28,28))\n",
        "\n",
        "        self.proj_conc = nn.Linear(768,28)\n",
        "        self.proj_embed = nn.Linear(768,28)\n",
        "\n",
        "\n",
        "\n",
        "        self.grad_rev1 = RevGrad()\n",
        "        self.grad_rev2 = RevGrad()\n",
        "\n",
        "        self.w1 = torch.nn.Parameter(torch.tensor(0.2))\n",
        "\n",
        "\n",
        "    def forward(self,\n",
        "        all_concepts_,\n",
        "        input_ids = None,\n",
        "        attention_mask = None,\n",
        "        token_type_ids = None,\n",
        "        position_ids = None,\n",
        "        head_mask = None,\n",
        "        inputs_embeds = None,\n",
        "        visual_embeds = None,\n",
        "        visual_attention_mask = None,\n",
        "        visual_token_type_ids = None,\n",
        "        image_text_alignment = None,\n",
        "        output_attentions = None,\n",
        "        output_hidden_states = None,\n",
        "        return_dict = None):\n",
        "\n",
        "        #         print(all_concepts.shape)\n",
        "\n",
        "        B = visual_embeds.size(0)\n",
        "\n",
        "        #         print(B)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ##############################################\n",
        "\n",
        "        all_concepts = self.proj_conc(all_concepts_)\n",
        "\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs_embeds = self.vb.embeddings.word_embeddings(input_ids)\n",
        "\n",
        "        #print(0/0)\n",
        "\n",
        "        inputs_embeds = self.proj_embed(inputs_embeds)\n",
        "\n",
        "\n",
        "        #####################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #b1 = torch.bmm(inputs_embeds,all_concepts.repeat(B,1,1).transpose(1,2))\n",
        "\n",
        "        #print(inputs_embeds.shape)\n",
        "        #print(all_concepts.repeat(B,1,1).transpose(1,2).shape)\n",
        "\n",
        "        #b,64,28\n",
        "        #b,28,17\n",
        "\n",
        "\n",
        "        b = torch.einsum('blx,bxn->bln',inputs_embeds,all_concepts.repeat(B,1,1).transpose(1,2))\n",
        "\n",
        "        #         print(b1)\n",
        "\n",
        "        #         print('********')\n",
        "\n",
        "        #         print(b2)\n",
        "\n",
        "        #         print(torch.equal(b1,b2))\n",
        "\n",
        "\n",
        "\n",
        "        b = b.masked_fill(~attention_mask.unsqueeze(dim=2).bool(), -np.inf)\n",
        "        c = F.softmax(b,dim=1)\n",
        "\n",
        "\n",
        "\n",
        "        #inter = torch.einsum('bcsn,csnn->bcsn',inputs_embeds.unsqueeze(1).repeat(1,17,1,1), self.W_ij)\n",
        "\n",
        "        #print(inter[:,0,0,:])\n",
        "\n",
        "        einsum_expression = 'bln,clnx->bclx'\n",
        "        inter = torch.einsum(einsum_expression, inputs_embeds, self.W_ij)\n",
        "\n",
        "        k = torch.einsum('bcsn,bsc->bcn',inter,c)\n",
        "        norm = torch.norm(k,dim=2)\n",
        "\n",
        "\n",
        "\n",
        "        kk1 = ((norm**2)/(1+norm**2)).unsqueeze(dim=2) * (k/norm.unsqueeze(dim=2))\n",
        "\n",
        "        w_dynamic = kk1.norm(dim=2).unsqueeze(2)\n",
        "\n",
        "\n",
        "        raw_latent = (1-self.w1) * all_concepts_.unsqueeze(0).repeat(B,1,1) * self.w.unsqueeze(dim=0).repeat(B,1,1) + all_concepts_.unsqueeze(0).repeat(B,1,1) * w_dynamic * self.w1\n",
        "\n",
        "        raw_latent = torch.mean(raw_latent, dim=1)\n",
        "\n",
        "        #raw_latent = torch.mean(all_concepts*self.w,dim=0)\n",
        "        #raw_latent = raw_latent.unsqueeze(dim=0)\n",
        "\n",
        "        latent = self.project_latent(raw_latent)\n",
        "\n",
        "\n",
        "\n",
        "        latent = latent.unsqueeze(dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        visual_embeds_latent = torch.cat((visual_embeds,latent),dim=1)\n",
        "\n",
        "\n",
        "        ################################################################\n",
        "\n",
        "\n",
        "        #print(b)\n",
        "\n",
        "        #######################################################\n",
        "        #         print(b.shape)\n",
        "\n",
        "        #         b = []\n",
        "        #         for i in range(64):\n",
        "        #             bi = []\n",
        "        #             for j in range(17):\n",
        "        #                 #print(inputs_embeds[:,i,:].shape)\n",
        "        #                 ci = all_concepts[j].unsqueeze(0).repeat(32,1,1).squeeze()\n",
        "        #                 uj = inputs_embeds[:,i,:]\n",
        "        #                 #print(ci.shape)\n",
        "        #                 bij = torch.bmm(uj.unsqueeze(dim=1), ci.unsqueeze(dim=2))\n",
        "        #                 #print(bij.shape)\n",
        "        #                 #print(bij)\n",
        "        #                 bi.append(bij.squeeze())\n",
        "        #             bi = torch.stack(bi)\n",
        "\n",
        "        #             bi = bi.T\n",
        "        #             #print(bi.shape)\n",
        "        #             #print(bi)\n",
        "        #             b.append(bi)\n",
        "\n",
        "\n",
        "\n",
        "        #         b = torch.stack(b)\n",
        "        #         b = b.transpose(0,1)\n",
        "\n",
        "        #         print(b)\n",
        "        #         print(b.shape)\n",
        "\n",
        "\n",
        "        #print(0/0)\n",
        "\n",
        "        ####################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #c = F.softmax(b,dim=1)\n",
        "\n",
        "        #print(c)\n",
        "        #print(c.shape)\n",
        "        #print(0/0)\n",
        "        ############################################################\n",
        "\n",
        "        #print\n",
        "\n",
        "        #c = 32*64*17\n",
        "\n",
        "        #u_j_i = 32*768\n",
        "\n",
        "\n",
        "        #cij = 32*1\n",
        "\n",
        "\n",
        "        #         kk = []\n",
        "        #         for i in range(17):\n",
        "        #             si = 0\n",
        "        #             for j in range(64):\n",
        "        #                 #print(inputs_embeds[:,j,:].shape)\n",
        "        #                 #print(self.W_ij)\n",
        "        #                 #print(self.W_ij[i,j,:,:].shape)\n",
        "\n",
        "\n",
        "\n",
        "        #                 u_j_i = torch.mm(inputs_embeds[:,j,:],self.W_ij[i,j,:,:])\n",
        "\n",
        "\n",
        "        #                 #print(j,i,u_j_i)\n",
        "        #                 #print(0/0)\n",
        "\n",
        "        #                 cij = c[:,j,i].unsqueeze(dim=1)\n",
        "        #                 #print('cij ',cij)\n",
        "        #                 #print(cij.shape)\n",
        "        #                 #print(u_j_i)\n",
        "        #                 #print(u_j_i.shape)\n",
        "        #                 si+= u_j_i*cij\n",
        "\n",
        "        #                 #print(u_j_i*cij)\n",
        "        #             #print(si)\n",
        "        #             #print(0/0)\n",
        "\n",
        "        #             norm = torch.norm(si, dim=1)\n",
        "\n",
        "        #             #print(norm)\n",
        "        #             #print(norm.shape)\n",
        "\n",
        "        #             vi = ((norm**2)/(1+norm**2)).unsqueeze(dim=1) * (si/norm.unsqueeze(dim=1))\n",
        "\n",
        "        #             #print(vi)\n",
        "        #             kk.append(torch.norm(vi,dim=1))\n",
        "        #             #kk.append(vi)\n",
        "        #             #print(kk)\n",
        "\n",
        "\n",
        "        #         kk = torch.stack(kk)\n",
        "\n",
        "        #         print('kk', kk)\n",
        "        #         print('kk1', kk1)\n",
        "        #         print(kk.shape)\n",
        "        #         print(kk1.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Create a tensor of ones with the same number of columns as the original tensor\n",
        "        ones_column_vam = torch.ones((visual_attention_mask.size(0), 1), dtype=visual_attention_mask.dtype)\n",
        "\n",
        "        # Concatenate the original tensor with the tensor of ones along the second dimension (columns)\n",
        "        visual_attention_mask_latent = torch.cat((visual_attention_mask, ones_column_vam.to('cuda')), dim=1)\n",
        "\n",
        "        # Create a tensor of ones with the same number of columns as the original tensor\n",
        "        ones_column_vtt = torch.ones((visual_token_type_ids.size(0), 1), dtype=visual_token_type_ids.dtype)\n",
        "\n",
        "        # Concatenate the original tensor with the tensor of ones along the second dimension (columns)\n",
        "        visual_token_type_ids_latent = torch.cat((visual_token_type_ids, ones_column_vtt.to('cuda')), dim=1)\n",
        "\n",
        "        #         print(latent.shape)\n",
        "\n",
        "        #         print(visual_embeds.shape)\n",
        "        #         print(visual_attention_mask.shape)\n",
        "        #         print(visual_token_type_ids.shape)\n",
        "\n",
        "        #         print(self.w)\n",
        "\n",
        "        x = self.vb(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask, visual_embeds=visual_embeds_latent, visual_attention_mask=visual_attention_mask_latent, visual_token_type_ids=visual_token_type_ids_latent)\n",
        "\n",
        "        p = self.vb(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask, visual_embeds=visual_embeds, visual_attention_mask=visual_attention_mask, visual_token_type_ids=visual_token_type_ids)\n",
        "        p = self.grad_rev1(p.pooler_output)\n",
        "        #q = self.grad_rev2(raw_latent.repeat(B,1))\n",
        "        q = self.grad_rev2(raw_latent)\n",
        "\n",
        "        logits_p = self.linear_relu_stack_hidden(p)\n",
        "        logits_q = self.linear_relu_stack_latent(q)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        logits = self.linear_relu_stack(x.pooler_output)\n",
        "\n",
        "\n",
        "        if_logits_p = self.linear_relu_stack(p)\n",
        "        if_logits_q = self.linear_relu_stack(q)\n",
        "\n",
        "        return logits, logits_p, logits_q, if_logits_p, if_logits_q\n",
        "        #return logits, logits_p, logits_q, if_logits_p, if_logits_p\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:19:26.943531Z",
          "iopub.execute_input": "2024-09-20T21:19:26.943959Z",
          "iopub.status.idle": "2024-09-20T21:19:26.981134Z",
          "shell.execute_reply.started": "2024-09-20T21:19:26.943930Z",
          "shell.execute_reply": "2024-09-20T21:19:26.980277Z"
        },
        "trusted": true,
        "id": "uFGanLUdYRbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t8FL5nD0YRbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4V5CLqnHYRbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O6ABIvznYRbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cZVnyj5sYRbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jl7eAJ_AYRbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "vb = VB()\n",
        "\n",
        "# vb = nn.DataParallel(vb)\n",
        "vb.to(device)\n",
        "concept_classes = [\"holocaust\", \"nazism\", \"genocide\", \"funny\", \"anti muslim\", \"terrorism\", \"violence\", \"politics\", \"racism\", \"international relation\", \"adult\", \"gore\", \"misogynistic\", \"immigration\", \"extremism\", \"immoral\", \"white supremacy\", \"indecency\"]\n",
        "\n",
        "\n",
        "concept_embedding_map = {}\n",
        "\n",
        "all_concepts = []\n",
        "\n",
        "\n",
        "for c in concept_classes:\n",
        "    conc = torch.tensor(tokenizer.encode(c)[1:-1])\n",
        "    e = vb.vb.embeddings.word_embeddings(conc.to('cuda'))\n",
        "    x = e.detach().cpu().mean(dim=0)\n",
        "\n",
        "\n",
        "\n",
        "    all_concepts.append(x)\n",
        "\n",
        "    concept_embedding_map[c] = x\n",
        "\n",
        "\n",
        "all_concepts = torch.stack(all_concepts)\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:19:31.960952Z",
          "iopub.execute_input": "2024-09-20T21:19:31.961755Z",
          "iopub.status.idle": "2024-09-20T21:19:33.172582Z",
          "shell.execute_reply.started": "2024-09-20T21:19:31.961721Z",
          "shell.execute_reply": "2024-09-20T21:19:33.171596Z"
        },
        "trusted": true,
        "id": "Q-3sK2xYYRbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "all_concepts.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:19:33.703662Z",
          "iopub.execute_input": "2024-09-20T21:19:33.704290Z",
          "iopub.status.idle": "2024-09-20T21:19:33.710057Z",
          "shell.execute_reply.started": "2024-09-20T21:19:33.704258Z",
          "shell.execute_reply": "2024-09-20T21:19:33.709231Z"
        },
        "trusted": true,
        "id": "vIMEdCXZYRbW",
        "outputId": "b82098d7-9e9b-4d4f-8004-2cada1dacf4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 44,
          "output_type": "execute_result",
          "data": {
            "text/plain": "torch.Size([18, 768])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "vb = VB()\n",
        "vb.load_state_dict(torch.load('/kaggle/working/final_model_concept.pt'))\n",
        "vb.to('cuda')\n",
        "vb.eval()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:19:34.798173Z",
          "iopub.execute_input": "2024-09-20T21:19:34.798843Z",
          "iopub.status.idle": "2024-09-20T21:19:36.090506Z",
          "shell.execute_reply.started": "2024-09-20T21:19:34.798813Z",
          "shell.execute_reply": "2024-09-20T21:19:36.089515Z"
        },
        "trusted": true,
        "id": "4saba9tgYRbX",
        "outputId": "751aed58-c13d-4b3e-b76a-9598046a9350"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 45,
          "output_type": "execute_result",
          "data": {
            "text/plain": "VB(\n  (vb): VisualBertModel(\n    (embeddings): VisualBertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=1)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n      (visual_token_type_embeddings): Embedding(2, 768)\n      (visual_position_embeddings): Embedding(512, 768)\n      (visual_projection): Linear(in_features=2048, out_features=768, bias=True)\n    )\n    (encoder): VisualBertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x VisualBertLayer(\n          (attention): VisualBertAttention(\n            (self): VisualBertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): VisualBertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): VisualBertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): VisualBertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): VisualBertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (linear_relu_stack): Sequential(\n    (0): Dropout(p=0.1, inplace=False)\n    (1): Linear(in_features=768, out_features=512, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=512, out_features=128, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=128, out_features=2, bias=True)\n  )\n  (linear_relu_stack_latent): Sequential(\n    (0): Dropout(p=0.1, inplace=False)\n    (1): Linear(in_features=768, out_features=512, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=512, out_features=128, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=128, out_features=2, bias=True)\n  )\n  (linear_relu_stack_hidden): Sequential(\n    (0): Dropout(p=0.1, inplace=False)\n    (1): Linear(in_features=768, out_features=512, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=512, out_features=128, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=128, out_features=2, bias=True)\n  )\n  (project_latent): Linear(in_features=768, out_features=2048, bias=True)\n  (proj_conc): Linear(in_features=768, out_features=28, bias=True)\n  (proj_embed): Linear(in_features=768, out_features=28, bias=True)\n  (grad_rev1): RevGrad()\n  (grad_rev2): RevGrad()\n)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# all_concepts.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:19:42.185326Z",
          "iopub.execute_input": "2024-09-20T21:19:42.185694Z",
          "iopub.status.idle": "2024-09-20T21:19:42.190052Z",
          "shell.execute_reply.started": "2024-09-20T21:19:42.185666Z",
          "shell.execute_reply": "2024-09-20T21:19:42.189156Z"
        },
        "trusted": true,
        "id": "6u0ezW1mYRbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all_concepts[0].unsqueeze(0).shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:19:42.605705Z",
          "iopub.execute_input": "2024-09-20T21:19:42.606365Z",
          "iopub.status.idle": "2024-09-20T21:19:42.610386Z",
          "shell.execute_reply.started": "2024-09-20T21:19:42.606334Z",
          "shell.execute_reply": "2024-09-20T21:19:42.609343Z"
        },
        "trusted": true,
        "id": "e9rL3Ta2YRbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all_concepts[0].unsqueeze(0).repeat(32,1,1).squeeze().shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:19:42.931639Z",
          "iopub.execute_input": "2024-09-20T21:19:42.932003Z",
          "iopub.status.idle": "2024-09-20T21:19:42.936035Z",
          "shell.execute_reply.started": "2024-09-20T21:19:42.931975Z",
          "shell.execute_reply": "2024-09-20T21:19:42.935134Z"
        },
        "trusted": true,
        "id": "UGMXBbtHYRbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "# check evaluation performance separately on p and q using their classifier: should be less\n",
        "pred_r1, pred_r2 = [],[]\n",
        "pred_both = []\n",
        "act = []\n",
        "for ii, batch in enumerate(eval_dataloader):\n",
        "    with torch.no_grad():\n",
        "        batch_inp = {k: v.to(device) for k, v in batch[0].items()}\n",
        "        outputs, r1, r2,_,_ = vb(all_concepts.to('cuda'),**batch_inp)\n",
        "        act.append(batch[1].detach().cpu().numpy()[0])\n",
        "\n",
        "        pred_r1.append(np.argmax(r1.detach().cpu().numpy()[0]))\n",
        "        pred_r2.append(np.argmax(r2.detach().cpu().numpy()[0]))\n",
        "        pred_both.append(np.argmax(outputs.detach().cpu().numpy()[0]))\n",
        "\n",
        "print('*')\n",
        "print(f1_score(act,pred_r1,average='macro'))\n",
        "print(f1_score(act,pred_r1,average='weighted'))\n",
        "print(accuracy_score(act,pred_r1))\n",
        "print('*'*10)\n",
        "print(f1_score(act,pred_r2,average='macro'))\n",
        "print(f1_score(act,pred_r2,average='weighted'))\n",
        "print(accuracy_score(act,pred_r2))\n",
        "print('*'*10)\n",
        "print(f1_score(act,pred_both,average='macro'))\n",
        "print(f1_score(act,pred_both,average='weighted'))\n",
        "print(accuracy_score(act,pred_both))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:19:44.514402Z",
          "iopub.execute_input": "2024-09-20T21:19:44.515130Z",
          "iopub.status.idle": "2024-09-20T21:20:00.003372Z",
          "shell.execute_reply.started": "2024-09-20T21:19:44.515101Z",
          "shell.execute_reply": "2024-09-20T21:20:00.002427Z"
        },
        "trusted": true,
        "id": "hJnmX80nYRba",
        "outputId": "11aaedf1-34fc-4c45-e454-a7f6561bc7e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "*\n0.40647482014388486\n0.5567473294091998\n0.6848484848484848\n**********\n0.40647482014388486\n0.5567473294091998\n0.6848484848484848\n**********\n0.7036565354754966\n0.7469814601650707\n0.75\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check evaluation performance separately on p and q using parent classifier: should be also less\n",
        "pred_r1, pred_r2 = [],[]\n",
        "pred_both = []\n",
        "act = []\n",
        "for ii, batch in enumerate(eval_dataloader):\n",
        "    with torch.no_grad():\n",
        "        batch_inp = {k: v.to(device) for k, v in batch[0].items()}\n",
        "        outputs, _,_, r1, r2 = vb(all_concepts.to('cuda'),**batch_inp)\n",
        "        act.append(batch[1].detach().cpu().numpy()[0])\n",
        "\n",
        "        pred_r1.append(np.argmax(r1.detach().cpu().numpy()[0]))\n",
        "        pred_r2.append(np.argmax(r2.detach().cpu().numpy()[0]))\n",
        "        pred_both.append(np.argmax(outputs.detach().cpu().numpy()[0]))\n",
        "\n",
        "print('*'*10)\n",
        "print(f1_score(act,pred_r1,average='macro'))\n",
        "print(f1_score(act,pred_r1,average='weighted'))\n",
        "print(accuracy_score(act,pred_r1))\n",
        "print('*'*10)\n",
        "print(f1_score(act,pred_r2,average='macro'))\n",
        "print(f1_score(act,pred_r2,average='weighted'))\n",
        "print(accuracy_score(act,pred_r2))\n",
        "print('*'*10)\n",
        "print(f1_score(act,pred_both,average='macro'))\n",
        "print(f1_score(act,pred_both,average='weighted'))\n",
        "print(accuracy_score(act,pred_both))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:22:23.566480Z",
          "iopub.execute_input": "2024-09-20T21:22:23.566879Z",
          "iopub.status.idle": "2024-09-20T21:22:38.886507Z",
          "shell.execute_reply.started": "2024-09-20T21:22:23.566850Z",
          "shell.execute_reply": "2024-09-20T21:22:38.885395Z"
        },
        "trusted": true,
        "id": "eTbJMIiFYRbb",
        "outputId": "facf6345-3f6c-4b4c-a91a-1e2e4b893992"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "**********\n0.23963133640552994\n0.1510403574919704\n0.3151515151515151\n**********\n0.23963133640552994\n0.1510403574919704\n0.3151515151515151\n**********\n0.7036565354754966\n0.7469814601650707\n0.75\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.save(vb.state_dict(), '/kaggle/working/final_model_wo_adversarial.pt')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:22:49.607437Z",
          "iopub.execute_input": "2024-09-20T21:22:49.607808Z",
          "iopub.status.idle": "2024-09-20T21:22:49.612009Z",
          "shell.execute_reply.started": "2024-09-20T21:22:49.607780Z",
          "shell.execute_reply": "2024-09-20T21:22:49.611030Z"
        },
        "trusted": true,
        "id": "5uBbt7NcYRbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "from typing import Dict\n",
        "\n",
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "\n",
        "\n",
        "# import svm_classifier\n",
        "\n",
        "REGRESSION = False\n",
        "from typing import List\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "def get_weights(model) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        :return: final weights of the model, as np array\n",
        "        \"\"\"\n",
        "\n",
        "        w = model.coef_\n",
        "        if len(w.shape) == 1:\n",
        "                w = np.expand_dims(w, 0)\n",
        "\n",
        "        return w\n",
        "\n",
        "\n",
        "def train_network(model, X_train: np.ndarray, Y_train: np.ndarray, X_dev: np.ndarray, Y_dev: np.ndarray) -> float:\n",
        "\n",
        "        \"\"\"\n",
        "        :param X_train:\n",
        "        :param Y_train:\n",
        "        :param X_dev:\n",
        "        :param Y_dev:\n",
        "        :return: accuracy score on the dev set / Person's R in the case of regression\n",
        "        \"\"\"\n",
        "\n",
        "        model.fit(X_train, Y_train)\n",
        "        score = model.score(X_dev, Y_dev)\n",
        "        return score\n",
        "\n",
        "\n",
        "from typing import Dict\n",
        "\n",
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "\n",
        "# import svm_classifier\n",
        "\n",
        "REGRESSION = False\n",
        "from typing import List\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "\n",
        "def get_nullspace_projection(W: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    :param W: the matrix over its nullspace to project\n",
        "    :return: the projection matrix\n",
        "    \"\"\"\n",
        "    nullspace_basis = scipy.linalg.null_space(W)  # orthogonal basis\n",
        "    nullspace_basis = nullspace_basis * np.sign(nullspace_basis[0][0])  # handle sign ambiguity\n",
        "    projection_matrix = nullspace_basis.dot(nullspace_basis.T)\n",
        "\n",
        "    return projection_matrix\n",
        "\n",
        "from sklearn import svm"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:22:50.450409Z",
          "iopub.execute_input": "2024-09-20T21:22:50.450774Z",
          "iopub.status.idle": "2024-09-20T21:22:50.461101Z",
          "shell.execute_reply.started": "2024-09-20T21:22:50.450746Z",
          "shell.execute_reply": "2024-09-20T21:22:50.460105Z"
        },
        "trusted": true,
        "id": "c1hXPQt2YRbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#R\n",
        "num_concepts = 18\n",
        "BS = 8\n",
        "import torch\n",
        "from torch import nn\n",
        "from pytorch_revgrad import RevGrad\n",
        "class VB(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VB, self).__init__()\n",
        "        self.vb = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(768, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 2),\n",
        "        )\n",
        "\n",
        "\n",
        "        self.linear_relu_stack_latent = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(768, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 2),\n",
        "        )\n",
        "\n",
        "\n",
        "        self.linear_relu_stack_hidden = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(768, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 2),\n",
        "        )\n",
        "\n",
        "        #self.w = torch.nn.Parameter(torch.ones(11,1)/11)\n",
        "        self.w = torch.nn.Parameter(torch.ones(num_concepts,1))\n",
        "\n",
        "        self.project_latent = nn.Linear(768,2048)\n",
        "\n",
        "        #self.W_ij = [[nn.Linear(768,768) for i in range(64)] for j in range(17)]\n",
        "\n",
        "        self.W_ij = torch.nn.Parameter(torch.randn(18,64,28,28))\n",
        "\n",
        "        self.proj_conc = nn.Linear(768,28)\n",
        "        self.proj_embed = nn.Linear(768,28)\n",
        "\n",
        "\n",
        "\n",
        "        self.grad_rev1 = RevGrad()\n",
        "        self.grad_rev2 = RevGrad()\n",
        "\n",
        "        self.w1 = torch.nn.Parameter(torch.tensor(0.2))\n",
        "\n",
        "\n",
        "    def forward(self,\n",
        "        all_concepts_,\n",
        "        input_ids = None,\n",
        "        attention_mask = None,\n",
        "        token_type_ids = None,\n",
        "        position_ids = None,\n",
        "        head_mask = None,\n",
        "        inputs_embeds = None,\n",
        "        visual_embeds = None,\n",
        "        visual_attention_mask = None,\n",
        "        visual_token_type_ids = None,\n",
        "        image_text_alignment = None,\n",
        "        output_attentions = None,\n",
        "        output_hidden_states = None,\n",
        "        return_dict = None):\n",
        "\n",
        "        #         print(all_concepts.shape)\n",
        "\n",
        "        B = visual_embeds.size(0)\n",
        "\n",
        "        #         print(B)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ##############################################\n",
        "\n",
        "        all_concepts = self.proj_conc(all_concepts_)\n",
        "\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs_embeds = self.vb.embeddings.word_embeddings(input_ids)\n",
        "\n",
        "        #print(0/0)\n",
        "\n",
        "        inputs_embeds = self.proj_embed(inputs_embeds)\n",
        "\n",
        "\n",
        "        #####################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #b1 = torch.bmm(inputs_embeds,all_concepts.repeat(B,1,1).transpose(1,2))\n",
        "\n",
        "        #print(inputs_embeds.shape)\n",
        "        #print(all_concepts.repeat(B,1,1).transpose(1,2).shape)\n",
        "\n",
        "        #b,64,28\n",
        "        #b,28,17\n",
        "\n",
        "\n",
        "        b = torch.einsum('blx,bxn->bln',inputs_embeds,all_concepts.repeat(B,1,1).transpose(1,2))\n",
        "\n",
        "        #         print(b1)\n",
        "\n",
        "        #         print('********')\n",
        "\n",
        "        #         print(b2)\n",
        "\n",
        "        #         print(torch.equal(b1,b2))\n",
        "\n",
        "\n",
        "\n",
        "        b = b.masked_fill(~attention_mask.unsqueeze(dim=2).bool(), -np.inf)\n",
        "        c = F.softmax(b,dim=1)\n",
        "\n",
        "\n",
        "\n",
        "        #inter = torch.einsum('bcsn,csnn->bcsn',inputs_embeds.unsqueeze(1).repeat(1,17,1,1), self.W_ij)\n",
        "\n",
        "        #print(inter[:,0,0,:])\n",
        "\n",
        "        einsum_expression = 'bln,clnx->bclx'\n",
        "        inter = torch.einsum(einsum_expression, inputs_embeds, self.W_ij)\n",
        "\n",
        "        k = torch.einsum('bcsn,bsc->bcn',inter,c)\n",
        "        norm = torch.norm(k,dim=2)\n",
        "\n",
        "\n",
        "\n",
        "        kk1 = ((norm**2)/(1+norm**2)).unsqueeze(dim=2) * (k/norm.unsqueeze(dim=2))\n",
        "\n",
        "        w_dynamic = kk1.norm(dim=2).unsqueeze(2)\n",
        "\n",
        "\n",
        "\n",
        "        raw_latent = (1-self.w1) * all_concepts_.unsqueeze(0).repeat(B,1,1) * self.w.unsqueeze(dim=0).repeat(B,1,1) + all_concepts_.unsqueeze(0).repeat(B,1,1) * w_dynamic * self.w1\n",
        "\n",
        "\n",
        "        #raw_latent = 0 * all_concepts_.unsqueeze(0).repeat(B,1,1) * self.w.unsqueeze(dim=0).repeat(B,1,1) + all_concepts_.unsqueeze(0).repeat(B,1,1) * w_dynamic *1\n",
        "\n",
        "\n",
        "        raw_latent = torch.mean(raw_latent, dim=1)\n",
        "\n",
        "        #raw_latent = torch.mean(all_concepts*self.w,dim=0)\n",
        "        #raw_latent = raw_latent.unsqueeze(dim=0)\n",
        "\n",
        "        latent = self.project_latent(raw_latent)\n",
        "\n",
        "\n",
        "\n",
        "        latent = latent.unsqueeze(dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        visual_embeds_latent = torch.cat((visual_embeds,latent),dim=1)\n",
        "\n",
        "\n",
        "        ################################################################\n",
        "\n",
        "\n",
        "        #print(b)\n",
        "\n",
        "        #######################################################\n",
        "        #         print(b.shape)\n",
        "\n",
        "        #         b = []\n",
        "        #         for i in range(64):\n",
        "        #             bi = []\n",
        "        #             for j in range(17):\n",
        "        #                 #print(inputs_embeds[:,i,:].shape)\n",
        "        #                 ci = all_concepts[j].unsqueeze(0).repeat(32,1,1).squeeze()\n",
        "        #                 uj = inputs_embeds[:,i,:]\n",
        "        #                 #print(ci.shape)\n",
        "        #                 bij = torch.bmm(uj.unsqueeze(dim=1), ci.unsqueeze(dim=2))\n",
        "        #                 #print(bij.shape)\n",
        "        #                 #print(bij)\n",
        "        #                 bi.append(bij.squeeze())\n",
        "        #             bi = torch.stack(bi)\n",
        "\n",
        "        #             bi = bi.T\n",
        "        #             #print(bi.shape)\n",
        "        #             #print(bi)\n",
        "        #             b.append(bi)\n",
        "\n",
        "\n",
        "\n",
        "        #         b = torch.stack(b)\n",
        "        #         b = b.transpose(0,1)\n",
        "\n",
        "        #         print(b)\n",
        "        #         print(b.shape)\n",
        "\n",
        "\n",
        "        #print(0/0)\n",
        "\n",
        "        ####################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #c = F.softmax(b,dim=1)\n",
        "\n",
        "        #print(c)\n",
        "        #print(c.shape)\n",
        "        #print(0/0)\n",
        "        ############################################################\n",
        "\n",
        "        #print\n",
        "\n",
        "        #c = 32*64*17\n",
        "\n",
        "        #u_j_i = 32*768\n",
        "\n",
        "\n",
        "        #cij = 32*1\n",
        "\n",
        "\n",
        "        #         kk = []\n",
        "        #         for i in range(17):\n",
        "        #             si = 0\n",
        "        #             for j in range(64):\n",
        "        #                 #print(inputs_embeds[:,j,:].shape)\n",
        "        #                 #print(self.W_ij)\n",
        "        #                 #print(self.W_ij[i,j,:,:].shape)\n",
        "\n",
        "\n",
        "\n",
        "        #                 u_j_i = torch.mm(inputs_embeds[:,j,:],self.W_ij[i,j,:,:])\n",
        "\n",
        "\n",
        "        #                 #print(j,i,u_j_i)\n",
        "        #                 #print(0/0)\n",
        "\n",
        "        #                 cij = c[:,j,i].unsqueeze(dim=1)\n",
        "        #                 #print('cij ',cij)\n",
        "        #                 #print(cij.shape)\n",
        "        #                 #print(u_j_i)\n",
        "        #                 #print(u_j_i.shape)\n",
        "        #                 si+= u_j_i*cij\n",
        "\n",
        "        #                 #print(u_j_i*cij)\n",
        "        #             #print(si)\n",
        "        #             #print(0/0)\n",
        "\n",
        "        #             norm = torch.norm(si, dim=1)\n",
        "\n",
        "        #             #print(norm)\n",
        "        #             #print(norm.shape)\n",
        "\n",
        "        #             vi = ((norm**2)/(1+norm**2)).unsqueeze(dim=1) * (si/norm.unsqueeze(dim=1))\n",
        "\n",
        "        #             #print(vi)\n",
        "        #             kk.append(torch.norm(vi,dim=1))\n",
        "        #             #kk.append(vi)\n",
        "        #             #print(kk)\n",
        "\n",
        "\n",
        "        #         kk = torch.stack(kk)\n",
        "\n",
        "        #         print('kk', kk)\n",
        "        #         print('kk1', kk1)\n",
        "        #         print(kk.shape)\n",
        "        #         print(kk1.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Create a tensor of ones with the same number of columns as the original tensor\n",
        "        ones_column_vam = torch.ones((visual_attention_mask.size(0), 1), dtype=visual_attention_mask.dtype)\n",
        "\n",
        "        # Concatenate the original tensor with the tensor of ones along the second dimension (columns)\n",
        "        visual_attention_mask_latent = torch.cat((visual_attention_mask, ones_column_vam.to('cuda')), dim=1)\n",
        "\n",
        "        # Create a tensor of ones with the same number of columns as the original tensor\n",
        "        ones_column_vtt = torch.ones((visual_token_type_ids.size(0), 1), dtype=visual_token_type_ids.dtype)\n",
        "\n",
        "        # Concatenate the original tensor with the tensor of ones along the second dimension (columns)\n",
        "        visual_token_type_ids_latent = torch.cat((visual_token_type_ids, ones_column_vtt.to('cuda')), dim=1)\n",
        "\n",
        "        #         print(latent.shape)\n",
        "\n",
        "        #         print(visual_embeds.shape)\n",
        "        #         print(visual_attention_mask.shape)\n",
        "        #         print(visual_token_type_ids.shape)\n",
        "\n",
        "        #         print(self.w)\n",
        "\n",
        "        x = self.vb(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask, visual_embeds=visual_embeds_latent, visual_attention_mask=visual_attention_mask_latent, visual_token_type_ids=visual_token_type_ids_latent)\n",
        "\n",
        "        p = self.vb(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask, visual_embeds=visual_embeds, visual_attention_mask=visual_attention_mask, visual_token_type_ids=visual_token_type_ids)\n",
        "        p = self.grad_rev1(p.pooler_output)\n",
        "        #q = self.grad_rev2(raw_latent.repeat(B,1))\n",
        "        q = self.grad_rev2(raw_latent)\n",
        "\n",
        "        logits_p = self.linear_relu_stack_hidden(p)\n",
        "        logits_q = self.linear_relu_stack_latent(q)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        logits = self.linear_relu_stack(x.pooler_output)\n",
        "\n",
        "\n",
        "        if_logits_p = self.linear_relu_stack(p)\n",
        "        if_logits_q = self.linear_relu_stack(q)\n",
        "\n",
        "        return logits, logits_p, logits_q, if_logits_p, if_logits_q\n",
        "        #return logits, logits_p, logits_q, if_logits_p, if_logits_p\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:22:51.638617Z",
          "iopub.execute_input": "2024-09-20T21:22:51.638971Z",
          "iopub.status.idle": "2024-09-20T21:22:51.667812Z",
          "shell.execute_reply.started": "2024-09-20T21:22:51.638942Z",
          "shell.execute_reply": "2024-09-20T21:22:51.666801Z"
        },
        "trusted": true,
        "id": "aFVuyTi4YRbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "def get_inlp(all_concepts):\n",
        "    print('INLP step')\n",
        "    num_concepts = 18\n",
        "    all_concepts_ = all_concepts.numpy()\n",
        "    multiplier = np.logical_xor(1, np.eye(num_concepts)).astype(np.int32)/ (num_concepts-1)\n",
        "    X = np.matmul(multiplier,all_concepts_)\n",
        "    assert X.shape[0]==num_concepts\n",
        "    Y = np.arange(num_concepts)\n",
        "\n",
        "    #             x_train_cp = x_train\n",
        "    #             x_test_cp = x_test\n",
        "    x_train_cp = X\n",
        "    x_test_cp = X\n",
        "    y_train = Y\n",
        "    y_test = Y\n",
        "    print('******')\n",
        "    for i in range(1):\n",
        "\n",
        "        clf = svm.SVC(kernel=\"linear\")\n",
        "        scr = train_network(clf, x_train_cp, y_train, x_test_cp, y_test)\n",
        "        print('initial {}'.format(scr))\n",
        "        W = get_weights(clf)\n",
        "        P_i = get_nullspace_projection(W)\n",
        "        x_train_cp = x_train_cp.dot(P_i)\n",
        "        x_test_cp = x_test_cp.dot(P_i)\n",
        "        print(clf.score(x_test_cp, y_test))\n",
        "        print(clf.score(x_train_cp, y_train))\n",
        "        clf = svm.SVC(kernel=\"linear\")\n",
        "        train_network(clf, x_train_cp, y_train, x_test_cp, y_test)\n",
        "    print('******')\n",
        "    all_concepts_ = torch.tensor(all_concepts_)\n",
        "    all_concepts_ = all_concepts_.mm(torch.tensor(P_i, dtype = all_concepts_.dtype))\n",
        "\n",
        "    return all_concepts_"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:22:54.239731Z",
          "iopub.execute_input": "2024-09-20T21:22:54.240416Z",
          "iopub.status.idle": "2024-09-20T21:22:54.250104Z",
          "shell.execute_reply.started": "2024-09-20T21:22:54.240383Z",
          "shell.execute_reply": "2024-09-20T21:22:54.249166Z"
        },
        "trusted": true,
        "id": "eMzxXCm8YRbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "import torch\n",
        "from torch import nn\n",
        "from pytorch_revgrad import RevGrad\n",
        "class VB_wrapper(nn.Module):\n",
        "    def __init__(self, vb):\n",
        "        super(VB_wrapper, self).__init__()\n",
        "\n",
        "        self.vb = vb\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,\n",
        "        all_concepts_,\n",
        "        input_embeds = None,\n",
        "        visual_embeds = None,\n",
        "        attention_mask = None,\n",
        "        token_type_ids = None,\n",
        "        position_ids = None,\n",
        "        head_mask = None,\n",
        "        inputs_embeds = None,\n",
        "\n",
        "        visual_attention_mask = None,\n",
        "        visual_token_type_ids = None,\n",
        "        image_text_alignment = None,\n",
        "        output_attentions = None,\n",
        "        output_hidden_states = None,\n",
        "        return_dict = None):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #         print('ac', all_concepts_.shape)\n",
        "        #         print('ii', input_embeds.shape)\n",
        "        #         print('am', attention_mask)\n",
        "        #         print('tti', token_type_ids)\n",
        "        #         print('pi', position_ids)\n",
        "        #         print('hm', head_mask)\n",
        "        #         print('ie', inputs_embeds)\n",
        "        #         print('ve', visual_embeds)\n",
        "        #         print('vam', visual_attention_mask)\n",
        "        #         print('vtti', visual_token_type_ids)\n",
        "        #         print(image_text_alignment)\n",
        "        #         print(output_attentions)\n",
        "        #         print(output_hidden_states)\n",
        "        #         print(return_dict)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        B = visual_embeds.size(0)\n",
        "\n",
        "\n",
        "        all_concepts = self.vb.proj_conc(all_concepts_)\n",
        "\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            inputs_embeds = self.vb.vb.embeddings.word_embeddings(input_ids.to('cuda'))\n",
        "        \"\"\"\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs_embeds = self.vb.vb.embeddings.word_embeddings(input_ids.to('cuda'))\n",
        "\n",
        "        #print(0/0)\n",
        "\n",
        "        inputs_embeds = self.vb.proj_embed(inputs_embeds)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        b = torch.einsum('blx,bxn->bln',inputs_embeds,all_concepts.transpose(1,2))\n",
        "\n",
        "\n",
        "        #print(b.shape)\n",
        "\n",
        "\n",
        "\n",
        "        b = b.masked_fill(~attention_mask.unsqueeze(dim=2).bool(), -np.inf)\n",
        "        c = F.softmax(b,dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        einsum_expression = 'bln,clnx->bclx'\n",
        "        inter = torch.einsum(einsum_expression, inputs_embeds, self.vb.W_ij)\n",
        "\n",
        "        k = torch.einsum('bcsn,bsc->bcn',inter,c)\n",
        "        norm = torch.norm(k,dim=2)\n",
        "\n",
        "\n",
        "\n",
        "        kk1 = ((norm**2)/(1+norm**2)).unsqueeze(dim=2) * (k/norm.unsqueeze(dim=2))\n",
        "\n",
        "        w_dynamic = kk1.norm(dim=2).unsqueeze(2)\n",
        "\n",
        "        num_reps = int(all_concepts.shape[0])\n",
        "        w_static = self.vb.w.unsqueeze(0).repeat(num_reps,1,1)\n",
        "\n",
        "\n",
        "        #         print('*'*10)\n",
        "\n",
        "        #         print(self.vb.w1.shape)\n",
        "        #         print(all_concepts_.shape)\n",
        "        #         print(w_static.shape)\n",
        "        #         print(w_dynamic.shape)\n",
        "\n",
        "\n",
        "\n",
        "        if DYN_ROUTE:\n",
        "            raw_latent = (1-self.vb.w1) * all_concepts_ * w_static + all_concepts_ * w_dynamic * self.vb.w1\n",
        "\n",
        "        else:\n",
        "            raw_latent = (1-self.vb.w1) * all_concepts_ * w_static\n",
        "\n",
        "\n",
        "\n",
        "        #raw_latent = (1-self.vb.w1) * all_concepts_ * w_static\n",
        "        raw_latent = torch.mean(raw_latent, dim=1)\n",
        "\n",
        "\n",
        "        latent = self.vb.project_latent(raw_latent)\n",
        "\n",
        "\n",
        "\n",
        "        latent = latent.unsqueeze(dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        visual_embeds_latent = torch.cat((visual_embeds,latent),dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #         num_reps = int(all_concepts.shape[0]/17)\n",
        "        #         w_static = self.vb.w.unsqueeze(0).repeat(num_reps,1,1).reshape(17*num_reps,1)\n",
        "        #         raw_latent = torch.mean(all_concepts*w_static,dim=0)\n",
        "        #         raw_latent = raw_latent.unsqueeze(dim=0)\n",
        "\n",
        "        #         latent = self.vb.project_latent(raw_latent)\n",
        "\n",
        "\n",
        "\n",
        "        #         latent = latent.repeat(B,1).unsqueeze(dim=1)\n",
        "\n",
        "\n",
        "        #         visual_embeds_latent = torch.cat((visual_embeds,latent),dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Create a tensor of ones with the same number of columns as the original tensor\n",
        "        ones_column_vam = torch.ones((visual_attention_mask.size(0), 1), dtype=visual_attention_mask.dtype)\n",
        "\n",
        "        # Concatenate the original tensor with the tensor of ones along the second dimension (columns)\n",
        "        visual_attention_mask_latent = torch.cat((visual_attention_mask, ones_column_vam.to('cuda')), dim=1)\n",
        "\n",
        "        # Create a tensor of ones with the same number of columns as the original tensor\n",
        "        ones_column_vtt = torch.ones((visual_token_type_ids.size(0), 1), dtype=visual_token_type_ids.dtype)\n",
        "\n",
        "        # Concatenate the original tensor with the tensor of ones along the second dimension (columns)\n",
        "        visual_token_type_ids_latent = torch.cat((visual_token_type_ids, ones_column_vtt.to('cuda')), dim=1)\n",
        "\n",
        "        #         print(latent.shape)\n",
        "\n",
        "        #         print(visual_embeds.shape)\n",
        "        #         print(visual_attention_mask.shape)\n",
        "        #         print(visual_token_type_ids.shape)\n",
        "\n",
        "        #         print(self.w)\n",
        "\n",
        "        x = self.vb.vb(inputs_embeds = input_embeds, token_type_ids = token_type_ids, attention_mask = attention_mask, visual_embeds=visual_embeds_latent, visual_attention_mask=visual_attention_mask_latent, visual_token_type_ids=visual_token_type_ids_latent)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        logits = self.vb.linear_relu_stack(x.pooler_output)\n",
        "        #print('softmax logit', torch.softmax(logits, dim=1))\n",
        "        #print(0/0)\n",
        "\n",
        "        #logits = torch.softmax(logits, dim=1)[:, 1].unsqueeze(1)\n",
        "\n",
        "        #print('previous',torch.softmax(logits, dim=1)[:, 1].unsqueeze(1) )\n",
        "\n",
        "        logits = torch.softmax(logits, dim=1).max(dim=1).values.unsqueeze(1)\n",
        "\n",
        "        #print('logits', logits)\n",
        "\n",
        "        #print(0/0)\n",
        "\n",
        "        #print(logits)\n",
        "        #print(logits.shape)\n",
        "\n",
        "\n",
        "\n",
        "        return logits\n",
        "\n",
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:22:54.945968Z",
          "iopub.execute_input": "2024-09-20T21:22:54.946366Z",
          "iopub.status.idle": "2024-09-20T21:22:54.967779Z",
          "shell.execute_reply.started": "2024-09-20T21:22:54.946336Z",
          "shell.execute_reply": "2024-09-20T21:22:54.966898Z"
        },
        "trusted": true,
        "id": "ByYpMZK7YRbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "vb = VB()\n",
        "\n",
        "vb.load_state_dict(torch.load('/kaggle/working/final_model_concept.pt'))\n",
        "vb.to('cuda')\n",
        "vb.eval()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:22:55.749708Z",
          "iopub.execute_input": "2024-09-20T21:22:55.750438Z",
          "iopub.status.idle": "2024-09-20T21:22:57.063330Z",
          "shell.execute_reply.started": "2024-09-20T21:22:55.750407Z",
          "shell.execute_reply": "2024-09-20T21:22:57.062373Z"
        },
        "trusted": true,
        "id": "RsJgQRlKYRbd",
        "outputId": "04c51d3e-d433-4af9-8d87-f0e8a05c347e"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 56,
          "output_type": "execute_result",
          "data": {
            "text/plain": "VB(\n  (vb): VisualBertModel(\n    (embeddings): VisualBertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=1)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n      (visual_token_type_embeddings): Embedding(2, 768)\n      (visual_position_embeddings): Embedding(512, 768)\n      (visual_projection): Linear(in_features=2048, out_features=768, bias=True)\n    )\n    (encoder): VisualBertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x VisualBertLayer(\n          (attention): VisualBertAttention(\n            (self): VisualBertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): VisualBertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): VisualBertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): VisualBertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): VisualBertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (linear_relu_stack): Sequential(\n    (0): Dropout(p=0.1, inplace=False)\n    (1): Linear(in_features=768, out_features=512, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=512, out_features=128, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=128, out_features=2, bias=True)\n  )\n  (linear_relu_stack_latent): Sequential(\n    (0): Dropout(p=0.1, inplace=False)\n    (1): Linear(in_features=768, out_features=512, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=512, out_features=128, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=128, out_features=2, bias=True)\n  )\n  (linear_relu_stack_hidden): Sequential(\n    (0): Dropout(p=0.1, inplace=False)\n    (1): Linear(in_features=768, out_features=512, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=512, out_features=128, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=128, out_features=2, bias=True)\n  )\n  (project_latent): Linear(in_features=768, out_features=2048, bias=True)\n  (proj_conc): Linear(in_features=768, out_features=28, bias=True)\n  (proj_embed): Linear(in_features=768, out_features=28, bias=True)\n  (grad_rev1): RevGrad()\n  (grad_rev2): RevGrad()\n)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "vb_c = VB()\n",
        "vb_c.to(device)\n",
        "# concept_classes = [\"jew\", \"holocaust\", \"hitler\", \"genocide\", \"funny\", \"muslim\", \"terrorism\", \"violence\", \"politics\", \"naive\", \"international relation\"]\n",
        "\n",
        "\n",
        "# concept_embedding_map = {}\n",
        "\n",
        "# all_concepts = []\n",
        "\n",
        "\n",
        "# for c in concept_classes:\n",
        "#     conc = torch.tensor(tokenizer.encode(c)[1:-1])\n",
        "#     e = vb_c.vb.embeddings.word_embeddings(conc.to('cuda'))\n",
        "#     x = e.detach().cpu().mean(dim=0)\n",
        "\n",
        "#     all_concepts.append(x)\n",
        "\n",
        "#     concept_embedding_map[c] = x\n",
        "\n",
        "\n",
        "# all_concepts = torch.stack(all_concepts)\n",
        "\n",
        "# concept_classes = [\"jew\", \"holocaust\", \"hitler\", \"genocide\", \"funny\", \"muslim\", \"terrorism\", \"violence\", \"politics\", \"racism\", \"international relation\", \"sex\", \"gore\", \"violence\", \"immigration\", \"extremism\", \"immoral\"]\n",
        "\n",
        "concept_classes = [\"holocaust\", \"nazism\", \"genocide\", \"funny\", \"anti muslim\", \"terrorism\", \"violence\", \"politics\", \"racism\", \"international relation\", \"adult\", \"gore\", \"misogynistic\", \"immigration\", \"extremism\", \"immoral\", \"white supremacy\", \"indecency\"]\n",
        "\n",
        "\n",
        "concept_embedding_map = {}\n",
        "\n",
        "all_concepts = []\n",
        "\n",
        "\n",
        "for c in concept_classes:\n",
        "    conc = torch.tensor(tokenizer.encode(c)[1:-1])\n",
        "    e = vb_c.vb.embeddings.word_embeddings(conc.to('cuda'))\n",
        "    x = e.detach().cpu().mean(dim=0)\n",
        "\n",
        "\n",
        "    all_concepts.append(x)\n",
        "\n",
        "    concept_embedding_map[c] = x\n",
        "\n",
        "\n",
        "all_concepts = torch.stack(all_concepts)\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:23:00.759283Z",
          "iopub.execute_input": "2024-09-20T21:23:00.759648Z",
          "iopub.status.idle": "2024-09-20T21:23:01.694085Z",
          "shell.execute_reply.started": "2024-09-20T21:23:00.759619Z",
          "shell.execute_reply": "2024-09-20T21:23:01.693278Z"
        },
        "trusted": true,
        "id": "FsYziHc1YRbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "def get_inlp(all_concepts):\n",
        "\n",
        "    num_concepts = 18\n",
        "    all_concepts_ = all_concepts.cpu().numpy()\n",
        "    multiplier = np.logical_xor(1, np.eye(num_concepts)).astype(np.int32)/ (num_concepts-1)\n",
        "    X = np.matmul(multiplier,all_concepts_)\n",
        "    assert X.shape[0]==num_concepts\n",
        "    Y = np.arange(num_concepts)\n",
        "\n",
        "    #             x_train_cp = x_train\n",
        "    #             x_test_cp = x_test\n",
        "    x_train_cp = X\n",
        "    x_test_cp = X\n",
        "    y_train = Y\n",
        "    y_test = Y\n",
        "\n",
        "    for i in range(1):\n",
        "\n",
        "        clf = svm.SVC(kernel=\"linear\")\n",
        "        scr = train_network(clf, x_train_cp, y_train, x_test_cp, y_test)\n",
        "\n",
        "        W = get_weights(clf)\n",
        "        P_i = get_nullspace_projection(W)\n",
        "        x_train_cp = x_train_cp.dot(P_i)\n",
        "        x_test_cp = x_test_cp.dot(P_i)\n",
        "\n",
        "        clf = svm.SVC(kernel=\"linear\")\n",
        "        train_network(clf, x_train_cp, y_train, x_test_cp, y_test)\n",
        "\n",
        "    all_concepts_ = torch.tensor(all_concepts_)\n",
        "    all_concepts_ = all_concepts_.mm(torch.tensor(P_i, dtype = all_concepts_.dtype))\n",
        "\n",
        "    return all_concepts_"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:23:02.718535Z",
          "iopub.execute_input": "2024-09-20T21:23:02.718910Z",
          "iopub.status.idle": "2024-09-20T21:23:02.727847Z",
          "shell.execute_reply.started": "2024-09-20T21:23:02.718882Z",
          "shell.execute_reply": "2024-09-20T21:23:02.726858Z"
        },
        "trusted": true,
        "id": "UI1gIJqXYRbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "#torch.save(vb.state_dict(), '/kaggle/working/final_model_concept_inlp.pt')\n",
        "# check evaluation performance separately on p and q using their classifier: should be less\n",
        "pred_r1, pred_r2 = [],[]\n",
        "pred_both = []\n",
        "act = []\n",
        "if DECONFOUND:\n",
        "    all_concepts_ = get_inlp(all_concepts).to('cuda')\n",
        "else:\n",
        "    all_concepts_ = all_concepts.to('cuda')\n",
        "\n",
        "for ii, batch in enumerate(eval_dataloader):\n",
        "    with torch.no_grad():\n",
        "        batch_inp = {k: v.to(device) for k, v in batch[0].items()}\n",
        "        outputs, r1, r2,_,_ = vb(all_concepts_.to('cuda'),**batch_inp)\n",
        "        act.append(batch[1].detach().cpu().numpy()[0])\n",
        "\n",
        "        pred_r1.append(np.argmax(r1.detach().cpu().numpy()[0]))\n",
        "        pred_r2.append(np.argmax(r2.detach().cpu().numpy()[0]))\n",
        "        pred_both.append(np.argmax(outputs.detach().cpu().numpy()[0]))\n",
        "\n",
        "print('*')\n",
        "print(f1_score(act,pred_r1,average='macro'))\n",
        "print(f1_score(act,pred_r1,average='weighted'))\n",
        "print(accuracy_score(act,pred_r1))\n",
        "print('*'*10)\n",
        "print(f1_score(act,pred_r2,average='macro'))\n",
        "print(f1_score(act,pred_r2,average='weighted'))\n",
        "print(accuracy_score(act,pred_r2))\n",
        "print('*'*10)\n",
        "print(f1_score(act,pred_both,average='macro'))\n",
        "print(f1_score(act,pred_both,average='weighted'))\n",
        "print(accuracy_score(act,pred_both))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:23:04.015373Z",
          "iopub.execute_input": "2024-09-20T21:23:04.015728Z",
          "iopub.status.idle": "2024-09-20T21:23:19.497344Z",
          "shell.execute_reply.started": "2024-09-20T21:23:04.015700Z",
          "shell.execute_reply": "2024-09-20T21:23:19.496317Z"
        },
        "trusted": true,
        "id": "QhlMSnKiYRbn",
        "outputId": "5b4042d7-6c5d-49cf-89e1-af94ce0d8d41"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "*\n0.40647482014388486\n0.5567473294091998\n0.6848484848484848\n**********\n0.40647482014388486\n0.5567473294091998\n0.6848484848484848\n**********\n0.7036565354754966\n0.7469814601650707\n0.75\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('h')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:23:19.499357Z",
          "iopub.execute_input": "2024-09-20T21:23:19.499835Z",
          "iopub.status.idle": "2024-09-20T21:23:19.504678Z",
          "shell.execute_reply.started": "2024-09-20T21:23:19.499795Z",
          "shell.execute_reply": "2024-09-20T21:23:19.503749Z"
        },
        "trusted": true,
        "id": "zerlcvbwYRbp",
        "outputId": "9ff035ce-4f7c-47f0-d1eb-c327402324ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "h\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "def model_pred(input_ids, attention_mask, token_type_ids, visual_embeds, visual_attention_mask, visual_token_type_ids, all_concepts, concept_no=None):\n",
        "\n",
        "\n",
        "\n",
        "    B = visual_embeds.size(0)\n",
        "\n",
        "\n",
        "    #######################################################\n",
        "    input_ids = input_ids.to('cuda')\n",
        "    attention_mask = attention_mask.to('cuda')\n",
        "    token_type_ids = token_type_ids.to('cuda')\n",
        "    visual_embeds = visual_embeds.to('cuda')\n",
        "    visual_attention_mask = visual_attention_mask.to('cuda')\n",
        "    visual_token_type_ids = visual_token_type_ids.to('cuda')\n",
        "\n",
        "    #####################################################################\n",
        "\n",
        "    ##############################################\n",
        "\n",
        "    all_concepts = all_concepts.to('cuda')\n",
        "\n",
        "    # deconfounding step by passing the concepts through INLP (1)\n",
        "    if DECONFOUND:\n",
        "        all_concepts_ = get_inlp(all_concepts).to('cuda')\n",
        "    else:\n",
        "        all_concepts_ = all_concepts.to('cuda')\n",
        "\n",
        "    # this is without deconfounding while testing (2)\n",
        "    # either use (1) or (2) or mean of (1) and (2)\n",
        "\n",
        "    # all_concepts_ = all_concepts.to('cuda')\n",
        "\n",
        "    #all_concepts_ = (get_inlp(all_concepts).to('cuda') + all_concepts.to('cuda')) / 2\n",
        "\n",
        "    all_concepts = vb.proj_conc(all_concepts_)\n",
        "\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        inputs_embeds = vb.vb.embeddings.word_embeddings(input_ids.to('cuda'))\n",
        "\n",
        "    #print(0/0)\n",
        "\n",
        "    inputs_embeds = vb.proj_embed(inputs_embeds)\n",
        "\n",
        "\n",
        "    #####################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #B, 64, 28, #B, 28, 17\n",
        "\n",
        "    b = torch.einsum('blx,bxn->bln',inputs_embeds,all_concepts.repeat(B,1,1).transpose(1,2))\n",
        "\n",
        "\n",
        "\n",
        "    b = b.masked_fill(~attention_mask.unsqueeze(dim=2).bool(), -np.inf)\n",
        "    c = F.softmax(b,dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    einsum_expression = 'bln,clnx->bclx'\n",
        "    inter = torch.einsum(einsum_expression, inputs_embeds, vb.W_ij)\n",
        "\n",
        "    k = torch.einsum('bcsn,bsc->bcn',inter,c)\n",
        "    norm = torch.norm(k,dim=2)\n",
        "\n",
        "\n",
        "\n",
        "    kk1 = ((norm**2)/(1+norm**2)).unsqueeze(dim=2) * (k/norm.unsqueeze(dim=2))\n",
        "\n",
        "    w_dynamic = kk1.norm(dim=2).unsqueeze(2)\n",
        "\n",
        "    #     print(w_dynamic.shape)\n",
        "    #     print(vb.w.shape)\n",
        "\n",
        "\n",
        "    #     print(w_dynamic)\n",
        "    #     print(vb.w)\n",
        "    #     print('*'*10)\n",
        "\n",
        "\n",
        "    #print(concept_no)\n",
        "    if concept_no==None:\n",
        "        k_static = vb.w.data.clone()\n",
        "        k_dynamic = w_dynamic.data.clone()\n",
        "    else:\n",
        "        k_static = vb.w.data.clone()\n",
        "        k_dynamic = w_dynamic.data.clone()\n",
        "        k_static[concept_no][0] = 0\n",
        "        k_dynamic[0][concept_no][0] = 0\n",
        "\n",
        "    #     print(k_dynamic)\n",
        "    #     print(k_static)\n",
        "    #     print('*'*10)\n",
        "\n",
        "    #     print(0/0)\n",
        "\n",
        "    #either use the following line or raw_latent = (1-vb.w1) * all_concepts_.unsqueeze(0).repeat(B,1,1) * k_static.unsqueeze(dim=0).repeat(B,1,1)\n",
        "\n",
        "    if DYN_ROUTE:\n",
        "        raw_latent = (1-vb.w1) * all_concepts_.unsqueeze(0).repeat(B,1,1) * k_static.unsqueeze(dim=0).repeat(B,1,1) + all_concepts_.unsqueeze(0).repeat(B,1,1) * k_dynamic * vb.w1\n",
        "    else:\n",
        "        raw_latent = (1-vb.w1) * all_concepts_.unsqueeze(0).repeat(B,1,1) * k_static.unsqueeze(dim=0).repeat(B,1,1)\n",
        "\n",
        "    #raw_latent = 0.0 * all_concepts_.unsqueeze(0).repeat(B,1,1) * k_static.unsqueeze(dim=0).repeat(B,1,1) + all_concepts_.unsqueeze(0).repeat(B,1,1) * k_dynamic * 1.0\n",
        "\n",
        "    #raw_latent = (1-vb.w1) * all_concepts_.unsqueeze(0).repeat(B,1,1) * k_static.unsqueeze(dim=0).repeat(B,1,1)\n",
        "\n",
        "    raw_latent = torch.mean(raw_latent, dim=1)\n",
        "\n",
        "\n",
        "\n",
        "    latent = vb.project_latent(raw_latent)\n",
        "\n",
        "\n",
        "\n",
        "    latent = latent.unsqueeze(dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    visual_embeds_latent = torch.cat((visual_embeds,latent),dim=1)\n",
        "\n",
        "\n",
        "    ################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Create a tensor of ones with the same number of columns as the original tensor\n",
        "    ones_column_vam = torch.ones((visual_attention_mask.size(0), 1), dtype=visual_attention_mask.dtype)\n",
        "\n",
        "    # Concatenate the original tensor with the tensor of ones along the second dimension (columns)\n",
        "    visual_attention_mask_latent = torch.cat((visual_attention_mask.to('cuda'), ones_column_vam.to('cuda')), dim=1)\n",
        "\n",
        "    # Create a tensor of ones with the same number of columns as the original tensor\n",
        "    ones_column_vtt = torch.ones((visual_token_type_ids.size(0), 1), dtype=visual_token_type_ids.dtype)\n",
        "\n",
        "    # Concatenate the original tensor with the tensor of ones along the second dimension (columns)\n",
        "    visual_token_type_ids_latent = torch.cat((visual_token_type_ids.to('cuda'), ones_column_vtt.to('cuda')), dim=1)\n",
        "\n",
        "\n",
        "\n",
        "    x = vb.vb(input_ids = input_ids.to('cuda'), token_type_ids = token_type_ids.to('cuda'), attention_mask = attention_mask.to('cuda'), visual_embeds=visual_embeds_latent.to('cuda'), visual_attention_mask=visual_attention_mask_latent.to('cuda'), visual_token_type_ids=visual_token_type_ids_latent.to('cuda'))\n",
        "\n",
        "    p = vb.vb(input_ids = input_ids.to('cuda'), token_type_ids = token_type_ids.to('cuda'), attention_mask = attention_mask.to('cuda'), visual_embeds=visual_embeds.to('cuda'), visual_attention_mask=visual_attention_mask.to('cuda'), visual_token_type_ids=visual_token_type_ids.to('cuda'))\n",
        "    p = vb.grad_rev1(p.pooler_output)\n",
        "    q = vb.grad_rev2(raw_latent.repeat(B,1))\n",
        "\n",
        "    logits_p = vb.linear_relu_stack_hidden(p)\n",
        "    logits_q = vb.linear_relu_stack_latent(q.to('cuda'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    logits = vb.linear_relu_stack(x.pooler_output)\n",
        "\n",
        "\n",
        "    if_logits_p = vb.linear_relu_stack(p)\n",
        "    if_logits_q = vb.linear_relu_stack(q.to('cuda'))\n",
        "\n",
        "    return torch.softmax(logits,dim=-1), x.pooler_output"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:24:21.129274Z",
          "iopub.execute_input": "2024-09-20T21:24:21.129939Z",
          "iopub.status.idle": "2024-09-20T21:24:21.152715Z",
          "shell.execute_reply.started": "2024-09-20T21:24:21.129909Z",
          "shell.execute_reply": "2024-09-20T21:24:21.151822Z"
        },
        "trusted": true,
        "id": "CcumGdoUYRbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:24:24.124552Z",
          "iopub.execute_input": "2024-09-20T21:24:24.125505Z",
          "iopub.status.idle": "2024-09-20T21:24:24.129620Z",
          "shell.execute_reply.started": "2024-09-20T21:24:24.125470Z",
          "shell.execute_reply": "2024-09-20T21:24:24.128567Z"
        },
        "trusted": true,
        "id": "xRy24NDNYRbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(50):\n",
        "    if train_set[i]['label']==1:\n",
        "        print(i, train_set[i])\n",
        "        Image.open('/kaggle/input/facebook-hateful-meme-dataset/data/'+train_set[i]['img'])\n",
        "        inputs = {}\n",
        "        inputs = tokenizer(train_set[i]['text'], padding=\"max_length\", truncation=True, max_length=64, return_tensors='pt')\n",
        "\n",
        "\n",
        "        inputs['input_ids'] = inputs['input_ids']\n",
        "        inputs['token_type_ids'] = inputs['token_type_ids']\n",
        "        inputs['attention_mask'] = inputs['attention_mask']\n",
        "\n",
        "        input_ids = inputs['input_ids']\n",
        "        token_type_ids = inputs['token_type_ids']\n",
        "        attention_mask = inputs['attention_mask']\n",
        "\n",
        "        visual_embeds,visual_token_type_ids,visual_attention_mask = get_visual_embedding('/kaggle/input/facebook-hateful-meme-dataset/data/'+train_set[i]['img'])\n",
        "        # visual_embeds,visual_token_type_ids,visual_attention_mask = visual_feats[train_set[874]['img']]\n",
        "\n",
        "\n",
        "\n",
        "        inputs.update({\n",
        "          \"visual_embeds\":  visual_embeds,\n",
        "          \"visual_token_type_ids\": visual_token_type_ids,\n",
        "          \"visual_attention_mask\": visual_attention_mask\n",
        "        })\n",
        "\n",
        "\n",
        "        class_map = {0:'non offensive', 1: 'offensive'}\n",
        "        #print('original model prediction {}'.format(model_pred(input_ids,attention_mask,token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids)))\n",
        "        orig,_ = model_pred(input_ids,attention_mask,token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids,all_concepts)\n",
        "        orig = orig[0]\n",
        "        print(orig)\n",
        "        print('predicted class {}'.format(class_map[np.argmax(orig.detach().cpu().numpy())]))\n",
        "        pred_class = np.argmax(orig.detach().cpu().numpy())\n",
        "        orig = orig[pred_class]\n",
        "\n",
        "        k = []\n",
        "        for i in range(18):\n",
        "            #print('Removing concept: {}'.format(concept_classes[i]))\n",
        "\n",
        "            after_intervention,_ = model_pred(input_ids,attention_mask,token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids,all_concepts,concept_no=i)\n",
        "            #print('hello')\n",
        "            after_intervention = after_intervention[0][pred_class]\n",
        "            #print(after_intervention, orig)\n",
        "            #print(0/0)\n",
        "            k.append(abs(after_intervention-orig).item())\n",
        "            #print('model prediction after removal {}'.format(model_pred(input_ids,attention_mask,token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids,concept_no=i)))\n",
        "        k = np.asarray(k)\n",
        "        print(k)\n",
        "        k = list(map(lambda x: ((x-np.mean(k))/np.mean(k))*100, k))\n",
        "\n",
        "        conc_scr = []\n",
        "        for i in range(17):\n",
        "            cc = concept_classes[i]\n",
        "            print('relative ICaCE scores for concept {} is {}%'.format(cc,k[i]))\n",
        "            conc_scr.append((cc,k[i]))\n",
        "        conc_scr.sort(key=lambda tup: tup[1], reverse=True)\n",
        "\n",
        "        for i in conc_scr:\n",
        "            if i[1]> 0:\n",
        "                print(i[0], end = \" \")\n",
        "        print(\"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:24:24.927166Z",
          "iopub.execute_input": "2024-09-20T21:24:24.927523Z",
          "iopub.status.idle": "2024-09-20T21:24:29.170429Z",
          "shell.execute_reply.started": "2024-09-20T21:24:24.927499Z",
          "shell.execute_reply": "2024-09-20T21:24:29.169446Z"
        },
        "trusted": true,
        "id": "pgFR4f6RYRbs",
        "outputId": "163a2e84-d476-4a72-e4c7-09d69c611f7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "10 {'text': 'jew mad? get fuhrerious!', 'img': 'img/79351.png', 'label': 1}\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:3526.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "tensor([0.2218, 0.7782], device='cuda:0', grad_fn=<SelectBackward0>)\npredicted class offensive\n[0.00049394 0.00036418 0.00048065 0.0003106  0.00023121 0.00040692\n 0.000395   0.0002991  0.00050139 0.00040191 0.00026208 0.00041997\n 0.00047863 0.00031585 0.00046337 0.0003497  0.00031334 0.00055778]\nrelative ICaCE scores for concept holocaust is 26.19156388000609%\nrelative ICaCE scores for concept nazism is -6.959037612303944%\nrelative ICaCE scores for concept genocide is 22.795797167656463%\nrelative ICaCE scores for concept funny is -20.64869803563271%\nrelative ICaCE scores for concept anti muslim is -40.93193238921882%\nrelative ICaCE scores for concept terrorism is 3.95918988883813%\nrelative ICaCE scores for concept violence is 0.9136592051164915%\nrelative ICaCE scores for concept politics is -23.58763514542409%\nrelative ICaCE scores for concept racism is 28.095020557332116%\nrelative ICaCE scores for concept international relation is 2.680067001675042%\nrelative ICaCE scores for concept adult is -33.044007918379776%\nrelative ICaCE scores for concept gore is 7.294045987513324%\nrelative ICaCE scores for concept misogynistic is 22.278056951423785%\nrelative ICaCE scores for concept immigration is -19.308664534795188%\nrelative ICaCE scores for concept extremism is 18.37977767626009%\nrelative ICaCE scores for concept immoral is -10.659357393025735%\nrelative ICaCE scores for concept white supremacy is -19.948225978376733%\nracism holocaust genocide misogynistic extremism gore terrorism international relation violence \n\n12 {'text': 'brother... a day without a blast is a day wasted', 'img': 'img/25489.png', 'label': 1}\ntensor([0.8936, 0.1064], device='cuda:0', grad_fn=<SelectBackward0>)\npredicted class non offensive\n[1.82867050e-04 1.47521496e-04 1.70648098e-04 1.34289265e-04\n 1.06394291e-04 2.16722488e-04 1.40547752e-04 1.39772892e-04\n 1.80363655e-04 1.39236450e-04 8.20159912e-05 1.92165375e-04\n 1.68621540e-04 1.91211700e-04 1.62065029e-04 1.75297260e-04\n 1.38342381e-04 1.78813934e-04]\nrelative ICaCE scores for concept holocaust is 15.62087808554739%\nrelative ICaCE scores for concept nazism is -6.726964386659128%\nrelative ICaCE scores for concept genocide is 7.8952327115130965%\nrelative ICaCE scores for concept funny is -15.093273035613342%\nrelative ICaCE scores for concept anti muslim is -32.730356133408705%\nrelative ICaCE scores for concept terrorism is 37.02656868287168%\nrelative ICaCE scores for concept violence is -11.13623516110797%\nrelative ICaCE scores for concept politics is -11.62615413604673%\nrelative ICaCE scores for concept racism is 14.038062935745241%\nrelative ICaCE scores for concept international relation is -11.965328811004333%\nrelative ICaCE scores for concept adult is -48.143960806482006%\nrelative ICaCE scores for concept gore is 21.499905784812512%\nrelative ICaCE scores for concept misogynistic is 6.613906161673262%\nrelative ICaCE scores for concept immigration is 20.896928584887885%\nrelative ICaCE scores for concept extremism is 2.4684379121914453%\nrelative ICaCE scores for concept immoral is 10.834746561145657%\nrelative ICaCE scores for concept white supremacy is -12.530619935933673%\nterrorism gore immigration holocaust racism immoral genocide misogynistic extremism \n\n27 {'text': \"is bribing muslims for liberal votes justin trudeau's only skill? why does justin trudeau love foreigners so much while openly disrespecting canadians, canadian values, our history and traditions, our seniors and veterans? sharia law has no place in canada! never has... never will.\", 'img': 'img/72640.png', 'label': 1}\ntensor([0.1697, 0.8303], device='cuda:0', grad_fn=<SelectBackward0>)\npredicted class offensive\n[2.86102295e-05 1.12652779e-05 2.16364861e-05 8.40425491e-06\n 0.00000000e+00 2.83122063e-05 1.88350677e-05 4.03523445e-05\n 4.76837158e-05 3.53455544e-05 2.90870667e-05 8.13603401e-05\n 1.66893005e-06 2.10404396e-05 2.74777412e-05 2.83122063e-05\n 2.00867653e-05 3.26633453e-05]\nrelative ICaCE scores for concept holocaust is 6.811719619235994%\nrelative ICaCE scores for concept nazism is -57.94288539992583%\nrelative ICaCE scores for concept genocide is -19.223637037952777%\nrelative ICaCE scores for concept funny is -68.62405736184942%\nrelative ICaCE scores for concept anti muslim is -100.0%\nrelative ICaCE scores for concept terrorism is 5.699097539868951%\nrelative ICaCE scores for concept violence is -29.68228458400297%\nrelative ICaCE scores for concept politics is 50.64902954629743%\nrelative ICaCE scores for concept racism is 78.01953269872666%\nrelative ICaCE scores for concept international relation is 31.956978612931135%\nrelative ICaCE scores for concept adult is 8.59191494622326%\nrelative ICaCE scores for concept gore is 203.74582766720235%\nrelative ICaCE scores for concept misogynistic is -93.76931635554456%\nrelative ICaCE scores for concept immigration is -21.448881196686862%\nrelative ICaCE scores for concept extremism is 2.5837557176412353%\nrelative ICaCE scores for concept immoral is 5.699097539868951%\nrelative ICaCE scores for concept white supremacy is -25.009271850661396%\ngore racism politics international relation adult holocaust terrorism immoral extremism \n\n30 {'text': \"d.j. osama spin laden droppin' beats like the twin towers\", 'img': 'img/93547.png', 'label': 1}\ntensor([0.2787, 0.7213], device='cuda:0', grad_fn=<SelectBackward0>)\npredicted class offensive\n[0.00095087 0.00075227 0.00111985 0.00071597 0.00068808 0.00103629\n 0.0007     0.00073165 0.00101686 0.000745   0.00065243 0.00102443\n 0.00089788 0.00092971 0.0009405  0.00095934 0.00065583 0.00111824]\nrelative ICaCE scores for concept holocaust is 9.469149686445693%\nrelative ICaCE scores for concept nazism is -13.394964069915934%\nrelative ICaCE scores for concept genocide is 28.922859920324807%\nrelative ICaCE scores for concept funny is -17.573909231267745%\nrelative ICaCE scores for concept anti muslim is -20.785315365114457%\nrelative ICaCE scores for concept terrorism is 19.302365476621624%\nrelative ICaCE scores for concept violence is -19.41291958141928%\nrelative ICaCE scores for concept politics is -15.76920877570859%\nrelative ICaCE scores for concept racism is 17.06536034919849%\nrelative ICaCE scores for concept international relation is -14.232125497969994%\nrelative ICaCE scores for concept adult is -24.888778758363035%\nrelative ICaCE scores for concept gore is 17.936831671844924%\nrelative ICaCE scores for concept misogynistic is 3.3688504279206355%\nrelative ICaCE scores for concept immigration is 7.033147170386754%\nrelative ICaCE scores for concept extremism is 8.275165354630888%\nrelative ICaCE scores for concept immoral is 10.443550692869266%\nrelative ICaCE scores for concept white supremacy is -24.49764596000991%\ngenocide terrorism gore racism immoral holocaust extremism immigration misogynistic \n\n48 {'text': 'we said we would never forget why are you voting them into our government?', 'img': 'img/74386.png', 'label': 1}\ntensor([0.7774, 0.2226], device='cuda:0', grad_fn=<SelectBackward0>)\npredicted class non offensive\n[7.18474388e-04 2.65836716e-05 1.73926353e-04 1.88648701e-04\n 1.94072723e-04 1.76846981e-04 1.14023685e-04 1.39415264e-04\n 4.39047813e-04 2.36213207e-04 3.18527222e-04 3.37660313e-04\n 1.16407871e-04 3.44276428e-04 6.19888306e-06 2.52783298e-04\n 9.79900360e-05 6.12735748e-05]\nrelative ICaCE scores for concept holocaust is 228.03967222037437%\nrelative ICaCE scores for concept nazism is -87.86247769949502%\nrelative ICaCE scores for concept genocide is -20.589035711045934%\nrelative ICaCE scores for concept funny is -13.867134347313359%\nrelative ICaCE scores for concept anti muslim is -11.390644371201358%\nrelative ICaCE scores for concept terrorism is -19.25554110852409%\nrelative ICaCE scores for concept violence is -47.93928215052463%\nrelative ICaCE scores for concept politics is -36.34604336125306%\nrelative ICaCE scores for concept racism is 100.45961718726375%\nrelative ICaCE scores for concept international relation is 7.849777750899577%\nrelative ICaCE scores for concept adult is 45.43255420156632%\nrelative ICaCE scores for concept gore is 54.1683045568625%\nrelative ICaCE scores for concept misogynistic is -46.85071512805782%\nrelative ICaCE scores for concept immigration is 57.18907804420791%\nrelative ICaCE scores for concept extremism is -97.16972574158629%\nrelative ICaCE scores for concept immoral is 15.415318557043934%\nrelative ICaCE scores for concept white supremacy is -55.259895376613954%\nholocaust racism immigration gore adult immoral international relation \n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from captum.attr import IntegratedGradients\n",
        "\n",
        "# # input1 = torch.tensor([[1.0, 3.0], [3.0, 5.0]], requires_grad=True)\n",
        "# # input2 = torch.tensor([[1.0, 4.0], [0.0, 2.0]], requires_grad=True)\n",
        "# # # Initializing our toy model\n",
        "# # model = ToyModel_With_Additional_Forward_Args()\n",
        "# # # Applying integrated gradients on the input\n",
        "# ig = IntegratedGradients(vb)\n",
        "# (input1_attr, input2_attr), delta = ig.attribute((input1, input2), n_steps=100,\n",
        "#                                     additional_forward_args=1, return_convergence_delta=True)\n",
        "# output\n",
        "# .........\n",
        "# input1_attr: tensor([[0.0000, 0.0000],\n",
        "#                      [0.0000, 3.3428]], grad_fn=<MulBackward0>)\n",
        "# input2_attr:  tensor([[ 0.0000,  0.0000],\n",
        "#                       [0.0000, -1.3371]], grad_fn=<MulBackward0>)\n",
        "# approximation_error (aka delta): 0.005693793296813965"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:27:19.055486Z",
          "iopub.execute_input": "2024-09-20T21:27:19.055904Z",
          "iopub.status.idle": "2024-09-20T21:27:19.061115Z",
          "shell.execute_reply.started": "2024-09-20T21:27:19.055874Z",
          "shell.execute_reply": "2024-09-20T21:27:19.060036Z"
        },
        "trusted": true,
        "id": "FQsupHumYRbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "concept_classes"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:27:19.456818Z",
          "iopub.execute_input": "2024-09-20T21:27:19.457216Z",
          "iopub.status.idle": "2024-09-20T21:27:19.465897Z",
          "shell.execute_reply.started": "2024-09-20T21:27:19.457187Z",
          "shell.execute_reply": "2024-09-20T21:27:19.464996Z"
        },
        "trusted": true,
        "id": "CLUF_hTAYRbt",
        "outputId": "a4257966-cb1c-4cbe-c558-48d18b19f788"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 65,
          "output_type": "execute_result",
          "data": {
            "text/plain": "['holocaust',\n 'nazism',\n 'genocide',\n 'funny',\n 'anti muslim',\n 'terrorism',\n 'violence',\n 'politics',\n 'racism',\n 'international relation',\n 'adult',\n 'gore',\n 'misogynistic',\n 'immigration',\n 'extremism',\n 'immoral',\n 'white supremacy',\n 'indecency']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vb"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:27:21.861388Z",
          "iopub.execute_input": "2024-09-20T21:27:21.862183Z",
          "iopub.status.idle": "2024-09-20T21:27:21.870559Z",
          "shell.execute_reply.started": "2024-09-20T21:27:21.862148Z",
          "shell.execute_reply": "2024-09-20T21:27:21.869599Z"
        },
        "trusted": true,
        "id": "q8kGLzngYRbt",
        "outputId": "83c265ab-9619-4b56-9b4a-546e507475d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 66,
          "output_type": "execute_result",
          "data": {
            "text/plain": "VB(\n  (vb): VisualBertModel(\n    (embeddings): VisualBertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=1)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n      (visual_token_type_embeddings): Embedding(2, 768)\n      (visual_position_embeddings): Embedding(512, 768)\n      (visual_projection): Linear(in_features=2048, out_features=768, bias=True)\n    )\n    (encoder): VisualBertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x VisualBertLayer(\n          (attention): VisualBertAttention(\n            (self): VisualBertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): VisualBertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): VisualBertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): VisualBertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): VisualBertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (linear_relu_stack): Sequential(\n    (0): Dropout(p=0.1, inplace=False)\n    (1): Linear(in_features=768, out_features=512, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=512, out_features=128, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=128, out_features=2, bias=True)\n  )\n  (linear_relu_stack_latent): Sequential(\n    (0): Dropout(p=0.1, inplace=False)\n    (1): Linear(in_features=768, out_features=512, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=512, out_features=128, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=128, out_features=2, bias=True)\n  )\n  (linear_relu_stack_hidden): Sequential(\n    (0): Dropout(p=0.1, inplace=False)\n    (1): Linear(in_features=768, out_features=512, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=512, out_features=128, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=128, out_features=2, bias=True)\n  )\n  (project_latent): Linear(in_features=768, out_features=2048, bias=True)\n  (proj_conc): Linear(in_features=768, out_features=28, bias=True)\n  (proj_embed): Linear(in_features=768, out_features=28, bias=True)\n  (grad_rev1): RevGrad()\n  (grad_rev2): RevGrad()\n)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "baselines = (torch.randn(20,18,768).to('cuda'), torch.randn(20,64,768).to('cuda'), torch.randn(20,36,2048).to('cuda'))\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:27:26.343985Z",
          "iopub.execute_input": "2024-09-20T21:27:26.344882Z",
          "iopub.status.idle": "2024-09-20T21:27:26.378483Z",
          "shell.execute_reply.started": "2024-09-20T21:27:26.344850Z",
          "shell.execute_reply": "2024-09-20T21:27:26.377668Z"
        },
        "trusted": true,
        "id": "iHwroBAHYRbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "from captum.attr import IntegratedGradients,Saliency,DeepLift,DeepLiftShap, GradientShap, InputXGradient, KernelShap, FeatureAblation\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:27:26.783121Z",
          "iopub.execute_input": "2024-09-20T21:27:26.783621Z",
          "iopub.status.idle": "2024-09-20T21:27:26.856167Z",
          "shell.execute_reply.started": "2024-09-20T21:27:26.783578Z",
          "shell.execute_reply": "2024-09-20T21:27:26.855270Z"
        },
        "trusted": true,
        "id": "fGJKvff1YRbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.manual_seed(42)\n",
        "# print(torch.randn(2,3))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:27:27.322819Z",
          "iopub.execute_input": "2024-09-20T21:27:27.323206Z",
          "iopub.status.idle": "2024-09-20T21:27:27.327296Z",
          "shell.execute_reply.started": "2024-09-20T21:27:27.323176Z",
          "shell.execute_reply": "2024-09-20T21:27:27.326404Z"
        },
        "trusted": true,
        "id": "xKI9pBf8YRbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "#vb_wrapper = VB_wrapper(vb)\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:27:29.689843Z",
          "iopub.execute_input": "2024-09-20T21:27:29.690601Z",
          "iopub.status.idle": "2024-09-20T21:27:29.694703Z",
          "shell.execute_reply.started": "2024-09-20T21:27:29.690568Z",
          "shell.execute_reply": "2024-09-20T21:27:29.693598Z"
        },
        "trusted": true,
        "id": "hgNkeGWkYRbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "!pip install shutup"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:27:31.710072Z",
          "iopub.execute_input": "2024-09-20T21:27:31.710798Z",
          "iopub.status.idle": "2024-09-20T21:27:44.442574Z",
          "shell.execute_reply.started": "2024-09-20T21:27:31.710761Z",
          "shell.execute_reply": "2024-09-20T21:27:44.441394Z"
        },
        "trusted": true,
        "id": "tYjeTyU7YRbu",
        "outputId": "1c179dec-e03e-40ed-e51e-fbed1d6cc520"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting shutup\n  Downloading shutup-0.2.0-py3-none-any.whl.metadata (530 bytes)\nDownloading shutup-0.2.0-py3-none-any.whl (1.5 kB)\nInstalling collected packages: shutup\nSuccessfully installed shutup-0.2.0\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(gc)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:27:44.444995Z",
          "iopub.execute_input": "2024-09-20T21:27:44.445865Z",
          "iopub.status.idle": "2024-09-20T21:27:44.452643Z",
          "shell.execute_reply.started": "2024-09-20T21:27:44.445827Z",
          "shell.execute_reply": "2024-09-20T21:27:44.451386Z"
        },
        "trusted": true,
        "id": "qXbvMWiIYRbu",
        "outputId": "e20fb786-4237-4023-b094-300b01c5afb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 72,
          "output_type": "execute_result",
          "data": {
            "text/plain": "208"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def call_sim():\n",
        "\n",
        "    import math\n",
        "    # Quantitative case study (1) Faithfulness on concepts\n",
        "    # Quantitative case study (2) Are they explainable?\n",
        "    test_dataloader=DataLoader(t_p, shuffle=True, batch_size=1)\n",
        "\n",
        "    pred_r1, pred_r2 = [],[]\n",
        "    pred_both = []\n",
        "    act = []\n",
        "\n",
        "    multimodal_repr = []\n",
        "    concept_repr = []\n",
        "\n",
        "    concept_repr1 = []\n",
        "    concept_repr2 = []\n",
        "    y_p = []\n",
        "\n",
        "    cr, rr = [],[]\n",
        "    cr1,rr1 = [],[]\n",
        "\n",
        "\n",
        "    ours_icace = []\n",
        "    ia_icace = []\n",
        "\n",
        "    concept_cat = {}\n",
        "    for i,c in enumerate(concept_classes):\n",
        "        concept_cat[c] = all_concepts[i]\n",
        "\n",
        "    for ii, batch in tqdm(enumerate(eval_dataloader)):\n",
        "\n",
        "        #if batch[1].detach().cpu().numpy()[0] == 1:\n",
        "\n",
        "        ll = batch[1].detach().cpu().numpy()[0]\n",
        "        y_p.append(ll)\n",
        "\n",
        "        #print('*'*10)\n",
        "\n",
        "        input_ids = batch[0]['input_ids'].to('cuda')\n",
        "        #print(tokenizer.batch_decode(batch[0]['input_ids'], skip_special_tokens=True))\n",
        "        token_type_ids = batch[0]['token_type_ids'].to('cuda')\n",
        "        attention_mask = batch[0]['attention_mask'].to('cuda')\n",
        "        visual_embeds = batch[0]['visual_embeds'].to('cuda')\n",
        "        visual_token_type_ids = batch[0]['visual_token_type_ids'].to('cuda')\n",
        "        visual_attention_mask = batch[0]['visual_attention_mask'].to('cuda')\n",
        "\n",
        "        inputs_embeds = vb_wrapper.vb.vb.embeddings.word_embeddings(input_ids)\n",
        "\n",
        "        #     (input1_attr, input2_attr), delta = ig.attribute(inputs = (all_concepts.to('cuda'), inputs_embeds), additional_forward_args = (attention_mask,\n",
        "        #         token_type_ids,\n",
        "        #         None,\n",
        "        #         None,\n",
        "        #         None,\n",
        "        #         visual_embeds,\n",
        "        #         visual_attention_mask,\n",
        "        #         visual_token_type_ids), return_convergence_delta=True)\n",
        "\n",
        "        #print(visual_embeds)\n",
        "\n",
        "        #     (input1_attr, input2_attr, input3_attr) = ig.attribute(inputs = (all_concepts.unsqueeze(0).to('cuda'), inputs_embeds, visual_embeds), additional_forward_args = (attention_mask,\n",
        "        #         token_type_ids,\n",
        "        #         None,\n",
        "        #         None,\n",
        "        #         None,\n",
        "        #         visual_attention_mask,\n",
        "        #         visual_token_type_ids))\n",
        "        #vb(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask, visual_embeds=visual_embeds_latent, visual_attention_mask=visual_attention_mask_latent, visual_token_type_ids=visual_token_type_ids_latent)\n",
        "\n",
        "        if type(ig) in [captum.attr._core.kernel_shap.KernelShap, captum.attr._core.input_x_gradient.InputXGradient, captum.attr._core.deep_lift.DeepLift, captum.attr._core.saliency.Saliency, captum.attr._core.integrated_gradients.IntegratedGradients, captum.attr._core.feature_ablation.FeatureAblation]:\n",
        "                baseline = None\n",
        "                #print('none')\n",
        "        else:\n",
        "            baseline = baselines\n",
        "            #print('baseline')\n",
        "\n",
        "        all_conc = get_inlp(all_concepts).to('cuda')\n",
        "        if type(ig) in [captum.attr._core.input_x_gradient.InputXGradient,captum.attr._core.saliency.Saliency]:\n",
        "            #print('ip_grad')\n",
        "            (input1_attr, input2_attr, input3_attr) = ig.attribute(inputs = (all_conc.unsqueeze(0), inputs_embeds, visual_embeds),\n",
        "\n",
        "                                                                additional_forward_args = (attention_mask,\n",
        "            token_type_ids,\n",
        "            None,\n",
        "            None,\n",
        "            None,\n",
        "            visual_attention_mask,\n",
        "            visual_token_type_ids))\n",
        "\n",
        "        else:\n",
        "            #print('others')\n",
        "\n",
        "            (input1_attr, input2_attr, input3_attr) = ig.attribute(inputs = (all_conc.unsqueeze(0), inputs_embeds, visual_embeds),\n",
        "                                                                  baselines = baseline,\n",
        "                                                                    additional_forward_args = (attention_mask,\n",
        "                token_type_ids,\n",
        "                None,\n",
        "                None,\n",
        "                None,\n",
        "                visual_attention_mask,\n",
        "                visual_token_type_ids))\n",
        "\n",
        "\n",
        "        #print(input1_attr, input2_attr)\n",
        "        #print(input1_attr.shape, input2_attr.shape)\n",
        "        k1=input1_attr.sum(dim=-1).squeeze(0).detach().cpu().numpy()\n",
        "\n",
        "        conc_scr = []\n",
        "        attribution_score = {}\n",
        "        for i in range(18):\n",
        "            cc = concept_classes[i]\n",
        "            #print('relative attribution scores for concept {} is {}'.format(cc,k[i]))\n",
        "            conc_scr.append((cc,k1[i]))\n",
        "            attribution_score[cc] = k1[i]\n",
        "            #conc_scr[cc] = k1[i]\n",
        "        conc_scr.sort(key=lambda tup: tup[1], reverse=True)\n",
        "\n",
        "        d = {}\n",
        "\n",
        "        for cnt,i in enumerate(conc_scr):\n",
        "            d[i[0]] = cnt\n",
        "\n",
        "        #print(d)\n",
        "\n",
        "\n",
        "        orig,pooled_op = model_pred(input_ids,attention_mask,token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids,all_concepts)\n",
        "        orig = orig[0]\n",
        "        pooled_op = pooled_op.detach().cpu()\n",
        "        multimodal_repr.append(pooled_op)\n",
        "        #print('predicted class {}'.format(class_map[np.argmax(orig.detach().cpu().numpy())]))\n",
        "        #print('actual class {}'.format(ll))\n",
        "        #print(0/0)\n",
        "        #orig = orig[0]\n",
        "        k = []\n",
        "        pred_class = np.argmax(orig.detach().cpu().numpy())\n",
        "        causal_score = {}\n",
        "        #print(pred_class)\n",
        "        for i in range(18):\n",
        "            #print('Removing concept: {}'.format(concept_classes[i]))\n",
        "            cc = concept_classes[i]\n",
        "            after_intervention,_ = model_pred(input_ids,attention_mask,token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids,all_concepts,concept_no=i)\n",
        "            after_intervention = after_intervention[0][pred_class]\n",
        "            #print('hello')\n",
        "            #print(orig[pred_class], after_intervention)\n",
        "            #print(0/0)\n",
        "            k.append((cc,abs(after_intervention-orig[pred_class]).item()))\n",
        "            causal_score[cc] = abs(after_intervention-orig[pred_class]).item()\n",
        "            #print('model prediction after removal {}'.format(model_pred(input_ids,attention_mask,token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids,concept_no=i)))\n",
        "        #k = np.asarray(k)\n",
        "        #k = np.asarray(k)\n",
        "        #k = list(map(lambda x: ((x-np.mean(k))/np.mean(k))*100, k))\n",
        "        k.sort(key=lambda tup: tup[1], reverse=True)\n",
        "        #print(k)\n",
        "\n",
        "        #print(list(map(lambda x: x[0], list(k))))\n",
        "\n",
        "        gamma = 0.9\n",
        "        temp = []\n",
        "        for cnt,i in enumerate(list(map(lambda x: x[0], list(k)))):\n",
        "            temp.append((gamma**cnt) * concept_cat[i])\n",
        "            #temp.append((math.e**(-cnt)) * concept_cat[i])\n",
        "            #temp.append((1.1**(-cnt)) * concept_cat[i])\n",
        "\n",
        "        temp = torch.stack(temp).mean(dim=0).unsqueeze(dim=0)\n",
        "        #print(temp)\n",
        "        #print(temp.shape)\n",
        "\n",
        "        concept_repr.append(temp)\n",
        "\n",
        "\n",
        "\n",
        "        temp = []\n",
        "        for cnt,i in enumerate(list(map(lambda x: x[0], list(conc_scr)))):\n",
        "            temp.append((gamma**cnt) * concept_cat[i])\n",
        "            #temp.append((math.e**(-cnt)) * concept_cat[i])\n",
        "            #temp.append((1.1**(-cnt)) * concept_cat[i])\n",
        "\n",
        "        temp = torch.stack(temp).mean(dim=0).unsqueeze(dim=0)\n",
        "        #print(temp)\n",
        "        #print(temp.shape)\n",
        "\n",
        "        concept_repr1.append(temp)\n",
        "\n",
        "\n",
        "        #print(list(map(lambda x: x[0], list(conc_scr))))\n",
        "        #print(0/0)\n",
        "\n",
        "        #considering the k to be the causally sorted rank, we estimate ranks of\n",
        "\n",
        "        ip_attrib_causal = []\n",
        "\n",
        "\n",
        "\n",
        "        for i in range(18):\n",
        "            cc = concept_classes[i]\n",
        "            ip_attrib_causal.append((cc,((2*attribution_score[cc]*causal_score[cc]) / (attribution_score[cc]+causal_score[cc]))))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ip_attrib_causal.sort(key=lambda tup: tup[1], reverse=True)\n",
        "\n",
        "\n",
        "\n",
        "        ra_icace_ours = []\n",
        "        for cnt,i in enumerate(list(map(lambda x: x[0], list(k)))):\n",
        "            ra_icace_ours.append((gamma**cnt) * causal_score[i])\n",
        "            #ra_icace_ours.append((math.e**(-cnt)) * causal_score[i])\n",
        "            #ra_icace_ours.append((1.1**(-cnt)) * causal_score[i])\n",
        "\n",
        "        ra_icace_ia = []\n",
        "        for cnt,i in enumerate(list(map(lambda x: x[0], list(conc_scr)))):\n",
        "            ra_icace_ia.append((gamma**cnt) * causal_score[i])\n",
        "            #ra_icace_ia.append((math.e**(-cnt)) * causal_score[i])\n",
        "            #ra_icace_ia.append((1.1**(-cnt)) * causal_score[i])\n",
        "\n",
        "\n",
        "\n",
        "        ours_icace.append(np.mean(ra_icace_ours))\n",
        "        ia_icace.append(np.mean(ra_icace_ia))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        temp = []\n",
        "        for cnt,i in enumerate(list(map(lambda x: x[0], list(ip_attrib_causal)))):\n",
        "            temp.append((gamma**cnt) * concept_cat[i])\n",
        "            #temp.append((math.e**(-cnt)) * concept_cat[i])\n",
        "            #temp.append((1.1**(-cnt)) * concept_cat[i])\n",
        "\n",
        "        temp = torch.stack(temp).mean(dim=0).unsqueeze(dim=0)\n",
        "\n",
        "        concept_repr2.append(temp)\n",
        "\n",
        "\n",
        "\n",
        "        d1 = {}\n",
        "\n",
        "        for cnt,i in enumerate(ip_attrib_causal):\n",
        "            d1[i[0]] = cnt\n",
        "\n",
        "        r1,r2,r3 = [],[],[]\n",
        "        for cnt,i in enumerate(list(map(lambda x: x[0], list(k)))):\n",
        "            r1.append(cnt)\n",
        "            r2.append(d[i])\n",
        "            r3.append(d1[i])\n",
        "\n",
        "        #print(r1,r2)\n",
        "        corr, _ = stats.kendalltau(r1, r2)\n",
        "        res,_ = stats.spearmanr(r1, r2)\n",
        "\n",
        "        corr1, _ = stats.kendalltau(r1, r3)\n",
        "        res1,_ = stats.spearmanr(r1, r3)\n",
        "\n",
        "        #print('kendall tau', corr)\n",
        "        cr.append(corr)\n",
        "        #print('spearmans rho', res)\n",
        "        rr.append(res)\n",
        "\n",
        "        cr1.append(corr1)\n",
        "        rr1.append(res1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    multimodal_repr = torch.stack(multimodal_repr).squeeze(1)\n",
        "\n",
        "    #causal\n",
        "    concept_repr = torch.stack(concept_repr).squeeze(1)\n",
        "\n",
        "\n",
        "    #attribution\n",
        "    concept_repr1 = torch.stack(concept_repr1).squeeze(1)\n",
        "\n",
        "    #HM\n",
        "    concept_repr2 = torch.stack(concept_repr2).squeeze(1)\n",
        "\n",
        "\n",
        "    return np.mean(cr), np.mean(rr), multimodal_repr, concept_repr, concept_repr1, y_p\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:27:44.453976Z",
          "iopub.execute_input": "2024-09-20T21:27:44.454354Z",
          "iopub.status.idle": "2024-09-20T21:27:44.497427Z",
          "shell.execute_reply.started": "2024-09-20T21:27:44.454329Z",
          "shell.execute_reply": "2024-09-20T21:27:44.496438Z"
        },
        "trusted": true,
        "id": "sFQlOvvIYRbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /kaggle/working"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:27:44.500612Z",
          "iopub.execute_input": "2024-09-20T21:27:44.500909Z",
          "iopub.status.idle": "2024-09-20T21:27:45.607676Z",
          "shell.execute_reply.started": "2024-09-20T21:27:44.500884Z",
          "shell.execute_reply": "2024-09-20T21:27:45.606672Z"
        },
        "trusted": true,
        "id": "BCHRQBvxYRbv",
        "outputId": "60788513-b3fa-44b9-de9b-138a11f099ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "25461.png\t   cr1.pt\t\t\t  mean_comparison_plot.png\n28076.png\t   cr2.pt\t\t\t  mr.pt\n94162.png\t   final_model_concept.pt\t  rr.npy\n96704.png\t   final_model_concept_inlp.pt\t  rr1.npy\ncausal_vb.pt\t   final_model_wo_adversarial.pt  transformers\ncausal_vb_inlp.pt  foo1.png\t\t\t  vs_tensors.pt\ncr.npy\t\t   full_annotation.pkl\t\t  y_p.pt\ncr.pt\t\t   image1.jpg\ncr1.npy\t\t   image2.jpg\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#clf = svm.SVC(kernel='poly', C=2, random_state=42)\n",
        "\n",
        "def get_sim(multimodal_repr, concept_repr1, y_p):\n",
        "\n",
        "    X = np.concatenate([multimodal_repr.numpy(),concept_repr1.numpy()],axis=-1) # inp w/ clip representation w/ exp -> w/ both\n",
        "    #X = (multimodal_repr.numpy()*concept_repr.numpy())\n",
        "    X_ = multimodal_repr.numpy() # inp w/ clip representation w/o exp -> w/ only input\n",
        "    X__ = concept_repr1.numpy() # inp w/o clip representation w/ exp -> w/ only exp\n",
        "\n",
        "    print(X.shape)\n",
        "    print(X_.shape)\n",
        "    print(X__.shape)\n",
        "    print(y_p)\n",
        "    #print(0/0)\n",
        "\n",
        "\n",
        "\n",
        "    kf = KFold(n_splits=5)\n",
        "    y = np.array(y_p)\n",
        "    vals = []\n",
        "    vals_lus = []\n",
        "    k1,k2,k3 = [],[],[]\n",
        "    C,S = [],[]\n",
        "    for train, test in kf.split(X):\n",
        "\n",
        "        X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]\n",
        "        X_train_, X_test_, y_train, y_test = X_[train], X_[test], y[train], y[test]\n",
        "        X_train__, X_test__, y_train, y_test = X__[train], X__[test], y[train], y[test]\n",
        "\n",
        "\n",
        "        clf1 = make_pipeline(StandardScaler(),svm.SVC(probability=True, kernel='rbf', C=2, random_state=42))\n",
        "        clf1.fit(X_train, y_train)\n",
        "        y_pred = clf1.predict(X_test)\n",
        "\n",
        "        clf2 = make_pipeline(StandardScaler(),svm.SVC(probability=True,kernel='rbf', C=2, random_state=42))\n",
        "        clf2.fit(X_train_, y_train)\n",
        "        y_pred_ = clf2.predict(X_test_)\n",
        "\n",
        "        clf3 = make_pipeline(StandardScaler(), svm.SVC(probability=True,kernel='rbf', C=2, random_state=42))\n",
        "        clf3.fit(X_train__, y_train)\n",
        "        y_pred__ = clf3.predict(X_test__)\n",
        "\n",
        "\n",
        "        y_pred_proba_w_both = clf1.predict_proba(X_test)\n",
        "        y_pred_proba_w_inp = clf2.predict_proba(X_test_)\n",
        "        y_pred_proba_w_exp = clf3.predict_proba(X_test__)\n",
        "\n",
        "\n",
        "\n",
        "        comprehensiveness, sufficiency = 0,0\n",
        "\n",
        "        counter = 0\n",
        "        for pred_proba_w_both, pred_proba_w_inp, pred_proba_w_exp, pred_class in zip(y_pred_proba_w_both,y_pred_proba_w_inp,y_pred_proba_w_exp,y_pred):\n",
        "            comprehensiveness += (pred_proba_w_both[pred_class] - pred_proba_w_inp[pred_class])\n",
        "            sufficiency += (pred_proba_w_both[pred_class] - pred_proba_w_exp[pred_class])\n",
        "            counter+=1\n",
        "\n",
        "\n",
        "        comprehensiveness = comprehensiveness / counter\n",
        "        sufficiency = sufficiency / counter\n",
        "\n",
        "        C.append(comprehensiveness)\n",
        "        S.append(sufficiency)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        l, nl = 0,0\n",
        "        us = 0\n",
        "        for i ,j in zip(y_pred__,y_test):\n",
        "            if i==j:\n",
        "                l+=1\n",
        "            else:\n",
        "                nl+=1\n",
        "        a,b=0,0\n",
        "        for i,j,k,x in zip(y_pred,y_pred_,y_test,y_pred__):\n",
        "            us+=int(i==k) - int(j==k)\n",
        "            if k==x:\n",
        "                a += int(i==k) - int(j==k)\n",
        "            elif k!=x:\n",
        "                b += int(i==k) - int(j==k)\n",
        "\n",
        "        #print('LAS ',.5*(a/l)+.5*(b/nl))\n",
        "        #print('Leakage Unadjusted Simulatability (LUS)', (us/(l+nl)))\n",
        "        vals_lus.append(us/(l+nl))\n",
        "        #print('Comprehensiveness ', comprehensiveness)\n",
        "        #print('Sufficiency ', sufficiency)\n",
        "        #print('f1 between prediction of proposed model and SVM w/ both inp and exp {}'.format(f1_score(y_test, y_pred, average='macro')))\n",
        "        #print('f1 between prediction of proposed model and SVM w/ only inp {}'.format(f1_score(y_test, y_pred_, average='macro')))\n",
        "        #print('f1 between prediction of proposed model and SVM w/ only exp {}'.format(f1_score(y_test, y_pred__, average='macro')))\n",
        "        vals.append(.5*(a/l)+.5*(b/nl))\n",
        "        k1.append(f1_score(y_test, y_pred, average='macro'))\n",
        "        k2.append(f1_score(y_test, y_pred_, average='macro'))\n",
        "        k3.append(f1_score(y_test, y_pred__, average='macro'))\n",
        "\n",
        "\n",
        "    #print('Intra mean ', np.mean(ID), 'Intra std ', np.std(ID))\n",
        "    #print('Inter mean ', np.mean(ID1), 'Inter std ', np.std(ID1))\n",
        "    print('LAS mean ', np.mean(vals), 'LAS std ', np.std(vals))\n",
        "    print('LUS mean ', np.mean(vals_lus), 'LUS std ', np.std(vals_lus))\n",
        "    print('Comprehensiveness mean ', np.mean(C), 'Comprehensiveness std ', np.std(C))\n",
        "    print('Sufficiency mean ', np.mean(S), 'Sufficiency std ', np.std(S))\n",
        "    print('Avg. f1 between prediction of proposed model and SVM w/ both inp and exp {}'.format(np.mean(k1)))\n",
        "    print('Avg. f1 between prediction of proposed model and SVM w/ only inp {}'.format(np.mean(k2)))\n",
        "    print('Avg. f1 between prediction of proposed model and SVM w/ only exp {}'.format(np.mean(k3)))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:27:45.609952Z",
          "iopub.execute_input": "2024-09-20T21:27:45.610294Z",
          "iopub.status.idle": "2024-09-20T21:27:45.637715Z",
          "shell.execute_reply.started": "2024-09-20T21:27:45.610266Z",
          "shell.execute_reply": "2024-09-20T21:27:45.636712Z"
        },
        "trusted": true,
        "id": "kr0AD5ieYRbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fTxe19l1YRbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "cd = {i:[] for i in concept_classes}\n",
        "import shutup; shutup.please()\n",
        "def call():\n",
        "    import math\n",
        "    # Quantitative case study (1) Faithfulness on concepts\n",
        "    # Quantitative case study (2) Are they explainable?\n",
        "    test_dataloader=DataLoader(t_p, shuffle=True, batch_size=1)\n",
        "\n",
        "    pred_r1, pred_r2 = [],[]\n",
        "    pred_both = []\n",
        "    act = []\n",
        "\n",
        "    multimodal_repr = []\n",
        "    concept_repr = []\n",
        "\n",
        "    concept_repr1 = []\n",
        "    concept_repr2 = []\n",
        "    y_p = []\n",
        "\n",
        "    cr, rr = [],[]\n",
        "    cr1,rr1 = [],[]\n",
        "\n",
        "\n",
        "    ours_icace = []\n",
        "    ia_icace = []\n",
        "\n",
        "    counter = 0\n",
        "\n",
        "\n",
        "    c_p=0\n",
        "    c_r=0\n",
        "\n",
        "    a_p=0\n",
        "    a_r=0\n",
        "\n",
        "    average_precision_causal = 0\n",
        "    average_precision_attrib = 0\n",
        "\n",
        "    concept_cat = {}\n",
        "    for i,c in enumerate(concept_classes):\n",
        "        concept_cat[c] = all_concepts[i]\n",
        "\n",
        "    for ii, batch in tqdm(enumerate(eval_dataloader)):\n",
        "\n",
        "        if ii==0:\n",
        "            print(ig)\n",
        "\n",
        "        #if batch[1].detach().cpu().numpy()[0] == 1:\n",
        "\n",
        "        ll = batch[1].detach().cpu().numpy()[0]\n",
        "        if ll==1:\n",
        "\n",
        "            #ll = batch[1].detach().cpu().numpy()[0]\n",
        "            #print(ll, ii)\n",
        "            y_p.append(ll)\n",
        "\n",
        "            #print('*'*10)\n",
        "\n",
        "            input_ids = batch[0]['input_ids'].to('cuda')\n",
        "            #print(tokenizer.batch_decode(batch[0]['input_ids'], skip_special_tokens=True))\n",
        "            token_type_ids = batch[0]['token_type_ids'].to('cuda')\n",
        "            attention_mask = batch[0]['attention_mask'].to('cuda')\n",
        "            visual_embeds = batch[0]['visual_embeds'].to('cuda')\n",
        "            visual_token_type_ids = batch[0]['visual_token_type_ids'].to('cuda')\n",
        "            visual_attention_mask = batch[0]['visual_attention_mask'].to('cuda')\n",
        "\n",
        "            inputs_embeds = vb_wrapper.vb.vb.embeddings.word_embeddings(input_ids)\n",
        "\n",
        "            # print(inputs_embeds)\n",
        "            #     (input1_attr, input2_attr), delta = ig.attribute(inputs = (all_concepts.to('cuda'), inputs_embeds), additional_forward_args = (attention_mask,\n",
        "            #         token_type_ids,\n",
        "            #         None,\n",
        "            #         None,\n",
        "            #         None,\n",
        "            #         visual_embeds,\n",
        "            #         visual_attention_mask,\n",
        "            #         visual_token_type_ids), return_convergence_delta=True)\n",
        "\n",
        "            #print(visual_embeds)\n",
        "\n",
        "            #     (input1_attr, input2_attr, input3_attr) = ig.attribute(inputs = (all_concepts.unsqueeze(0).to('cuda'), inputs_embeds, visual_embeds), additional_forward_args = (attention_mask,\n",
        "            #         token_type_ids,\n",
        "            #         None,\n",
        "            #         None,\n",
        "            #         None,\n",
        "            #         visual_attention_mask,\n",
        "            #         visual_token_type_ids))\n",
        "            #vb(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask, visual_embeds=visual_embeds_latent, visual_attention_mask=visual_attention_mask_latent, visual_token_type_ids=visual_token_type_ids_latent)\n",
        "\n",
        "            if type(ig) in [captum.attr._core.kernel_shap.KernelShap, captum.attr._core.input_x_gradient.InputXGradient, captum.attr._core.deep_lift.DeepLift, captum.attr._core.saliency.Saliency, captum.attr._core.integrated_gradients.IntegratedGradients, captum.attr._core.feature_ablation.FeatureAblation]:\n",
        "                    baseline = None\n",
        "                    #print('none')\n",
        "            else:\n",
        "                baseline = baselines\n",
        "                #print('baseline')\n",
        "\n",
        "            if DECONFOUND:\n",
        "                all_conc = get_inlp(all_concepts).to('cuda')\n",
        "            else:\n",
        "                all_conc = all_concepts.to('cuda')\n",
        "\n",
        "            #print(all_conc)\n",
        "            #print(all_concepts)\n",
        "\n",
        "            #print(0/0)\n",
        "\n",
        "            #all_conc = all_concepts.to('cuda')\n",
        "\n",
        "            if type(ig) in [captum.attr._core.input_x_gradient.InputXGradient,captum.attr._core.saliency.Saliency]:\n",
        "                #print('ip_grad')\n",
        "                (input1_attr, input2_attr, input3_attr) = ig.attribute(inputs = (all_conc.unsqueeze(0).to('cuda'), inputs_embeds, visual_embeds),\n",
        "\n",
        "                                                                    additional_forward_args = (attention_mask,\n",
        "                token_type_ids,\n",
        "                None,\n",
        "                None,\n",
        "                None,\n",
        "                visual_attention_mask,\n",
        "                visual_token_type_ids))\n",
        "\n",
        "            else:\n",
        "                #print('others')\n",
        "\n",
        "                (input1_attr, input2_attr, input3_attr) = ig.attribute(inputs = (all_conc.unsqueeze(0).to('cuda'), inputs_embeds, visual_embeds),\n",
        "                                                                      baselines = baseline,\n",
        "                                                                        additional_forward_args = (attention_mask,\n",
        "                    token_type_ids,\n",
        "                    None,\n",
        "                    None,\n",
        "                    None,\n",
        "                    visual_attention_mask,\n",
        "                    visual_token_type_ids))\n",
        "\n",
        "\n",
        "            #print(input1_attr, input2_attr)\n",
        "            #print(input1_attr.shape, input2_attr.shape)\n",
        "            k1=input1_attr.sum(dim=-1).squeeze(0).detach().cpu().numpy()\n",
        "\n",
        "            conc_scr = []\n",
        "            attribution_score = {}\n",
        "            for i in range(18):\n",
        "                cc = concept_classes[i]\n",
        "                #print('relative attribution scores for concept {} is {}'.format(cc,k[i]))\n",
        "                conc_scr.append((cc,k1[i]))\n",
        "                attribution_score[cc] = k1[i]\n",
        "                #conc_scr[cc] = k1[i]\n",
        "            conc_scr.sort(key=lambda tup: tup[1], reverse=True)\n",
        "\n",
        "            d = {}\n",
        "\n",
        "            for cnt,i in enumerate(conc_scr):\n",
        "                d[i[0]] = cnt\n",
        "\n",
        "            #print(d)\n",
        "\n",
        "\n",
        "            orig,pooled_op = model_pred(input_ids,attention_mask,token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids,all_concepts)\n",
        "            orig = orig[0]\n",
        "            pooled_op = pooled_op.detach().cpu()\n",
        "            multimodal_repr.append(pooled_op)\n",
        "            #print('predicted class {}'.format(class_map[np.argmax(orig.detach().cpu().numpy())]))\n",
        "            #print('actual class {}'.format(ll))\n",
        "            #print(0/0)\n",
        "            #orig = orig[0]\n",
        "            k = []\n",
        "            pred_class = np.argmax(orig.detach().cpu().numpy())\n",
        "            causal_score = {}\n",
        "            #print(pred_class)\n",
        "            for i in range(18):\n",
        "                #print('Removing concept: {}'.format(concept_classes[i]))\n",
        "                cc = concept_classes[i]\n",
        "                after_intervention,_ = model_pred(input_ids,attention_mask,token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids,all_concepts,concept_no=i)\n",
        "                after_intervention = after_intervention[0][pred_class]\n",
        "                #print('hello')\n",
        "                #print(orig[pred_class], after_intervention)\n",
        "                #print(0/0)\n",
        "                k.append((cc,abs(after_intervention-orig[pred_class]).item()))\n",
        "                causal_score[cc] = abs(after_intervention-orig[pred_class]).item()\n",
        "                #print('model prediction after removal {}'.format(model_pred(input_ids,attention_mask,token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids,concept_no=i)))\n",
        "            #k = np.asarray(k)\n",
        "            #k = np.asarray(k)\n",
        "            #k = list(map(lambda x: ((x-np.mean(k))/np.mean(k))*100, k))\n",
        "            k.sort(key=lambda tup: tup[1], reverse=True)\n",
        "\n",
        "            #print(k)\n",
        "\n",
        "            #random.seed(counter)\n",
        "            #random.shuffle(k)\n",
        "\n",
        "            #print(k)\n",
        "\n",
        "            #print(0/0)\n",
        "\n",
        "\n",
        "            for i in k:\n",
        "                cd[i[0]].append(i[1])\n",
        "\n",
        "\n",
        "\n",
        "            #print(list(map(lambda x: x[0], list(k))))\n",
        "\n",
        "            gamma = 0.9\n",
        "            temp = []\n",
        "            for cnt,i in enumerate(list(map(lambda x: x[0], list(k)))):\n",
        "                temp.append((gamma**cnt) * concept_cat[i])\n",
        "                #temp.append((math.e**(-cnt)) * concept_cat[i])\n",
        "                #temp.append((1.1**(-cnt)) * concept_cat[i])\n",
        "\n",
        "            temp = torch.stack(temp).mean(dim=0).unsqueeze(dim=0)\n",
        "            #print(temp)\n",
        "            #print(temp.shape)\n",
        "\n",
        "            concept_repr.append(temp)\n",
        "\n",
        "\n",
        "\n",
        "            temp = []\n",
        "            for cnt,i in enumerate(list(map(lambda x: x[0], list(conc_scr)))):\n",
        "                temp.append((gamma**cnt) * concept_cat[i])\n",
        "                #temp.append((math.e**(-cnt)) * concept_cat[i])\n",
        "                #temp.append((1.1**(-cnt)) * concept_cat[i])\n",
        "\n",
        "            temp = torch.stack(temp).mean(dim=0).unsqueeze(dim=0)\n",
        "            #print(temp)\n",
        "            #print(temp.shape)\n",
        "\n",
        "            concept_repr1.append(temp)\n",
        "\n",
        "\n",
        "            #print(list(map(lambda x: x[0], list(conc_scr))))\n",
        "            #print(0/0)\n",
        "\n",
        "            #considering the k to be the causally sorted rank, we estimate ranks of\n",
        "\n",
        "            ip_attrib_causal = []\n",
        "\n",
        "\n",
        "\n",
        "            for i in range(18):\n",
        "                cc = concept_classes[i]\n",
        "                ip_attrib_causal.append((cc,((2*attribution_score[cc]*causal_score[cc]) / (attribution_score[cc]+causal_score[cc]))))\n",
        "\n",
        "\n",
        "\n",
        "            ip_attrib = []\n",
        "            for i in range(18):\n",
        "                cc = concept_classes[i]\n",
        "                ip_attrib.append((cc,attribution_score[cc]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            ip_attrib_causal.sort(key=lambda tup: tup[1], reverse=True)\n",
        "            ip_attrib.sort(key=lambda tup: tup[1], reverse=True)\n",
        "\n",
        "\n",
        "\n",
        "            ra_icace_ours = []\n",
        "            for cnt,i in enumerate(list(map(lambda x: x[0], list(k)))):\n",
        "                ra_icace_ours.append((gamma**cnt) * causal_score[i])\n",
        "                #ra_icace_ours.append((math.e**(-cnt)) * causal_score[i])\n",
        "                #ra_icace_ours.append((1.1**(-cnt)) * causal_score[i])\n",
        "\n",
        "            ra_icace_ia = []\n",
        "            for cnt,i in enumerate(list(map(lambda x: x[0], list(conc_scr)))):\n",
        "                ra_icace_ia.append((gamma**cnt) * causal_score[i])\n",
        "                #ra_icace_ia.append((math.e**(-cnt)) * causal_score[i])\n",
        "                #ra_icace_ia.append((1.1**(-cnt)) * causal_score[i])\n",
        "\n",
        "\n",
        "\n",
        "            ours_icace.append(np.mean(ra_icace_ours))\n",
        "            ia_icace.append(np.mean(ra_icace_ia))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            temp = []\n",
        "            for cnt,i in enumerate(list(map(lambda x: x[0], list(ip_attrib_causal)))):\n",
        "                temp.append((gamma**cnt) * concept_cat[i])\n",
        "                #temp.append((math.e**(-cnt)) * concept_cat[i])\n",
        "                #temp.append((1.1**(-cnt)) * concept_cat[i])\n",
        "\n",
        "            temp = torch.stack(temp).mean(dim=0).unsqueeze(dim=0)\n",
        "\n",
        "            concept_repr2.append(temp)\n",
        "\n",
        "\n",
        "\n",
        "            d1 = {}\n",
        "\n",
        "            for cnt,i in enumerate(ip_attrib_causal):\n",
        "                d1[i[0]] = cnt\n",
        "\n",
        "            r1,r2,r3 = [],[],[]\n",
        "            for cnt,i in enumerate(list(map(lambda x: x[0], list(k)))):\n",
        "                r1.append(cnt)\n",
        "                r2.append(d[i])\n",
        "                r3.append(d1[i])\n",
        "\n",
        "            #print(r1,r2)\n",
        "            corr, _ = stats.kendalltau(r1, r2)\n",
        "            res,_ = stats.spearmanr(r1, r2)\n",
        "\n",
        "            corr1, _ = stats.kendalltau(r1, r3)\n",
        "            res1,_ = stats.spearmanr(r1, r3)\n",
        "\n",
        "            #print('kendall tau', corr)\n",
        "            cr.append(corr)\n",
        "            #print('spearmans rho', res)\n",
        "            rr.append(res)\n",
        "\n",
        "            cr1.append(corr1)\n",
        "            rr1.append(res1)\n",
        "\n",
        "\n",
        "            rel_items = gc[counter]\n",
        "            causal = [i[0] for i in k]\n",
        "            #causal = random.sample(concept_classes,5)\n",
        "            attrib = [i[0] for i in ip_attrib]\n",
        "\n",
        "\n",
        "            #         mrr_causal = []\n",
        "            #         for i in range(len(causal)):\n",
        "            #             item = causal[i]\n",
        "            #             if item in rel_items:\n",
        "            #                 mrr_causal.append(1/(i+1))\n",
        "\n",
        "            #         mrr_attrib = []\n",
        "            #         for i in range(len(attrib)):\n",
        "            #             item = attrib[i]\n",
        "            #             if item in rel_items:\n",
        "            #                 mrr_attrib.append(1/(i+1))\n",
        "\n",
        "\n",
        "\n",
        "            #         mrr_causal = np.mean(np.asarray(mrr_causal))\n",
        "\n",
        "            #print('gc', rel_items)\n",
        "            #print('causal', causal)\n",
        "            #print('attrib', attrib)\n",
        "\n",
        "            K = 5\n",
        "\n",
        "            rel_rec_items_at_K = set(causal[:K]) & set(rel_items)\n",
        "            rec_items_at_K = causal[:K]\n",
        "            tot_rel_items = len(rel_items)\n",
        "\n",
        "            #print('Causal P@{} '.format(K), len(rel_rec_items_at_K) / len(rec_items_at_K) )\n",
        "            #print('Causal R@{} '.format(K), len(rel_rec_items_at_K) / tot_rel_items )\n",
        "\n",
        "            c_p+=len(rel_rec_items_at_K) / len(rec_items_at_K)\n",
        "            c_r+=len(rel_rec_items_at_K) / tot_rel_items\n",
        "\n",
        "\n",
        "            rel_rec_items_at_K = set(attrib[:K]) & set(rel_items)\n",
        "            rec_items_at_K = attrib[:K]\n",
        "            tot_rel_items = len(rel_items)\n",
        "\n",
        "            #print('Attrib P@{} '.format(K), len(rel_rec_items_at_K) / len(rec_items_at_K) )\n",
        "            #print('Attrib R@{} '.format(K), len(rel_rec_items_at_K) / tot_rel_items )\n",
        "\n",
        "            a_p+=len(rel_rec_items_at_K) / len(rec_items_at_K)\n",
        "            a_r+=len(rel_rec_items_at_K) / tot_rel_items\n",
        "\n",
        "\n",
        "            tot_precision_causal = []\n",
        "            for k in range(1,K+1):\n",
        "                tot_precision_causal.append(len(set(causal[:k]) & set(rel_items)) / len(causal[:k]))\n",
        "\n",
        "            tot_precision_attrib = []\n",
        "            for k in range(1,K+1):\n",
        "                tot_precision_attrib.append(len(set(attrib[:k]) & set(rel_items)) / len(attrib[:k]))\n",
        "\n",
        "\n",
        "\n",
        "            average_precision_causal += np.mean(np.asarray(tot_precision_causal))\n",
        "            average_precision_attrib += np.mean(np.asarray(tot_precision_attrib))\n",
        "\n",
        "            counter+=1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    multimodal_repr = torch.stack(multimodal_repr).squeeze(1)\n",
        "\n",
        "    #causal\n",
        "    concept_repr = torch.stack(concept_repr).squeeze(1)\n",
        "\n",
        "\n",
        "    #attribution\n",
        "    concept_repr1 = torch.stack(concept_repr1).squeeze(1)\n",
        "\n",
        "    #HM\n",
        "    concept_repr2 = torch.stack(concept_repr2).squeeze(1)\n",
        "\n",
        "    print('mean precision causal@5',c_p/counter)\n",
        "    print('mean recall causal@5', c_r/counter)\n",
        "\n",
        "    print('mean precision attrb@5',a_p/counter)\n",
        "    print('mean recall attrb@5', a_r/counter)\n",
        "\n",
        "    print('MAP@5 Causal ', average_precision_causal/counter)\n",
        "    print('MAP@5 Attrib ', average_precision_attrib/counter)\n",
        "\n",
        "    #     print('kendall tau', np.mean(cr))\n",
        "\n",
        "    #     print('spearmans rho', np.mean(rr))\n",
        "\n",
        "\n",
        "    #     print('-----------CAUSAL----------')\n",
        "    #     get_sim(multimodal_repr, concept_repr)\n",
        "\n",
        "    #     print('-----------Attribution----------')\n",
        "    #     get_sim(multimodal_repr, concept_repr1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:27:47.455484Z",
          "iopub.execute_input": "2024-09-20T21:27:47.455854Z",
          "iopub.status.idle": "2024-09-20T21:27:47.529316Z",
          "shell.execute_reply.started": "2024-09-20T21:27:47.455826Z",
          "shell.execute_reply": "2024-09-20T21:27:47.528548Z"
        },
        "trusted": true,
        "id": "4AtVk6tdYRbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baire = ['ig', 'saliency', 'deeplift', 'deepliftshap', 'gradshap', 'ipxgrad']\n",
        "#R\n",
        "#baire = ['ig', 'saliency']\n",
        "import captum\n",
        "from scipy import stats\n",
        "for fu in range(len(baire)):\n",
        "\n",
        "\n",
        "    vb_wrapper = VB_wrapper(vb)\n",
        "    vb_wrapper.eval()\n",
        "    vb_wrapper.zero_grad()\n",
        "\n",
        "    if baire[fu]=='ig':\n",
        "        ig = IntegratedGradients(vb_wrapper)\n",
        "    elif baire[fu]=='saliency':\n",
        "        ig = Saliency(vb_wrapper)\n",
        "    elif baire[fu]=='deeplift':\n",
        "        ig = DeepLift(vb_wrapper)\n",
        "    elif baire[fu]=='deepliftshap':\n",
        "        ig = DeepLiftShap(vb_wrapper)\n",
        "    elif baire[fu]=='gradshap':\n",
        "        ig = GradientShap(vb_wrapper)\n",
        "    elif baire[fu]=='ipxgrad':\n",
        "        ig = InputXGradient(vb_wrapper)\n",
        "\n",
        "    call()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-20T21:27:58.589267Z",
          "iopub.execute_input": "2024-09-20T21:27:58.589650Z",
          "iopub.status.idle": "2024-09-20T21:40:42.687030Z",
          "shell.execute_reply.started": "2024-09-20T21:27:58.589622Z",
          "shell.execute_reply": "2024-09-20T21:40:42.686106Z"
        },
        "trusted": true,
        "id": "BltmmKz4YRbz",
        "outputId": "f11e94f5-e7fa-49d7-e754-fbab6edacc3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "0it [00:00, ?it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "<captum.attr._core.integrated_gradients.IntegratedGradients object at 0x7f8e3c2787c0>\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "660it [02:52,  3.83it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "mean precision causal@5 0.20480769230769236\nmean recall causal@5 0.30384043040293046\nmean precision attrb@5 0.18461538461538451\nmean recall attrb@5 0.2655219780219781\nMAP@5 Causal  0.18879807692307685\nMAP@5 Attrib  0.16665064102564106\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "0it [00:00, ?it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "<captum.attr._core.saliency.Saliency object at 0x7f8e3c27be80>\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "660it [01:48,  6.06it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "mean precision causal@5 0.20480769230769236\nmean recall causal@5 0.30384043040293046\nmean precision attrb@5 0.1653846153846152\nmean recall attrb@5 0.22588141025641026\nMAP@5 Causal  0.18879807692307685\nMAP@5 Attrib  0.1741826923076923\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "0it [00:00, ?it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "<captum.attr._core.deep_lift.DeepLift object at 0x7f8e3c2790c0>\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "660it [01:49,  6.00it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "mean precision causal@5 0.20480769230769236\nmean recall causal@5 0.30384043040293046\nmean precision attrb@5 0.18557692307692308\nmean recall attrb@5 0.26626030219780233\nMAP@5 Causal  0.18879807692307685\nMAP@5 Attrib  0.17830128205128187\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "0it [00:00, ?it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "<captum.attr._core.deep_lift.DeepLiftShap object at 0x7f8e3674f580>\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "660it [02:32,  4.33it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "mean precision causal@5 0.20480769230769236\nmean recall causal@5 0.30384043040293046\nmean precision attrb@5 0.19423076923076918\nmean recall attrb@5 0.2869333791208793\nMAP@5 Causal  0.18879807692307685\nMAP@5 Attrib  0.19846153846153844\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "0it [00:00, ?it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "<captum.attr._core.gradient_shap.GradientShap object at 0x7f8e3c10f100>\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "660it [01:51,  5.90it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "mean precision causal@5 0.20480769230769236\nmean recall causal@5 0.30384043040293046\nmean precision attrb@5 0.19423076923076926\nmean recall attrb@5 0.2840831043956045\nMAP@5 Causal  0.18879807692307685\nMAP@5 Attrib  0.1883653846153846\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "0it [00:00, ?it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "<captum.attr._core.input_x_gradient.InputXGradient object at 0x7f8e3c117e50>\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "660it [01:48,  6.10it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "mean precision causal@5 0.20480769230769236\nmean recall causal@5 0.30384043040293046\nmean precision attrb@5 0.1807692307692307\nmean recall attrb@5 0.2579727564102565\nMAP@5 Causal  0.18879807692307685\nMAP@5 Attrib  0.16804487179487168\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2xpMWbfNYRdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cYH3gUXEYRdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zyH0JibTYRdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D2WgjIv5YRdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XL5Nq9EXYRdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d8_qIzd-YRd5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}