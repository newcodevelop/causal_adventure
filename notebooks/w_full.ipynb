{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 1246182,
          "sourceType": "datasetVersion",
          "datasetId": 715500
        }
      ],
      "dockerImageVersionId": 30648,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!ls /kaggle/working/"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:28:16.154396Z",
          "iopub.execute_input": "2024-09-21T10:28:16.155421Z",
          "iopub.status.idle": "2024-09-21T10:28:17.191898Z",
          "shell.execute_reply.started": "2024-09-21T10:28:16.155386Z",
          "shell.execute_reply": "2024-09-21T10:28:17.190773Z"
        },
        "trusted": true,
        "id": "I9vowSr6Uhk5",
        "outputId": "6869482a-dc02-4b5f-9209-4ccda2926008"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "25461.png\t   cr1.pt\t\t\t  mean_comparison_plot.png\n28076.png\t   cr2.pt\t\t\t  mr.pt\n94162.png\t   final_model_concept.pt\t  rr.npy\n96704.png\t   final_model_concept_inlp.pt\t  rr1.npy\ncausal_vb.pt\t   final_model_wo_adversarial.pt  transformers\ncausal_vb_inlp.pt  foo1.png\t\t\t  vs_tensors.pt\ncr.npy\t\t   full_annotation.pkl\t\t  y_p.pt\ncr.pt\t\t   image1.jpg\ncr1.npy\t\t   image2.jpg\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN\n",
        "\n",
        "import ipywidgets as widgets\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:29:46.888678Z",
          "iopub.execute_input": "2024-09-21T10:29:46.889101Z",
          "iopub.status.idle": "2024-09-21T10:29:46.970159Z",
          "shell.execute_reply.started": "2024-09-21T10:29:46.889053Z",
          "shell.execute_reply": "2024-09-21T10:29:46.969200Z"
        },
        "trusted": true,
        "id": "M4YAJumVUhk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "QVLGS-7dUhk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN\n",
        "ff = []"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:29:47.740655Z",
          "iopub.execute_input": "2024-09-21T10:29:47.741494Z",
          "iopub.status.idle": "2024-09-21T10:29:47.745255Z",
          "shell.execute_reply.started": "2024-09-21T10:29:47.741462Z",
          "shell.execute_reply": "2024-09-21T10:29:47.744367Z"
        },
        "trusted": true,
        "id": "ZGSafGZEUhk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN\n",
        "!pip install jsonlines\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:29:49.492008Z",
          "iopub.execute_input": "2024-09-21T10:29:49.492689Z",
          "iopub.status.idle": "2024-09-21T10:30:02.571704Z",
          "shell.execute_reply.started": "2024-09-21T10:29:49.492657Z",
          "shell.execute_reply": "2024-09-21T10:30:02.570553Z"
        },
        "trusted": true,
        "id": "Ph5OmQ_CUhk8",
        "outputId": "04303b02-af7d-4b15-eba5-04fe141e2cde"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting jsonlines\n  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\nRequirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonlines) (23.2.0)\nDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\nInstalling collected packages: jsonlines\nSuccessfully installed jsonlines-4.0.0\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN\n",
        "!pip install captum==0.7.0\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:30:02.573854Z",
          "iopub.execute_input": "2024-09-21T10:30:02.574200Z",
          "iopub.status.idle": "2024-09-21T10:30:15.265456Z",
          "shell.execute_reply.started": "2024-09-21T10:30:02.574171Z",
          "shell.execute_reply": "2024-09-21T10:30:15.264284Z"
        },
        "trusted": true,
        "id": "i1WAOIgqUhk8",
        "outputId": "b2fb7b37-533c-4948-8b72-6900ec775fab"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting captum==0.7.0\n  Downloading captum-0.7.0-py3-none-any.whl.metadata (26 kB)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from captum==0.7.0) (3.7.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from captum==0.7.0) (1.24.4)\nRequirement already satisfied: torch>=1.6 in /opt/conda/lib/python3.10/site-packages (from captum==0.7.0) (2.1.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from captum==0.7.0) (4.66.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum==0.7.0) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum==0.7.0) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum==0.7.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum==0.7.0) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum==0.7.0) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum==0.7.0) (2023.12.2)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum==0.7.0) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum==0.7.0) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum==0.7.0) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum==0.7.0) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum==0.7.0) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum==0.7.0) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum==0.7.0) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum==0.7.0) (2.8.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->captum==0.7.0) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6->captum==0.7.0) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6->captum==0.7.0) (1.3.0)\nDownloading captum-0.7.0-py3-none-any.whl (1.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: captum\nSuccessfully installed captum-0.7.0\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "import jsonlines"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:30:15.266822Z",
          "iopub.execute_input": "2024-09-21T10:30:15.267145Z",
          "iopub.status.idle": "2024-09-21T10:30:15.308644Z",
          "shell.execute_reply.started": "2024-09-21T10:30:15.267110Z",
          "shell.execute_reply": "2024-09-21T10:30:15.307940Z"
        },
        "trusted": true,
        "id": "l4H4MWMDUhk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "!pip install wget\n",
        "#!pip install transformers"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:30:15.311001Z",
          "iopub.execute_input": "2024-09-21T10:30:15.311429Z",
          "iopub.status.idle": "2024-09-21T10:30:29.901582Z",
          "shell.execute_reply.started": "2024-09-21T10:30:15.311397Z",
          "shell.execute_reply": "2024-09-21T10:30:29.900651Z"
        },
        "trusted": true,
        "id": "MUDyEnzbUhk9",
        "outputId": "3505b72b-9085-4490-8834-62eaa80ad7e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting wget\n  Downloading wget-3.2.zip (10 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: wget\n  Building wheel for wget (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=f47574df8f283d550974ca4a0f2c8e372a9efc96477096046b588864cf5e3db6\n  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\nSuccessfully built wget\nInstalling collected packages: wget\nSuccessfully installed wget-3.2\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "import sys\n",
        "sys.path.append('/kaggle/working/transformers/examples/research_projects/visual_bert')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:30:29.902908Z",
          "iopub.execute_input": "2024-09-21T10:30:29.903230Z",
          "iopub.status.idle": "2024-09-21T10:30:29.908362Z",
          "shell.execute_reply.started": "2024-09-21T10:30:29.903202Z",
          "shell.execute_reply": "2024-09-21T10:30:29.907288Z"
        },
        "trusted": true,
        "id": "DP1rfDuJUhk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "import numpy as np\n",
        "import jsonlines\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import sys\n",
        "import os\n",
        "from IPython.display import Image, display\n",
        "import PIL.Image\n",
        "import io\n",
        "import torch\n",
        "import numpy as np\n",
        "from processing_image import Preprocess\n",
        "from visualizing_image import SingleImageViz\n",
        "from modeling_frcnn import GeneralizedRCNN\n",
        "from utils import Config\n",
        "import utils\n",
        "from transformers import VisualBertForQuestionAnswering, BertTokenizerFast\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
        "from sklearn import svm\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:30:29.909720Z",
          "iopub.execute_input": "2024-09-21T10:30:29.909979Z",
          "iopub.status.idle": "2024-09-21T10:30:39.535604Z",
          "shell.execute_reply.started": "2024-09-21T10:30:29.909955Z",
          "shell.execute_reply": "2024-09-21T10:30:39.534764Z"
        },
        "trusted": true,
        "id": "2AZ7N4tLUhk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "train_file = '/kaggle/input/facebook-hateful-meme-dataset/data/train.jsonl'\n",
        "k =0\n",
        "with jsonlines.open(train_file) as reader:\n",
        "    for obj in reader:\n",
        "        print(obj['img'])\n",
        "        if k==9:\n",
        "            break\n",
        "        k+=1"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:30:39.536858Z",
          "iopub.execute_input": "2024-09-21T10:30:39.537539Z",
          "iopub.status.idle": "2024-09-21T10:30:39.559710Z",
          "shell.execute_reply.started": "2024-09-21T10:30:39.537502Z",
          "shell.execute_reply": "2024-09-21T10:30:39.558869Z"
        },
        "trusted": true,
        "id": "ryaienxzUhk_",
        "outputId": "ba8e56ac-5dd6-4500-f59b-db9c0bb862a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "img/42953.png\nimg/23058.png\nimg/13894.png\nimg/37408.png\nimg/82403.png\nimg/16952.png\nimg/76932.png\nimg/70914.png\nimg/02973.png\nimg/58306.png\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "visual_feats = {}"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:30:39.560714Z",
          "iopub.execute_input": "2024-09-21T10:30:39.560981Z",
          "iopub.status.idle": "2024-09-21T10:30:39.564757Z",
          "shell.execute_reply.started": "2024-09-21T10:30:39.560957Z",
          "shell.execute_reply": "2024-09-21T10:30:39.563911Z"
        },
        "trusted": true,
        "id": "CzORV_K9Uhk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "device = 'cuda'\n",
        "frcnn_cfg = Config.from_pretrained(\"unc-nlp/frcnn-vg-finetuned\")\n",
        "frcnn_cfg.MODEL.device = device\n",
        "frcnn = GeneralizedRCNN.from_pretrained(\"unc-nlp/frcnn-vg-finetuned\", config=frcnn_cfg)\n",
        "image_preprocess = Preprocess(frcnn_cfg)\n",
        "frcnn.eval()\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:30:39.565964Z",
          "iopub.execute_input": "2024-09-21T10:30:39.566303Z",
          "iopub.status.idle": "2024-09-21T10:30:47.497131Z",
          "shell.execute_reply.started": "2024-09-21T10:30:39.566273Z",
          "shell.execute_reply": "2024-09-21T10:30:47.496235Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "dda4998ecd5947beacc3c56f51960d64",
            "09a2720e15e347d3bcd5163426510bfa"
          ]
        },
        "id": "A7rtT8G0Uhk_",
        "outputId": "8dcc4abe-ef02-4d1d-dfd2-08672391b6eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "%s not found in cache or force_download set to True, downloading to %s https://s3.amazonaws.com/models.huggingface.co/bert/unc-nlp/frcnn-vg-finetuned/config.yaml /root/.cache/torch/transformers/tmp3bgmp7fe\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/2.13k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dda4998ecd5947beacc3c56f51960d64"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "loading configuration file cache\n%s not found in cache or force_download set to True, downloading to %s https://cdn.huggingface.co/unc-nlp/frcnn-vg-finetuned/pytorch_model.bin /root/.cache/torch/transformers/tmp0h0mwj91\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/262M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "09a2720e15e347d3bcd5163426510bfa"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "loading weights file https://cdn.huggingface.co/unc-nlp/frcnn-vg-finetuned/pytorch_model.bin from cache at /root/.cache/torch/transformers/57f6df6abe353be2773f2700159c65615babf39ab5b48114d2b49267672ae10f.77b59256a4cf8343ae0f923246a81489fc8d82f98d082edc2d2037c977c0d9d0\nAll model checkpoint weights were used when initializing GeneralizedRCNN.\n\nAll the weights of GeneralizedRCNN were initialized from the model checkpoint at unc-nlp/frcnn-vg-finetuned.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use GeneralizedRCNN for predictions without further training.\n",
          "output_type": "stream"
        },
        {
          "execution_count": 12,
          "output_type": "execute_result",
          "data": {
            "text/plain": "GeneralizedRCNN(\n  (backbone): ResNet(\n    (stem): BasicStem(\n      (conv1): Conv2d(\n        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n        (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (res2): Sequential(\n      (0): BottleneckBlock(\n        (shortcut): Conv2d(\n          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv1): Conv2d(\n          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BottleneckBlock(\n        (conv1): Conv2d(\n          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (2): BottleneckBlock(\n        (conv1): Conv2d(\n          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (res3): Sequential(\n      (0): BottleneckBlock(\n        (shortcut): Conv2d(\n          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv1): Conv2d(\n          256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BottleneckBlock(\n        (conv1): Conv2d(\n          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (2): BottleneckBlock(\n        (conv1): Conv2d(\n          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (3): BottleneckBlock(\n        (conv1): Conv2d(\n          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (res4): Sequential(\n      (0): BottleneckBlock(\n        (shortcut): Conv2d(\n          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv1): Conv2d(\n          512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (2): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (3): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (4): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (5): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (6): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (7): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (8): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (9): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (10): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (11): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (12): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (13): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (14): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (15): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (16): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (17): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (18): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (19): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (20): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (21): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (22): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n    )\n  )\n  (proposal_generator): RPN(\n    (anchor_generator): AnchorGenerator(\n      (cell_anchors): ParameterList(  (0): Parameter containing: [torch.float32 of size 12x4 (cuda:0)])\n    )\n    (rpn_head): RPNHead(\n      (conv): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (objectness_logits): Conv2d(512, 12, kernel_size=(1, 1), stride=(1, 1))\n      (anchor_deltas): Conv2d(512, 48, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n  (roi_heads): Res5ROIHeads(\n    (pooler): ROIPooler(\n      (level_poolers): ModuleList(\n        (0): RoIPool(output_size=(14, 14), spatial_scale=0.0625)\n      )\n    )\n    (res5): Sequential(\n      (0): BottleneckBlock(\n        (shortcut): Conv2d(\n          1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv1): Conv2d(\n          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False\n          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BottleneckBlock(\n        (conv1): Conv2d(\n          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False\n          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (2): BottleneckBlock(\n        (conv1): Conv2d(\n          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False\n          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (box_predictor): FastRCNNOutputLayers(\n      (cls_score): Linear(in_features=2048, out_features=1601, bias=True)\n      (bbox_pred): Linear(in_features=2048, out_features=6400, bias=True)\n      (cls_embedding): Embedding(1601, 256)\n      (fc_attr): Linear(in_features=2304, out_features=512, bias=True)\n      (attr_score): Linear(in_features=512, out_features=401, bias=True)\n    )\n  )\n)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "def get_visual_embedding(img_paths):\n",
        "    images, sizes, scales_yx = image_preprocess(img_paths) # img_paths -> list of image paths\n",
        "    output_dict = frcnn(\n",
        "      images,\n",
        "      sizes,\n",
        "      scales_yx=scales_yx,\n",
        "      padding=\"max_detections\",\n",
        "      max_detections=frcnn_cfg.max_detections,\n",
        "      return_tensors=\"pt\",\n",
        "    )\n",
        "    features = output_dict.get(\"roi_features\")\n",
        "    visual_embeds = features\n",
        "    visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\n",
        "    visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n",
        "    return visual_embeds,visual_token_type_ids,visual_attention_mask"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:30:47.501612Z",
          "iopub.execute_input": "2024-09-21T10:30:47.501912Z",
          "iopub.status.idle": "2024-09-21T10:30:47.508161Z",
          "shell.execute_reply.started": "2024-09-21T10:30:47.501886Z",
          "shell.execute_reply": "2024-09-21T10:30:47.507283Z"
        },
        "trusted": true,
        "id": "kruV8da2UhlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "visual_feats = torch.load('./vs_tensors.pt')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:30:47.509487Z",
          "iopub.execute_input": "2024-09-21T10:30:47.509835Z",
          "iopub.status.idle": "2024-09-21T10:30:50.567061Z",
          "shell.execute_reply.started": "2024-09-21T10:30:47.509804Z",
          "shell.execute_reply": "2024-09-21T10:30:50.566259Z"
        },
        "trusted": true,
        "id": "lynKbo2VUhlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "len(visual_feats)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:30:50.568344Z",
          "iopub.execute_input": "2024-09-21T10:30:50.569168Z",
          "iopub.status.idle": "2024-09-21T10:30:50.574671Z",
          "shell.execute_reply.started": "2024-09-21T10:30:50.569130Z",
          "shell.execute_reply": "2024-09-21T10:30:50.573811Z"
        },
        "trusted": true,
        "id": "YrLGWMq-UhlB",
        "outputId": "e0732285-0936-4214-ccff-97cfc581ac45"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 15,
          "output_type": "execute_result",
          "data": {
            "text/plain": "8500"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# visual_feats"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:30:50.576048Z",
          "iopub.execute_input": "2024-09-21T10:30:50.576606Z",
          "iopub.status.idle": "2024-09-21T10:30:50.586279Z",
          "shell.execute_reply.started": "2024-09-21T10:30:50.576582Z",
          "shell.execute_reply": "2024-09-21T10:30:50.585345Z"
        },
        "trusted": true,
        "id": "VESENVreUhlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "from utils import Config\n",
        "import utils\n",
        "from transformers import VisualBertModel, BertTokenizer\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.metrics import accuracy_score,f1_score,roc_auc_score\n",
        "from scipy.special import softmax"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:30:50.587566Z",
          "iopub.execute_input": "2024-09-21T10:30:50.587891Z",
          "iopub.status.idle": "2024-09-21T10:30:50.597888Z",
          "shell.execute_reply.started": "2024-09-21T10:30:50.587859Z",
          "shell.execute_reply": "2024-09-21T10:30:50.597055Z"
        },
        "trusted": true,
        "id": "9oZYyqHUUhlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "train_set = []\n",
        "train_file = '/kaggle/input/facebook-hateful-meme-dataset/data/train.jsonl'\n",
        "k =0\n",
        "with jsonlines.open(train_file) as reader:\n",
        "    for obj in reader:\n",
        "    #         print(obj['img'])\n",
        "\n",
        "        train_set.append({'text': obj['text'],\n",
        "\n",
        "                       'img': obj['img'],\n",
        "\n",
        "                       'label': int(obj['label'])})\n",
        "    #         print(k)\n",
        "    #         if k==1500:\n",
        "    #             break\n",
        "        k+=1\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:30:50.598903Z",
          "iopub.execute_input": "2024-09-21T10:30:50.599164Z",
          "iopub.status.idle": "2024-09-21T10:30:50.642296Z",
          "shell.execute_reply.started": "2024-09-21T10:30:50.599140Z",
          "shell.execute_reply": "2024-09-21T10:30:50.641595Z"
        },
        "trusted": true,
        "id": "fV_FysLqUhlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "len(train_set)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:30:50.643192Z",
          "iopub.execute_input": "2024-09-21T10:30:50.643450Z",
          "iopub.status.idle": "2024-09-21T10:30:50.650053Z",
          "shell.execute_reply.started": "2024-09-21T10:30:50.643427Z",
          "shell.execute_reply": "2024-09-21T10:30:50.649292Z"
        },
        "trusted": true,
        "id": "dW357S2TUhlC",
        "outputId": "93627d60-f2d5-414b-f454-a1e7e08dc043"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 19,
          "output_type": "execute_result",
          "data": {
            "text/plain": "8500"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, ds, max_len=64):\n",
        "        self.ds = ds\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "        self.model = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "\n",
        "\n",
        "        self.max_len = max_len\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        ds_idx = self.ds[index]\n",
        "        inputs = self.tokenizer(ds_idx['text'], padding=\"max_length\", truncation=True, max_length=self.max_len, return_tensors='pt')\n",
        "\n",
        "\n",
        "        inputs['input_ids'] = inputs['input_ids'].squeeze(0)\n",
        "        inputs['token_type_ids'] = inputs['token_type_ids'].squeeze(0)\n",
        "        inputs['attention_mask'] = inputs['attention_mask'].squeeze(0)\n",
        "\n",
        "        # visual_embeds,visual_token_type_ids,visual_attention_mask = get_visual_embedding('./data/'+ds_idx['img'])\n",
        "        visual_embeds,visual_token_type_ids,visual_attention_mask = visual_feats[ds_idx['img']]\n",
        "\n",
        "        #print(ds_idx['img'])\n",
        "\n",
        "        inputs.update({\n",
        "          \"visual_embeds\": torch.squeeze(visual_embeds),\n",
        "          \"visual_token_type_ids\": torch.squeeze(visual_token_type_ids),\n",
        "          \"visual_attention_mask\": torch.squeeze(visual_attention_mask)\n",
        "        })\n",
        "\n",
        "\n",
        "\n",
        "        #inputs.update({\"path\": ds_idx['img']})\n",
        "\n",
        "        return inputs, int(ds_idx['label'])\n",
        "\n",
        "t = CustomDataset(train_set)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:30:50.651452Z",
          "iopub.execute_input": "2024-09-21T10:30:50.651706Z",
          "iopub.status.idle": "2024-09-21T10:31:09.027716Z",
          "shell.execute_reply.started": "2024-09-21T10:30:50.651683Z",
          "shell.execute_reply": "2024-09-21T10:31:09.026833Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "d46d45d40fbd444abe559b512cf4da0d",
            "486554096eed4ea6941f1497cf7f4adc",
            "0a053009e3554d068535b55bf337a94e",
            "9f18901f241f4a21b22d585ba53f3651",
            "9ed18f10eae9470ebc2ae8e454b501df",
            "67c9ac79574f4e18aaf3f0af8632723b"
          ]
        },
        "id": "2fjV_CWTUhlC",
        "outputId": "8b2cadd1-67d0-49d8-9aef-bf8e13ce7a0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d46d45d40fbd444abe559b512cf4da0d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "486554096eed4ea6941f1497cf7f4adc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a053009e3554d068535b55bf337a94e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f18901f241f4a21b22d585ba53f3651"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/631 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ed18f10eae9470ebc2ae8e454b501df"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "pytorch_model.bin:   0%|          | 0.00/448M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "67c9ac79574f4e18aaf3f0af8632723b"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!export CUBLAS_WORKSPACE_CONFIG=:16:8"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:31:09.029945Z",
          "iopub.execute_input": "2024-09-21T10:31:09.030489Z",
          "iopub.status.idle": "2024-09-21T10:31:09.034815Z",
          "shell.execute_reply.started": "2024-09-21T10:31:09.030452Z",
          "shell.execute_reply": "2024-09-21T10:31:09.033612Z"
        },
        "trusted": true,
        "id": "w1AzpmyhUhlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "seed=42\n",
        "random.seed(seed)     # python random generator\n",
        "np.random.seed(seed)  # numpy random generator\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "#torch.use_deterministic_algorithms(True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:31:09.036308Z",
          "iopub.execute_input": "2024-09-21T10:31:09.036597Z",
          "iopub.status.idle": "2024-09-21T10:31:09.083686Z",
          "shell.execute_reply.started": "2024-09-21T10:31:09.036571Z",
          "shell.execute_reply": "2024-09-21T10:31:09.082657Z"
        },
        "trusted": true,
        "id": "pnwoK3GRUhlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(t)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:31:09.084663Z",
          "iopub.execute_input": "2024-09-21T10:31:09.084945Z",
          "iopub.status.idle": "2024-09-21T10:31:09.095954Z",
          "shell.execute_reply.started": "2024-09-21T10:31:09.084911Z",
          "shell.execute_reply": "2024-09-21T10:31:09.095215Z"
        },
        "trusted": true,
        "id": "JIK0UOJgUhlD",
        "outputId": "58df17f9-7af7-443a-b194-1dabccb99b8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 23,
          "output_type": "execute_result",
          "data": {
            "text/plain": "8500"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "torch.manual_seed(42)\n",
        "t_p,te_p = torch.utils.data.random_split(t,[7840,660])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:31:09.096981Z",
          "iopub.execute_input": "2024-09-21T10:31:09.097300Z",
          "iopub.status.idle": "2024-09-21T10:31:09.108105Z",
          "shell.execute_reply.started": "2024-09-21T10:31:09.097273Z",
          "shell.execute_reply": "2024-09-21T10:31:09.107059Z"
        },
        "trusted": true,
        "id": "gl47CcPQUhlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(t_p, shuffle=True, batch_size=32)\n",
        "eval_dataloader = DataLoader(te_p, batch_size=1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:31:09.109154Z",
          "iopub.execute_input": "2024-09-21T10:31:09.109468Z",
          "iopub.status.idle": "2024-09-21T10:31:09.114714Z",
          "shell.execute_reply.started": "2024-09-21T10:31:09.109433Z",
          "shell.execute_reply": "2024-09-21T10:31:09.113984Z"
        },
        "trusted": true,
        "id": "Dm0t3RkAUhlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:31:09.115828Z",
          "iopub.execute_input": "2024-09-21T10:31:09.116119Z",
          "iopub.status.idle": "2024-09-21T10:31:09.124225Z",
          "shell.execute_reply.started": "2024-09-21T10:31:09.116068Z",
          "shell.execute_reply": "2024-09-21T10:31:09.123325Z"
        },
        "trusted": true,
        "id": "UPo8NdY7UhlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('h')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:31:09.125295Z",
          "iopub.execute_input": "2024-09-21T10:31:09.125704Z",
          "iopub.status.idle": "2024-09-21T10:31:09.134412Z",
          "shell.execute_reply.started": "2024-09-21T10:31:09.125678Z",
          "shell.execute_reply": "2024-09-21T10:31:09.133545Z"
        },
        "trusted": true,
        "id": "7v0_iMUDUhlE",
        "outputId": "7ad57f41-5755-4d31-d652-0c511587243f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "h\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "concept_classes = [\"holocaust\", \"nazism\", \"genocide\", \"funny\", \"anti muslim\", \"terrorism\", \"violence\", \"politics\", \"racism\", \"international relation\", \"adult\", \"gore\", \"misogynistic\", \"immigration\", \"extremism\", \"immoral\", \"white supremacy\", \"indecency\"]\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:31:09.135593Z",
          "iopub.execute_input": "2024-09-21T10:31:09.135866Z",
          "iopub.status.idle": "2024-09-21T10:31:09.144399Z",
          "shell.execute_reply.started": "2024-09-21T10:31:09.135834Z",
          "shell.execute_reply": "2024-09-21T10:31:09.143533Z"
        },
        "trusted": true,
        "id": "13b9_Pg0UhlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(concept_classes)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:31:09.145410Z",
          "iopub.execute_input": "2024-09-21T10:31:09.145668Z",
          "iopub.status.idle": "2024-09-21T10:31:09.159420Z",
          "shell.execute_reply.started": "2024-09-21T10:31:09.145634Z",
          "shell.execute_reply": "2024-09-21T10:31:09.158515Z"
        },
        "trusted": true,
        "id": "AZQ3g1gbUhlF",
        "outputId": "f0d2c71a-c556-4eb1-ed5a-c6c2b0958dda"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 29,
          "output_type": "execute_result",
          "data": {
            "text/plain": "18"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cnt"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:31:09.160511Z",
          "iopub.execute_input": "2024-09-21T10:31:09.160830Z",
          "iopub.status.idle": "2024-09-21T10:31:09.167147Z",
          "shell.execute_reply.started": "2024-09-21T10:31:09.160796Z",
          "shell.execute_reply": "2024-09-21T10:31:09.166286Z"
        },
        "trusted": true,
        "id": "lHmsBuziUhlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ld8CKQ3KUhlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9DEO9ASsUhlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SarwCidZUhlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ndJyHNYNUhlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LL5BW_gmUhlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "d9eRDkfdUhlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "with open('/kaggle/working/full_annotation.pkl', 'rb') as f:\n",
        "    mynewlist = pickle.load(f)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:31:09.171395Z",
          "iopub.execute_input": "2024-09-21T10:31:09.171655Z",
          "iopub.status.idle": "2024-09-21T10:31:09.177418Z",
          "shell.execute_reply.started": "2024-09-21T10:31:09.171632Z",
          "shell.execute_reply": "2024-09-21T10:31:09.176487Z"
        },
        "trusted": true,
        "id": "SpFu6gMjUhlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(mynewlist)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:31:09.178655Z",
          "iopub.execute_input": "2024-09-21T10:31:09.179231Z",
          "iopub.status.idle": "2024-09-21T10:31:09.188884Z",
          "shell.execute_reply.started": "2024-09-21T10:31:09.179205Z",
          "shell.execute_reply": "2024-09-21T10:31:09.188032Z"
        },
        "trusted": true,
        "id": "8YmVNIV5UhlG",
        "outputId": "49b16a36-a175-4120-a657-b21303055771"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 32,
          "output_type": "execute_result",
          "data": {
            "text/plain": "208"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "CshWBVN0UhlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "gc=mynewlist"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:31:09.189879Z",
          "iopub.execute_input": "2024-09-21T10:31:09.190215Z",
          "iopub.status.idle": "2024-09-21T10:31:09.196832Z",
          "shell.execute_reply.started": "2024-09-21T10:31:09.190190Z",
          "shell.execute_reply": "2024-09-21T10:31:09.196020Z"
        },
        "trusted": true,
        "id": "iO07eORHUhlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "device='cuda'"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:31:09.197818Z",
          "iopub.execute_input": "2024-09-21T10:31:09.198145Z",
          "iopub.status.idle": "2024-09-21T10:31:09.206520Z",
          "shell.execute_reply.started": "2024-09-21T10:31:09.198117Z",
          "shell.execute_reply": "2024-09-21T10:31:09.205431Z"
        },
        "trusted": true,
        "id": "agXOtOD1UhlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MzbVel8iUhlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:31:13.430531Z",
          "iopub.execute_input": "2024-09-21T10:31:13.431296Z",
          "iopub.status.idle": "2024-09-21T10:31:13.436762Z",
          "shell.execute_reply.started": "2024-09-21T10:31:13.431252Z",
          "shell.execute_reply": "2024-09-21T10:31:13.435846Z"
        },
        "trusted": true,
        "id": "3ajBEXSxUhlO",
        "outputId": "94e0d762-1e72-43ff-ce0c-061a43f3ba23"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 35,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'cuda'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:31:15.291421Z",
          "iopub.execute_input": "2024-09-21T10:31:15.291806Z",
          "iopub.status.idle": "2024-09-21T10:31:15.455612Z",
          "shell.execute_reply.started": "2024-09-21T10:31:15.291775Z",
          "shell.execute_reply": "2024-09-21T10:31:15.454799Z"
        },
        "trusted": true,
        "id": "DZjwt1rMUhlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(\"jew\")[1:-1]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:31:15.609208Z",
          "iopub.execute_input": "2024-09-21T10:31:15.609555Z",
          "iopub.status.idle": "2024-09-21T10:31:17.963266Z",
          "shell.execute_reply.started": "2024-09-21T10:31:15.609518Z",
          "shell.execute_reply": "2024-09-21T10:31:17.962224Z"
        },
        "trusted": true,
        "id": "GttbwlZSUhlP",
        "outputId": "5eae5fce-810f-4c7d-8744-ba659210be2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 37,
          "output_type": "execute_result",
          "data": {
            "text/plain": "[16522]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "DYN_ROUTE = True\n",
        "DECONFOUND = True"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:31:17.964977Z",
          "iopub.execute_input": "2024-09-21T10:31:17.965341Z",
          "iopub.status.idle": "2024-09-21T10:31:17.973350Z",
          "shell.execute_reply.started": "2024-09-21T10:31:17.965304Z",
          "shell.execute_reply": "2024-09-21T10:31:17.972407Z"
        },
        "trusted": true,
        "id": "zdx_vGUkUhlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "!pip install pytorch_revgrad\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:31:17.974387Z",
          "iopub.execute_input": "2024-09-21T10:31:17.974708Z",
          "iopub.status.idle": "2024-09-21T10:31:30.380165Z",
          "shell.execute_reply.started": "2024-09-21T10:31:17.974681Z",
          "shell.execute_reply": "2024-09-21T10:31:30.378931Z"
        },
        "trusted": true,
        "id": "zUxJmNpRUhlQ",
        "outputId": "de5c1c39-19fb-423f-c0ba-21d37a728e6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting pytorch_revgrad\n  Downloading pytorch_revgrad-0.2.0-py3-none-any.whl.metadata (1.7 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pytorch_revgrad) (1.24.4)\nRequirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_revgrad) (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->pytorch_revgrad) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->pytorch_revgrad) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->pytorch_revgrad) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->pytorch_revgrad) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->pytorch_revgrad) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->pytorch_revgrad) (2023.12.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->pytorch_revgrad) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->pytorch_revgrad) (1.3.0)\nDownloading pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\nInstalling collected packages: pytorch_revgrad\nSuccessfully installed pytorch_revgrad-0.2.0\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.nn.Parameter(torch.tensor(0.1))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:31:30.382692Z",
          "iopub.execute_input": "2024-09-21T10:31:30.383022Z",
          "iopub.status.idle": "2024-09-21T10:31:30.387438Z",
          "shell.execute_reply.started": "2024-09-21T10:31:30.382991Z",
          "shell.execute_reply": "2024-09-21T10:31:30.386447Z"
        },
        "trusted": true,
        "id": "yk03yeWHUhlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "num_concepts = 18\n",
        "#BS = 8\n",
        "import torch\n",
        "from torch import nn\n",
        "from pytorch_revgrad import RevGrad\n",
        "class VB(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VB, self).__init__()\n",
        "        self.vb = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(768, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 2),\n",
        "        )\n",
        "\n",
        "\n",
        "        self.linear_relu_stack_latent = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(768, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 2),\n",
        "        )\n",
        "\n",
        "\n",
        "        self.linear_relu_stack_hidden = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(768, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 2),\n",
        "        )\n",
        "\n",
        "        #self.w = torch.nn.Parameter(torch.ones(11,1)/11)\n",
        "        self.w = torch.nn.Parameter(torch.ones(num_concepts,1))\n",
        "\n",
        "        self.project_latent = nn.Linear(768,2048)\n",
        "\n",
        "        #self.W_ij = [[nn.Linear(768,768) for i in range(64)] for j in range(17)]\n",
        "\n",
        "        self.W_ij = torch.nn.Parameter(torch.randn(18,64,28,28))\n",
        "\n",
        "        self.proj_conc = nn.Linear(768,28)\n",
        "        self.proj_embed = nn.Linear(768,28)\n",
        "\n",
        "\n",
        "\n",
        "        self.grad_rev1 = RevGrad()\n",
        "        self.grad_rev2 = RevGrad()\n",
        "\n",
        "        self.w1 = torch.nn.Parameter(torch.tensor(0.2))\n",
        "\n",
        "\n",
        "    def forward(self,\n",
        "        all_concepts_,\n",
        "        input_ids = None,\n",
        "        attention_mask = None,\n",
        "        token_type_ids = None,\n",
        "        position_ids = None,\n",
        "        head_mask = None,\n",
        "        inputs_embeds = None,\n",
        "        visual_embeds = None,\n",
        "        visual_attention_mask = None,\n",
        "        visual_token_type_ids = None,\n",
        "        image_text_alignment = None,\n",
        "        output_attentions = None,\n",
        "        output_hidden_states = None,\n",
        "        return_dict = None):\n",
        "\n",
        "        #         print(all_concepts.shape)\n",
        "\n",
        "        B = visual_embeds.size(0)\n",
        "\n",
        "        #         print(B)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ##############################################\n",
        "\n",
        "        all_concepts = self.proj_conc(all_concepts_)\n",
        "\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs_embeds = self.vb.embeddings.word_embeddings(input_ids)\n",
        "\n",
        "        #print(0/0)\n",
        "\n",
        "        inputs_embeds = self.proj_embed(inputs_embeds)\n",
        "\n",
        "\n",
        "        #####################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #b1 = torch.bmm(inputs_embeds,all_concepts.repeat(B,1,1).transpose(1,2))\n",
        "\n",
        "        #print(inputs_embeds.shape)\n",
        "        #print(all_concepts.repeat(B,1,1).transpose(1,2).shape)\n",
        "\n",
        "        #b,64,28\n",
        "        #b,28,17\n",
        "\n",
        "\n",
        "        b = torch.einsum('blx,bxn->bln',inputs_embeds,all_concepts.repeat(B,1,1).transpose(1,2))\n",
        "\n",
        "        #         print(b1)\n",
        "\n",
        "        #         print('********')\n",
        "\n",
        "        #         print(b2)\n",
        "\n",
        "        #         print(torch.equal(b1,b2))\n",
        "\n",
        "\n",
        "\n",
        "        b = b.masked_fill(~attention_mask.unsqueeze(dim=2).bool(), -np.inf)\n",
        "        c = F.softmax(b,dim=1)\n",
        "\n",
        "\n",
        "\n",
        "        #inter = torch.einsum('bcsn,csnn->bcsn',inputs_embeds.unsqueeze(1).repeat(1,17,1,1), self.W_ij)\n",
        "\n",
        "        #print(inter[:,0,0,:])\n",
        "\n",
        "        einsum_expression = 'bln,clnx->bclx'\n",
        "        inter = torch.einsum(einsum_expression, inputs_embeds, self.W_ij)\n",
        "\n",
        "        k = torch.einsum('bcsn,bsc->bcn',inter,c)\n",
        "        norm = torch.norm(k,dim=2)\n",
        "\n",
        "\n",
        "\n",
        "        kk1 = ((norm**2)/(1+norm**2)).unsqueeze(dim=2) * (k/norm.unsqueeze(dim=2))\n",
        "\n",
        "        w_dynamic = kk1.norm(dim=2).unsqueeze(2)\n",
        "\n",
        "\n",
        "        raw_latent = (1-self.w1) * all_concepts_.unsqueeze(0).repeat(B,1,1) * self.w.unsqueeze(dim=0).repeat(B,1,1) + all_concepts_.unsqueeze(0).repeat(B,1,1) * w_dynamic * self.w1\n",
        "\n",
        "        raw_latent = torch.mean(raw_latent, dim=1)\n",
        "\n",
        "        #raw_latent = torch.mean(all_concepts*self.w,dim=0)\n",
        "        #raw_latent = raw_latent.unsqueeze(dim=0)\n",
        "\n",
        "        latent = self.project_latent(raw_latent)\n",
        "\n",
        "\n",
        "\n",
        "        latent = latent.unsqueeze(dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        visual_embeds_latent = torch.cat((visual_embeds,latent),dim=1)\n",
        "\n",
        "\n",
        "        ################################################################\n",
        "\n",
        "\n",
        "        #print(b)\n",
        "\n",
        "        #######################################################\n",
        "        #         print(b.shape)\n",
        "\n",
        "        #         b = []\n",
        "        #         for i in range(64):\n",
        "        #             bi = []\n",
        "        #             for j in range(17):\n",
        "        #                 #print(inputs_embeds[:,i,:].shape)\n",
        "        #                 ci = all_concepts[j].unsqueeze(0).repeat(32,1,1).squeeze()\n",
        "        #                 uj = inputs_embeds[:,i,:]\n",
        "        #                 #print(ci.shape)\n",
        "        #                 bij = torch.bmm(uj.unsqueeze(dim=1), ci.unsqueeze(dim=2))\n",
        "        #                 #print(bij.shape)\n",
        "        #                 #print(bij)\n",
        "        #                 bi.append(bij.squeeze())\n",
        "        #             bi = torch.stack(bi)\n",
        "\n",
        "        #             bi = bi.T\n",
        "        #             #print(bi.shape)\n",
        "        #             #print(bi)\n",
        "        #             b.append(bi)\n",
        "\n",
        "\n",
        "\n",
        "        #         b = torch.stack(b)\n",
        "        #         b = b.transpose(0,1)\n",
        "\n",
        "        #         print(b)\n",
        "        #         print(b.shape)\n",
        "\n",
        "\n",
        "        #print(0/0)\n",
        "\n",
        "        ####################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #c = F.softmax(b,dim=1)\n",
        "\n",
        "        #print(c)\n",
        "        #print(c.shape)\n",
        "        #print(0/0)\n",
        "        ############################################################\n",
        "\n",
        "        #print\n",
        "\n",
        "        #c = 32*64*17\n",
        "\n",
        "        #u_j_i = 32*768\n",
        "\n",
        "\n",
        "        #cij = 32*1\n",
        "\n",
        "\n",
        "        #         kk = []\n",
        "        #         for i in range(17):\n",
        "        #             si = 0\n",
        "        #             for j in range(64):\n",
        "        #                 #print(inputs_embeds[:,j,:].shape)\n",
        "        #                 #print(self.W_ij)\n",
        "        #                 #print(self.W_ij[i,j,:,:].shape)\n",
        "\n",
        "\n",
        "\n",
        "        #                 u_j_i = torch.mm(inputs_embeds[:,j,:],self.W_ij[i,j,:,:])\n",
        "\n",
        "\n",
        "        #                 #print(j,i,u_j_i)\n",
        "        #                 #print(0/0)\n",
        "\n",
        "        #                 cij = c[:,j,i].unsqueeze(dim=1)\n",
        "        #                 #print('cij ',cij)\n",
        "        #                 #print(cij.shape)\n",
        "        #                 #print(u_j_i)\n",
        "        #                 #print(u_j_i.shape)\n",
        "        #                 si+= u_j_i*cij\n",
        "\n",
        "        #                 #print(u_j_i*cij)\n",
        "        #             #print(si)\n",
        "        #             #print(0/0)\n",
        "\n",
        "        #             norm = torch.norm(si, dim=1)\n",
        "\n",
        "        #             #print(norm)\n",
        "        #             #print(norm.shape)\n",
        "\n",
        "        #             vi = ((norm**2)/(1+norm**2)).unsqueeze(dim=1) * (si/norm.unsqueeze(dim=1))\n",
        "\n",
        "        #             #print(vi)\n",
        "        #             kk.append(torch.norm(vi,dim=1))\n",
        "        #             #kk.append(vi)\n",
        "        #             #print(kk)\n",
        "\n",
        "\n",
        "        #         kk = torch.stack(kk)\n",
        "\n",
        "        #         print('kk', kk)\n",
        "        #         print('kk1', kk1)\n",
        "        #         print(kk.shape)\n",
        "        #         print(kk1.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Create a tensor of ones with the same number of columns as the original tensor\n",
        "        ones_column_vam = torch.ones((visual_attention_mask.size(0), 1), dtype=visual_attention_mask.dtype)\n",
        "\n",
        "        # Concatenate the original tensor with the tensor of ones along the second dimension (columns)\n",
        "        visual_attention_mask_latent = torch.cat((visual_attention_mask, ones_column_vam.to('cuda')), dim=1)\n",
        "\n",
        "        # Create a tensor of ones with the same number of columns as the original tensor\n",
        "        ones_column_vtt = torch.ones((visual_token_type_ids.size(0), 1), dtype=visual_token_type_ids.dtype)\n",
        "\n",
        "        # Concatenate the original tensor with the tensor of ones along the second dimension (columns)\n",
        "        visual_token_type_ids_latent = torch.cat((visual_token_type_ids, ones_column_vtt.to('cuda')), dim=1)\n",
        "\n",
        "        #         print(latent.shape)\n",
        "\n",
        "        #         print(visual_embeds.shape)\n",
        "        #         print(visual_attention_mask.shape)\n",
        "        #         print(visual_token_type_ids.shape)\n",
        "\n",
        "        #         print(self.w)\n",
        "\n",
        "        x = self.vb(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask, visual_embeds=visual_embeds_latent, visual_attention_mask=visual_attention_mask_latent, visual_token_type_ids=visual_token_type_ids_latent)\n",
        "\n",
        "        p = self.vb(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask, visual_embeds=visual_embeds, visual_attention_mask=visual_attention_mask, visual_token_type_ids=visual_token_type_ids)\n",
        "        p = self.grad_rev1(p.pooler_output)\n",
        "        #q = self.grad_rev2(raw_latent.repeat(B,1))\n",
        "        q = self.grad_rev2(raw_latent)\n",
        "\n",
        "        logits_p = self.linear_relu_stack_hidden(p)\n",
        "        logits_q = self.linear_relu_stack_latent(q)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        logits = self.linear_relu_stack(x.pooler_output)\n",
        "\n",
        "\n",
        "        if_logits_p = self.linear_relu_stack(p)\n",
        "        if_logits_q = self.linear_relu_stack(q)\n",
        "\n",
        "        return logits, logits_p, logits_q, if_logits_p, if_logits_q\n",
        "        #return logits, logits_p, logits_q, if_logits_p, if_logits_p\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:31:30.388864Z",
          "iopub.execute_input": "2024-09-21T10:31:30.389215Z",
          "iopub.status.idle": "2024-09-21T10:31:30.429896Z",
          "shell.execute_reply.started": "2024-09-21T10:31:30.389185Z",
          "shell.execute_reply": "2024-09-21T10:31:30.429064Z"
        },
        "trusted": true,
        "id": "tN-jInolUhlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LDWcdSfkUhlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y0tiGYUsUhlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2UDbvi49UhlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9mByLtDjUhlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WjtSsm7LUhlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "vb = VB()\n",
        "\n",
        "# vb = nn.DataParallel(vb)\n",
        "vb.to(device)\n",
        "concept_classes = [\"holocaust\", \"nazism\", \"genocide\", \"funny\", \"anti muslim\", \"terrorism\", \"violence\", \"politics\", \"racism\", \"international relation\", \"adult\", \"gore\", \"misogynistic\", \"immigration\", \"extremism\", \"immoral\", \"white supremacy\", \"indecency\"]\n",
        "\n",
        "\n",
        "concept_embedding_map = {}\n",
        "\n",
        "all_concepts = []\n",
        "\n",
        "\n",
        "for c in concept_classes:\n",
        "    conc = torch.tensor(tokenizer.encode(c)[1:-1])\n",
        "    e = vb.vb.embeddings.word_embeddings(conc.to('cuda'))\n",
        "    x = e.detach().cpu().mean(dim=0)\n",
        "\n",
        "\n",
        "\n",
        "    all_concepts.append(x)\n",
        "\n",
        "    concept_embedding_map[c] = x\n",
        "\n",
        "\n",
        "all_concepts = torch.stack(all_concepts)\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:31:30.432606Z",
          "iopub.execute_input": "2024-09-21T10:31:30.432869Z",
          "iopub.status.idle": "2024-09-21T10:31:31.438138Z",
          "shell.execute_reply.started": "2024-09-21T10:31:30.432846Z",
          "shell.execute_reply": "2024-09-21T10:31:31.437312Z"
        },
        "trusted": true,
        "id": "gwwFn2H3UhlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "all_concepts.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:31:31.439554Z",
          "iopub.execute_input": "2024-09-21T10:31:31.439864Z",
          "iopub.status.idle": "2024-09-21T10:31:31.447617Z",
          "shell.execute_reply.started": "2024-09-21T10:31:31.439838Z",
          "shell.execute_reply": "2024-09-21T10:31:31.446742Z"
        },
        "trusted": true,
        "id": "w46D4aXfUhlT",
        "outputId": "a9ff0b57-f6a2-407d-fdf6-34730a7bc3b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 43,
          "output_type": "execute_result",
          "data": {
            "text/plain": "torch.Size([18, 768])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "vb = VB()\n",
        "vb.load_state_dict(torch.load('/kaggle/working/final_model_concept.pt'))\n",
        "vb.to('cuda')\n",
        "vb.eval()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:31:31.450219Z",
          "iopub.execute_input": "2024-09-21T10:31:31.450695Z",
          "iopub.status.idle": "2024-09-21T10:31:32.502427Z",
          "shell.execute_reply.started": "2024-09-21T10:31:31.450661Z",
          "shell.execute_reply": "2024-09-21T10:31:32.501543Z"
        },
        "trusted": true,
        "id": "1tHyrGgsUhlT",
        "outputId": "93989fcc-209e-4969-e6fe-b6d8b74270cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 44,
          "output_type": "execute_result",
          "data": {
            "text/plain": "VB(\n  (vb): VisualBertModel(\n    (embeddings): VisualBertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=1)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n      (visual_token_type_embeddings): Embedding(2, 768)\n      (visual_position_embeddings): Embedding(512, 768)\n      (visual_projection): Linear(in_features=2048, out_features=768, bias=True)\n    )\n    (encoder): VisualBertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x VisualBertLayer(\n          (attention): VisualBertAttention(\n            (self): VisualBertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): VisualBertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): VisualBertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): VisualBertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): VisualBertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (linear_relu_stack): Sequential(\n    (0): Dropout(p=0.1, inplace=False)\n    (1): Linear(in_features=768, out_features=512, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=512, out_features=128, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=128, out_features=2, bias=True)\n  )\n  (linear_relu_stack_latent): Sequential(\n    (0): Dropout(p=0.1, inplace=False)\n    (1): Linear(in_features=768, out_features=512, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=512, out_features=128, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=128, out_features=2, bias=True)\n  )\n  (linear_relu_stack_hidden): Sequential(\n    (0): Dropout(p=0.1, inplace=False)\n    (1): Linear(in_features=768, out_features=512, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=512, out_features=128, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=128, out_features=2, bias=True)\n  )\n  (project_latent): Linear(in_features=768, out_features=2048, bias=True)\n  (proj_conc): Linear(in_features=768, out_features=28, bias=True)\n  (proj_embed): Linear(in_features=768, out_features=28, bias=True)\n  (grad_rev1): RevGrad()\n  (grad_rev2): RevGrad()\n)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# all_concepts.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:31:32.504393Z",
          "iopub.execute_input": "2024-09-21T10:31:32.504834Z",
          "iopub.status.idle": "2024-09-21T10:31:32.508903Z",
          "shell.execute_reply.started": "2024-09-21T10:31:32.504800Z",
          "shell.execute_reply": "2024-09-21T10:31:32.507964Z"
        },
        "trusted": true,
        "id": "G5pCZ-EAUhlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all_concepts[0].unsqueeze(0).shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:31:32.510056Z",
          "iopub.execute_input": "2024-09-21T10:31:32.510425Z",
          "iopub.status.idle": "2024-09-21T10:31:32.518865Z",
          "shell.execute_reply.started": "2024-09-21T10:31:32.510392Z",
          "shell.execute_reply": "2024-09-21T10:31:32.518019Z"
        },
        "trusted": true,
        "id": "gn7t9U0VUhlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all_concepts[0].unsqueeze(0).repeat(32,1,1).squeeze().shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:31:33.841039Z",
          "iopub.execute_input": "2024-09-21T10:31:33.842045Z",
          "iopub.status.idle": "2024-09-21T10:31:33.846064Z",
          "shell.execute_reply.started": "2024-09-21T10:31:33.842010Z",
          "shell.execute_reply": "2024-09-21T10:31:33.845044Z"
        },
        "trusted": true,
        "id": "wpCt3kznUhlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "# check evaluation performance separately on p and q using their classifier: should be less\n",
        "pred_r1, pred_r2 = [],[]\n",
        "pred_both = []\n",
        "act = []\n",
        "for ii, batch in enumerate(eval_dataloader):\n",
        "    with torch.no_grad():\n",
        "        batch_inp = {k: v.to(device) for k, v in batch[0].items()}\n",
        "        outputs, r1, r2,_,_ = vb(all_concepts.to('cuda'),**batch_inp)\n",
        "        act.append(batch[1].detach().cpu().numpy()[0])\n",
        "\n",
        "        pred_r1.append(np.argmax(r1.detach().cpu().numpy()[0]))\n",
        "        pred_r2.append(np.argmax(r2.detach().cpu().numpy()[0]))\n",
        "        pred_both.append(np.argmax(outputs.detach().cpu().numpy()[0]))\n",
        "\n",
        "print('*')\n",
        "print(f1_score(act,pred_r1,average='macro'))\n",
        "print(f1_score(act,pred_r1,average='weighted'))\n",
        "print(accuracy_score(act,pred_r1))\n",
        "print('*'*10)\n",
        "print(f1_score(act,pred_r2,average='macro'))\n",
        "print(f1_score(act,pred_r2,average='weighted'))\n",
        "print(accuracy_score(act,pred_r2))\n",
        "print('*'*10)\n",
        "print(f1_score(act,pred_both,average='macro'))\n",
        "print(f1_score(act,pred_both,average='weighted'))\n",
        "print(accuracy_score(act,pred_both))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:31:34.274964Z",
          "iopub.execute_input": "2024-09-21T10:31:34.275698Z",
          "iopub.status.idle": "2024-09-21T10:31:49.848003Z",
          "shell.execute_reply.started": "2024-09-21T10:31:34.275660Z",
          "shell.execute_reply": "2024-09-21T10:31:49.847076Z"
        },
        "trusted": true,
        "id": "T5m6P5LDUhlV",
        "outputId": "9979544a-907f-4ccc-b838-27a543149dc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "*\n0.40647482014388486\n0.5567473294091998\n0.6848484848484848\n**********\n0.40647482014388486\n0.5567473294091998\n0.6848484848484848\n**********\n0.7036565354754966\n0.7469814601650707\n0.75\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check evaluation performance separately on p and q using parent classifier: should be also less\n",
        "pred_r1, pred_r2 = [],[]\n",
        "pred_both = []\n",
        "act = []\n",
        "for ii, batch in enumerate(eval_dataloader):\n",
        "    with torch.no_grad():\n",
        "        batch_inp = {k: v.to(device) for k, v in batch[0].items()}\n",
        "        outputs, _,_, r1, r2 = vb(all_concepts.to('cuda'),**batch_inp)\n",
        "        act.append(batch[1].detach().cpu().numpy()[0])\n",
        "\n",
        "        pred_r1.append(np.argmax(r1.detach().cpu().numpy()[0]))\n",
        "        pred_r2.append(np.argmax(r2.detach().cpu().numpy()[0]))\n",
        "        pred_both.append(np.argmax(outputs.detach().cpu().numpy()[0]))\n",
        "\n",
        "print('*'*10)\n",
        "print(f1_score(act,pred_r1,average='macro'))\n",
        "print(f1_score(act,pred_r1,average='weighted'))\n",
        "print(accuracy_score(act,pred_r1))\n",
        "print('*'*10)\n",
        "print(f1_score(act,pred_r2,average='macro'))\n",
        "print(f1_score(act,pred_r2,average='weighted'))\n",
        "print(accuracy_score(act,pred_r2))\n",
        "print('*'*10)\n",
        "print(f1_score(act,pred_both,average='macro'))\n",
        "print(f1_score(act,pred_both,average='weighted'))\n",
        "print(accuracy_score(act,pred_both))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:31:49.849715Z",
          "iopub.execute_input": "2024-09-21T10:31:49.850049Z",
          "iopub.status.idle": "2024-09-21T10:32:05.324632Z",
          "shell.execute_reply.started": "2024-09-21T10:31:49.850019Z",
          "shell.execute_reply": "2024-09-21T10:32:05.323560Z"
        },
        "trusted": true,
        "id": "TP5SmP8KUhlV",
        "outputId": "72c8d194-0ba0-43d1-de79-d663ce6b6f2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "**********\n0.23963133640552994\n0.1510403574919704\n0.3151515151515151\n**********\n0.23963133640552994\n0.1510403574919704\n0.3151515151515151\n**********\n0.7036565354754966\n0.7469814601650707\n0.75\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.save(vb.state_dict(), '/kaggle/working/final_model_wo_adversarial.pt')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:32:05.325805Z",
          "iopub.execute_input": "2024-09-21T10:32:05.326129Z",
          "iopub.status.idle": "2024-09-21T10:32:05.330216Z",
          "shell.execute_reply.started": "2024-09-21T10:32:05.326101Z",
          "shell.execute_reply": "2024-09-21T10:32:05.329224Z"
        },
        "trusted": true,
        "id": "b8G2sHDHUhlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "from typing import Dict\n",
        "\n",
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "\n",
        "\n",
        "# import svm_classifier\n",
        "\n",
        "REGRESSION = False\n",
        "from typing import List\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "def get_weights(model) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        :return: final weights of the model, as np array\n",
        "        \"\"\"\n",
        "\n",
        "        w = model.coef_\n",
        "        if len(w.shape) == 1:\n",
        "                w = np.expand_dims(w, 0)\n",
        "\n",
        "        return w\n",
        "\n",
        "\n",
        "def train_network(model, X_train: np.ndarray, Y_train: np.ndarray, X_dev: np.ndarray, Y_dev: np.ndarray) -> float:\n",
        "\n",
        "        \"\"\"\n",
        "        :param X_train:\n",
        "        :param Y_train:\n",
        "        :param X_dev:\n",
        "        :param Y_dev:\n",
        "        :return: accuracy score on the dev set / Person's R in the case of regression\n",
        "        \"\"\"\n",
        "\n",
        "        model.fit(X_train, Y_train)\n",
        "        score = model.score(X_dev, Y_dev)\n",
        "        return score\n",
        "\n",
        "\n",
        "from typing import Dict\n",
        "\n",
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "\n",
        "# import svm_classifier\n",
        "\n",
        "REGRESSION = False\n",
        "from typing import List\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "\n",
        "def get_nullspace_projection(W: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    :param W: the matrix over its nullspace to project\n",
        "    :return: the projection matrix\n",
        "    \"\"\"\n",
        "    nullspace_basis = scipy.linalg.null_space(W)  # orthogonal basis\n",
        "    nullspace_basis = nullspace_basis * np.sign(nullspace_basis[0][0])  # handle sign ambiguity\n",
        "    projection_matrix = nullspace_basis.dot(nullspace_basis.T)\n",
        "\n",
        "    return projection_matrix\n",
        "\n",
        "from sklearn import svm"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:32:05.332881Z",
          "iopub.execute_input": "2024-09-21T10:32:05.333368Z",
          "iopub.status.idle": "2024-09-21T10:32:05.345138Z",
          "shell.execute_reply.started": "2024-09-21T10:32:05.333331Z",
          "shell.execute_reply": "2024-09-21T10:32:05.344347Z"
        },
        "trusted": true,
        "id": "pcR97TmrUhlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#R\n",
        "num_concepts = 18\n",
        "BS = 8\n",
        "import torch\n",
        "from torch import nn\n",
        "from pytorch_revgrad import RevGrad\n",
        "class VB(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VB, self).__init__()\n",
        "        self.vb = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(768, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 2),\n",
        "        )\n",
        "\n",
        "\n",
        "        self.linear_relu_stack_latent = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(768, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 2),\n",
        "        )\n",
        "\n",
        "\n",
        "        self.linear_relu_stack_hidden = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(768, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 2),\n",
        "        )\n",
        "\n",
        "        #self.w = torch.nn.Parameter(torch.ones(11,1)/11)\n",
        "        self.w = torch.nn.Parameter(torch.ones(num_concepts,1))\n",
        "\n",
        "        self.project_latent = nn.Linear(768,2048)\n",
        "\n",
        "        #self.W_ij = [[nn.Linear(768,768) for i in range(64)] for j in range(17)]\n",
        "\n",
        "        self.W_ij = torch.nn.Parameter(torch.randn(18,64,28,28))\n",
        "\n",
        "        self.proj_conc = nn.Linear(768,28)\n",
        "        self.proj_embed = nn.Linear(768,28)\n",
        "\n",
        "\n",
        "\n",
        "        self.grad_rev1 = RevGrad()\n",
        "        self.grad_rev2 = RevGrad()\n",
        "\n",
        "        self.w1 = torch.nn.Parameter(torch.tensor(0.2))\n",
        "\n",
        "\n",
        "    def forward(self,\n",
        "        all_concepts_,\n",
        "        input_ids = None,\n",
        "        attention_mask = None,\n",
        "        token_type_ids = None,\n",
        "        position_ids = None,\n",
        "        head_mask = None,\n",
        "        inputs_embeds = None,\n",
        "        visual_embeds = None,\n",
        "        visual_attention_mask = None,\n",
        "        visual_token_type_ids = None,\n",
        "        image_text_alignment = None,\n",
        "        output_attentions = None,\n",
        "        output_hidden_states = None,\n",
        "        return_dict = None):\n",
        "\n",
        "        #         print(all_concepts.shape)\n",
        "\n",
        "        B = visual_embeds.size(0)\n",
        "\n",
        "        #         print(B)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ##############################################\n",
        "\n",
        "        all_concepts = self.proj_conc(all_concepts_)\n",
        "\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs_embeds = self.vb.embeddings.word_embeddings(input_ids)\n",
        "\n",
        "        #print(0/0)\n",
        "\n",
        "        inputs_embeds = self.proj_embed(inputs_embeds)\n",
        "\n",
        "\n",
        "        #####################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #b1 = torch.bmm(inputs_embeds,all_concepts.repeat(B,1,1).transpose(1,2))\n",
        "\n",
        "        #print(inputs_embeds.shape)\n",
        "        #print(all_concepts.repeat(B,1,1).transpose(1,2).shape)\n",
        "\n",
        "        #b,64,28\n",
        "        #b,28,17\n",
        "\n",
        "\n",
        "        b = torch.einsum('blx,bxn->bln',inputs_embeds,all_concepts.repeat(B,1,1).transpose(1,2))\n",
        "\n",
        "        #         print(b1)\n",
        "\n",
        "        #         print('********')\n",
        "\n",
        "        #         print(b2)\n",
        "\n",
        "        #         print(torch.equal(b1,b2))\n",
        "\n",
        "\n",
        "\n",
        "        b = b.masked_fill(~attention_mask.unsqueeze(dim=2).bool(), -np.inf)\n",
        "        c = F.softmax(b,dim=1)\n",
        "\n",
        "\n",
        "\n",
        "        #inter = torch.einsum('bcsn,csnn->bcsn',inputs_embeds.unsqueeze(1).repeat(1,17,1,1), self.W_ij)\n",
        "\n",
        "        #print(inter[:,0,0,:])\n",
        "\n",
        "        einsum_expression = 'bln,clnx->bclx'\n",
        "        inter = torch.einsum(einsum_expression, inputs_embeds, self.W_ij)\n",
        "\n",
        "        k = torch.einsum('bcsn,bsc->bcn',inter,c)\n",
        "        norm = torch.norm(k,dim=2)\n",
        "\n",
        "\n",
        "\n",
        "        kk1 = ((norm**2)/(1+norm**2)).unsqueeze(dim=2) * (k/norm.unsqueeze(dim=2))\n",
        "\n",
        "        w_dynamic = kk1.norm(dim=2).unsqueeze(2)\n",
        "\n",
        "\n",
        "\n",
        "        raw_latent = (1-self.w1) * all_concepts_.unsqueeze(0).repeat(B,1,1) * self.w.unsqueeze(dim=0).repeat(B,1,1) + all_concepts_.unsqueeze(0).repeat(B,1,1) * w_dynamic * self.w1\n",
        "\n",
        "\n",
        "        #raw_latent = 0 * all_concepts_.unsqueeze(0).repeat(B,1,1) * self.w.unsqueeze(dim=0).repeat(B,1,1) + all_concepts_.unsqueeze(0).repeat(B,1,1) * w_dynamic *1\n",
        "\n",
        "\n",
        "        raw_latent = torch.mean(raw_latent, dim=1)\n",
        "\n",
        "        #raw_latent = torch.mean(all_concepts*self.w,dim=0)\n",
        "        #raw_latent = raw_latent.unsqueeze(dim=0)\n",
        "\n",
        "        latent = self.project_latent(raw_latent)\n",
        "\n",
        "\n",
        "\n",
        "        latent = latent.unsqueeze(dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        visual_embeds_latent = torch.cat((visual_embeds,latent),dim=1)\n",
        "\n",
        "\n",
        "        ################################################################\n",
        "\n",
        "\n",
        "        #print(b)\n",
        "\n",
        "        #######################################################\n",
        "        #         print(b.shape)\n",
        "\n",
        "        #         b = []\n",
        "        #         for i in range(64):\n",
        "        #             bi = []\n",
        "        #             for j in range(17):\n",
        "        #                 #print(inputs_embeds[:,i,:].shape)\n",
        "        #                 ci = all_concepts[j].unsqueeze(0).repeat(32,1,1).squeeze()\n",
        "        #                 uj = inputs_embeds[:,i,:]\n",
        "        #                 #print(ci.shape)\n",
        "        #                 bij = torch.bmm(uj.unsqueeze(dim=1), ci.unsqueeze(dim=2))\n",
        "        #                 #print(bij.shape)\n",
        "        #                 #print(bij)\n",
        "        #                 bi.append(bij.squeeze())\n",
        "        #             bi = torch.stack(bi)\n",
        "\n",
        "        #             bi = bi.T\n",
        "        #             #print(bi.shape)\n",
        "        #             #print(bi)\n",
        "        #             b.append(bi)\n",
        "\n",
        "\n",
        "\n",
        "        #         b = torch.stack(b)\n",
        "        #         b = b.transpose(0,1)\n",
        "\n",
        "        #         print(b)\n",
        "        #         print(b.shape)\n",
        "\n",
        "\n",
        "        #print(0/0)\n",
        "\n",
        "        ####################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #c = F.softmax(b,dim=1)\n",
        "\n",
        "        #print(c)\n",
        "        #print(c.shape)\n",
        "        #print(0/0)\n",
        "        ############################################################\n",
        "\n",
        "        #print\n",
        "\n",
        "        #c = 32*64*17\n",
        "\n",
        "        #u_j_i = 32*768\n",
        "\n",
        "\n",
        "        #cij = 32*1\n",
        "\n",
        "\n",
        "        #         kk = []\n",
        "        #         for i in range(17):\n",
        "        #             si = 0\n",
        "        #             for j in range(64):\n",
        "        #                 #print(inputs_embeds[:,j,:].shape)\n",
        "        #                 #print(self.W_ij)\n",
        "        #                 #print(self.W_ij[i,j,:,:].shape)\n",
        "\n",
        "\n",
        "\n",
        "        #                 u_j_i = torch.mm(inputs_embeds[:,j,:],self.W_ij[i,j,:,:])\n",
        "\n",
        "\n",
        "        #                 #print(j,i,u_j_i)\n",
        "        #                 #print(0/0)\n",
        "\n",
        "        #                 cij = c[:,j,i].unsqueeze(dim=1)\n",
        "        #                 #print('cij ',cij)\n",
        "        #                 #print(cij.shape)\n",
        "        #                 #print(u_j_i)\n",
        "        #                 #print(u_j_i.shape)\n",
        "        #                 si+= u_j_i*cij\n",
        "\n",
        "        #                 #print(u_j_i*cij)\n",
        "        #             #print(si)\n",
        "        #             #print(0/0)\n",
        "\n",
        "        #             norm = torch.norm(si, dim=1)\n",
        "\n",
        "        #             #print(norm)\n",
        "        #             #print(norm.shape)\n",
        "\n",
        "        #             vi = ((norm**2)/(1+norm**2)).unsqueeze(dim=1) * (si/norm.unsqueeze(dim=1))\n",
        "\n",
        "        #             #print(vi)\n",
        "        #             kk.append(torch.norm(vi,dim=1))\n",
        "        #             #kk.append(vi)\n",
        "        #             #print(kk)\n",
        "\n",
        "\n",
        "        #         kk = torch.stack(kk)\n",
        "\n",
        "        #         print('kk', kk)\n",
        "        #         print('kk1', kk1)\n",
        "        #         print(kk.shape)\n",
        "        #         print(kk1.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Create a tensor of ones with the same number of columns as the original tensor\n",
        "        ones_column_vam = torch.ones((visual_attention_mask.size(0), 1), dtype=visual_attention_mask.dtype)\n",
        "\n",
        "        # Concatenate the original tensor with the tensor of ones along the second dimension (columns)\n",
        "        visual_attention_mask_latent = torch.cat((visual_attention_mask, ones_column_vam.to('cuda')), dim=1)\n",
        "\n",
        "        # Create a tensor of ones with the same number of columns as the original tensor\n",
        "        ones_column_vtt = torch.ones((visual_token_type_ids.size(0), 1), dtype=visual_token_type_ids.dtype)\n",
        "\n",
        "        # Concatenate the original tensor with the tensor of ones along the second dimension (columns)\n",
        "        visual_token_type_ids_latent = torch.cat((visual_token_type_ids, ones_column_vtt.to('cuda')), dim=1)\n",
        "\n",
        "        #         print(latent.shape)\n",
        "\n",
        "        #         print(visual_embeds.shape)\n",
        "        #         print(visual_attention_mask.shape)\n",
        "        #         print(visual_token_type_ids.shape)\n",
        "\n",
        "        #         print(self.w)\n",
        "\n",
        "        x = self.vb(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask, visual_embeds=visual_embeds_latent, visual_attention_mask=visual_attention_mask_latent, visual_token_type_ids=visual_token_type_ids_latent)\n",
        "\n",
        "        p = self.vb(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask, visual_embeds=visual_embeds, visual_attention_mask=visual_attention_mask, visual_token_type_ids=visual_token_type_ids)\n",
        "        p = self.grad_rev1(p.pooler_output)\n",
        "        #q = self.grad_rev2(raw_latent.repeat(B,1))\n",
        "        q = self.grad_rev2(raw_latent)\n",
        "\n",
        "        logits_p = self.linear_relu_stack_hidden(p)\n",
        "        logits_q = self.linear_relu_stack_latent(q)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        logits = self.linear_relu_stack(x.pooler_output)\n",
        "\n",
        "\n",
        "        if_logits_p = self.linear_relu_stack(p)\n",
        "        if_logits_q = self.linear_relu_stack(q)\n",
        "\n",
        "        return logits, logits_p, logits_q, if_logits_p, if_logits_q\n",
        "        #return logits, logits_p, logits_q, if_logits_p, if_logits_p\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:32:05.346471Z",
          "iopub.execute_input": "2024-09-21T10:32:05.346772Z",
          "iopub.status.idle": "2024-09-21T10:32:05.378146Z",
          "shell.execute_reply.started": "2024-09-21T10:32:05.346749Z",
          "shell.execute_reply": "2024-09-21T10:32:05.377236Z"
        },
        "trusted": true,
        "id": "qeaUtypeUhlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "def get_inlp(all_concepts):\n",
        "    print('INLP step')\n",
        "    num_concepts = 18\n",
        "    all_concepts_ = all_concepts.numpy()\n",
        "    multiplier = np.logical_xor(1, np.eye(num_concepts)).astype(np.int32)/ (num_concepts-1)\n",
        "    X = np.matmul(multiplier,all_concepts_)\n",
        "    assert X.shape[0]==num_concepts\n",
        "    Y = np.arange(num_concepts)\n",
        "\n",
        "    #             x_train_cp = x_train\n",
        "    #             x_test_cp = x_test\n",
        "    x_train_cp = X\n",
        "    x_test_cp = X\n",
        "    y_train = Y\n",
        "    y_test = Y\n",
        "    print('******')\n",
        "    for i in range(1):\n",
        "\n",
        "        clf = svm.SVC(kernel=\"linear\")\n",
        "        scr = train_network(clf, x_train_cp, y_train, x_test_cp, y_test)\n",
        "        print('initial {}'.format(scr))\n",
        "        W = get_weights(clf)\n",
        "        P_i = get_nullspace_projection(W)\n",
        "        x_train_cp = x_train_cp.dot(P_i)\n",
        "        x_test_cp = x_test_cp.dot(P_i)\n",
        "        print(clf.score(x_test_cp, y_test))\n",
        "        print(clf.score(x_train_cp, y_train))\n",
        "        clf = svm.SVC(kernel=\"linear\")\n",
        "        train_network(clf, x_train_cp, y_train, x_test_cp, y_test)\n",
        "    print('******')\n",
        "    all_concepts_ = torch.tensor(all_concepts_)\n",
        "    all_concepts_ = all_concepts_.mm(torch.tensor(P_i, dtype = all_concepts_.dtype))\n",
        "\n",
        "    return all_concepts_"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:32:05.379267Z",
          "iopub.execute_input": "2024-09-21T10:32:05.379551Z",
          "iopub.status.idle": "2024-09-21T10:32:05.393180Z",
          "shell.execute_reply.started": "2024-09-21T10:32:05.379527Z",
          "shell.execute_reply": "2024-09-21T10:32:05.392213Z"
        },
        "trusted": true,
        "id": "YlApMFaXUhlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "import torch\n",
        "from torch import nn\n",
        "from pytorch_revgrad import RevGrad\n",
        "class VB_wrapper(nn.Module):\n",
        "    def __init__(self, vb):\n",
        "        super(VB_wrapper, self).__init__()\n",
        "\n",
        "        self.vb = vb\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,\n",
        "        all_concepts_,\n",
        "        input_embeds = None,\n",
        "        visual_embeds = None,\n",
        "        attention_mask = None,\n",
        "        token_type_ids = None,\n",
        "        position_ids = None,\n",
        "        head_mask = None,\n",
        "        inputs_embeds = None,\n",
        "\n",
        "        visual_attention_mask = None,\n",
        "        visual_token_type_ids = None,\n",
        "        image_text_alignment = None,\n",
        "        output_attentions = None,\n",
        "        output_hidden_states = None,\n",
        "        return_dict = None):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #         print('ac', all_concepts_.shape)\n",
        "        #         print('ii', input_embeds.shape)\n",
        "        #         print('am', attention_mask)\n",
        "        #         print('tti', token_type_ids)\n",
        "        #         print('pi', position_ids)\n",
        "        #         print('hm', head_mask)\n",
        "        #         print('ie', inputs_embeds)\n",
        "        #         print('ve', visual_embeds)\n",
        "        #         print('vam', visual_attention_mask)\n",
        "        #         print('vtti', visual_token_type_ids)\n",
        "        #         print(image_text_alignment)\n",
        "        #         print(output_attentions)\n",
        "        #         print(output_hidden_states)\n",
        "        #         print(return_dict)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        B = visual_embeds.size(0)\n",
        "\n",
        "\n",
        "        all_concepts = self.vb.proj_conc(all_concepts_)\n",
        "\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            inputs_embeds = self.vb.vb.embeddings.word_embeddings(input_ids.to('cuda'))\n",
        "        \"\"\"\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs_embeds = self.vb.vb.embeddings.word_embeddings(input_ids.to('cuda'))\n",
        "\n",
        "        #print(0/0)\n",
        "\n",
        "        inputs_embeds = self.vb.proj_embed(inputs_embeds)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        b = torch.einsum('blx,bxn->bln',inputs_embeds,all_concepts.transpose(1,2))\n",
        "\n",
        "\n",
        "        #print(b.shape)\n",
        "\n",
        "\n",
        "\n",
        "        b = b.masked_fill(~attention_mask.unsqueeze(dim=2).bool(), -np.inf)\n",
        "        c = F.softmax(b,dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        einsum_expression = 'bln,clnx->bclx'\n",
        "        inter = torch.einsum(einsum_expression, inputs_embeds, self.vb.W_ij)\n",
        "\n",
        "        k = torch.einsum('bcsn,bsc->bcn',inter,c)\n",
        "        norm = torch.norm(k,dim=2)\n",
        "\n",
        "\n",
        "\n",
        "        kk1 = ((norm**2)/(1+norm**2)).unsqueeze(dim=2) * (k/norm.unsqueeze(dim=2))\n",
        "\n",
        "        w_dynamic = kk1.norm(dim=2).unsqueeze(2)\n",
        "\n",
        "        num_reps = int(all_concepts.shape[0])\n",
        "        w_static = self.vb.w.unsqueeze(0).repeat(num_reps,1,1)\n",
        "\n",
        "\n",
        "        #         print('*'*10)\n",
        "\n",
        "        #         print(self.vb.w1.shape)\n",
        "        #         print(all_concepts_.shape)\n",
        "        #         print(w_static.shape)\n",
        "        #         print(w_dynamic.shape)\n",
        "\n",
        "\n",
        "\n",
        "        if DYN_ROUTE:\n",
        "            raw_latent = (1-self.vb.w1) * all_concepts_ * w_static + all_concepts_ * w_dynamic * self.vb.w1\n",
        "\n",
        "        else:\n",
        "            raw_latent = (1-self.vb.w1) * all_concepts_ * w_static\n",
        "\n",
        "\n",
        "\n",
        "        #raw_latent = (1-self.vb.w1) * all_concepts_ * w_static\n",
        "        raw_latent = torch.mean(raw_latent, dim=1)\n",
        "\n",
        "\n",
        "        latent = self.vb.project_latent(raw_latent)\n",
        "\n",
        "\n",
        "\n",
        "        latent = latent.unsqueeze(dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        visual_embeds_latent = torch.cat((visual_embeds,latent),dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #         num_reps = int(all_concepts.shape[0]/17)\n",
        "        #         w_static = self.vb.w.unsqueeze(0).repeat(num_reps,1,1).reshape(17*num_reps,1)\n",
        "        #         raw_latent = torch.mean(all_concepts*w_static,dim=0)\n",
        "        #         raw_latent = raw_latent.unsqueeze(dim=0)\n",
        "\n",
        "        #         latent = self.vb.project_latent(raw_latent)\n",
        "\n",
        "\n",
        "\n",
        "        #         latent = latent.repeat(B,1).unsqueeze(dim=1)\n",
        "\n",
        "\n",
        "        #         visual_embeds_latent = torch.cat((visual_embeds,latent),dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Create a tensor of ones with the same number of columns as the original tensor\n",
        "        ones_column_vam = torch.ones((visual_attention_mask.size(0), 1), dtype=visual_attention_mask.dtype)\n",
        "\n",
        "        # Concatenate the original tensor with the tensor of ones along the second dimension (columns)\n",
        "        visual_attention_mask_latent = torch.cat((visual_attention_mask, ones_column_vam.to('cuda')), dim=1)\n",
        "\n",
        "        # Create a tensor of ones with the same number of columns as the original tensor\n",
        "        ones_column_vtt = torch.ones((visual_token_type_ids.size(0), 1), dtype=visual_token_type_ids.dtype)\n",
        "\n",
        "        # Concatenate the original tensor with the tensor of ones along the second dimension (columns)\n",
        "        visual_token_type_ids_latent = torch.cat((visual_token_type_ids, ones_column_vtt.to('cuda')), dim=1)\n",
        "\n",
        "        #         print(latent.shape)\n",
        "\n",
        "        #         print(visual_embeds.shape)\n",
        "        #         print(visual_attention_mask.shape)\n",
        "        #         print(visual_token_type_ids.shape)\n",
        "\n",
        "        #         print(self.w)\n",
        "\n",
        "        x = self.vb.vb(inputs_embeds = input_embeds, token_type_ids = token_type_ids, attention_mask = attention_mask, visual_embeds=visual_embeds_latent, visual_attention_mask=visual_attention_mask_latent, visual_token_type_ids=visual_token_type_ids_latent)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        logits = self.vb.linear_relu_stack(x.pooler_output)\n",
        "        #print('softmax logit', torch.softmax(logits, dim=1))\n",
        "        #print(0/0)\n",
        "\n",
        "        #logits = torch.softmax(logits, dim=1)[:, 1].unsqueeze(1)\n",
        "\n",
        "        #print('previous',torch.softmax(logits, dim=1)[:, 1].unsqueeze(1) )\n",
        "\n",
        "        logits = torch.softmax(logits, dim=1).max(dim=1).values.unsqueeze(1)\n",
        "\n",
        "        #print('logits', logits)\n",
        "\n",
        "        #print(0/0)\n",
        "\n",
        "        #print(logits)\n",
        "        #print(logits.shape)\n",
        "\n",
        "\n",
        "\n",
        "        return logits\n",
        "\n",
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:32:05.394525Z",
          "iopub.execute_input": "2024-09-21T10:32:05.394822Z",
          "iopub.status.idle": "2024-09-21T10:32:05.415237Z",
          "shell.execute_reply.started": "2024-09-21T10:32:05.394799Z",
          "shell.execute_reply": "2024-09-21T10:32:05.414277Z"
        },
        "trusted": true,
        "id": "5q3vuPmzUhlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "vb = VB()\n",
        "\n",
        "vb.load_state_dict(torch.load('/kaggle/working/final_model_concept.pt'))\n",
        "vb.to('cuda')\n",
        "vb.eval()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:32:05.416278Z",
          "iopub.execute_input": "2024-09-21T10:32:05.416547Z",
          "iopub.status.idle": "2024-09-21T10:32:06.463671Z",
          "shell.execute_reply.started": "2024-09-21T10:32:05.416508Z",
          "shell.execute_reply": "2024-09-21T10:32:06.462705Z"
        },
        "trusted": true,
        "id": "69FmPmW6UhlX",
        "outputId": "9cac7ca2-32b2-4960-aae9-957b28ab9dd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 55,
          "output_type": "execute_result",
          "data": {
            "text/plain": "VB(\n  (vb): VisualBertModel(\n    (embeddings): VisualBertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=1)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n      (visual_token_type_embeddings): Embedding(2, 768)\n      (visual_position_embeddings): Embedding(512, 768)\n      (visual_projection): Linear(in_features=2048, out_features=768, bias=True)\n    )\n    (encoder): VisualBertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x VisualBertLayer(\n          (attention): VisualBertAttention(\n            (self): VisualBertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): VisualBertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): VisualBertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): VisualBertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): VisualBertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (linear_relu_stack): Sequential(\n    (0): Dropout(p=0.1, inplace=False)\n    (1): Linear(in_features=768, out_features=512, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=512, out_features=128, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=128, out_features=2, bias=True)\n  )\n  (linear_relu_stack_latent): Sequential(\n    (0): Dropout(p=0.1, inplace=False)\n    (1): Linear(in_features=768, out_features=512, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=512, out_features=128, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=128, out_features=2, bias=True)\n  )\n  (linear_relu_stack_hidden): Sequential(\n    (0): Dropout(p=0.1, inplace=False)\n    (1): Linear(in_features=768, out_features=512, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=512, out_features=128, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=128, out_features=2, bias=True)\n  )\n  (project_latent): Linear(in_features=768, out_features=2048, bias=True)\n  (proj_conc): Linear(in_features=768, out_features=28, bias=True)\n  (proj_embed): Linear(in_features=768, out_features=28, bias=True)\n  (grad_rev1): RevGrad()\n  (grad_rev2): RevGrad()\n)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "vb_c = VB()\n",
        "vb_c.to(device)\n",
        "# concept_classes = [\"jew\", \"holocaust\", \"hitler\", \"genocide\", \"funny\", \"muslim\", \"terrorism\", \"violence\", \"politics\", \"naive\", \"international relation\"]\n",
        "\n",
        "\n",
        "# concept_embedding_map = {}\n",
        "\n",
        "# all_concepts = []\n",
        "\n",
        "\n",
        "# for c in concept_classes:\n",
        "#     conc = torch.tensor(tokenizer.encode(c)[1:-1])\n",
        "#     e = vb_c.vb.embeddings.word_embeddings(conc.to('cuda'))\n",
        "#     x = e.detach().cpu().mean(dim=0)\n",
        "\n",
        "#     all_concepts.append(x)\n",
        "\n",
        "#     concept_embedding_map[c] = x\n",
        "\n",
        "\n",
        "# all_concepts = torch.stack(all_concepts)\n",
        "\n",
        "# concept_classes = [\"jew\", \"holocaust\", \"hitler\", \"genocide\", \"funny\", \"muslim\", \"terrorism\", \"violence\", \"politics\", \"racism\", \"international relation\", \"sex\", \"gore\", \"violence\", \"immigration\", \"extremism\", \"immoral\"]\n",
        "\n",
        "concept_classes = [\"holocaust\", \"nazism\", \"genocide\", \"funny\", \"anti muslim\", \"terrorism\", \"violence\", \"politics\", \"racism\", \"international relation\", \"adult\", \"gore\", \"misogynistic\", \"immigration\", \"extremism\", \"immoral\", \"white supremacy\", \"indecency\"]\n",
        "\n",
        "\n",
        "concept_embedding_map = {}\n",
        "\n",
        "all_concepts = []\n",
        "\n",
        "\n",
        "for c in concept_classes:\n",
        "    conc = torch.tensor(tokenizer.encode(c)[1:-1])\n",
        "    e = vb_c.vb.embeddings.word_embeddings(conc.to('cuda'))\n",
        "    x = e.detach().cpu().mean(dim=0)\n",
        "\n",
        "\n",
        "    all_concepts.append(x)\n",
        "\n",
        "    concept_embedding_map[c] = x\n",
        "\n",
        "\n",
        "all_concepts = torch.stack(all_concepts)\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:32:09.835678Z",
          "iopub.execute_input": "2024-09-21T10:32:09.836437Z",
          "iopub.status.idle": "2024-09-21T10:32:10.533112Z",
          "shell.execute_reply.started": "2024-09-21T10:32:09.836400Z",
          "shell.execute_reply": "2024-09-21T10:32:10.532306Z"
        },
        "trusted": true,
        "id": "Vv3-r1h2UhlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "def get_inlp(all_concepts):\n",
        "\n",
        "    num_concepts = 18\n",
        "    all_concepts_ = all_concepts.cpu().numpy()\n",
        "    multiplier = np.logical_xor(1, np.eye(num_concepts)).astype(np.int32)/ (num_concepts-1)\n",
        "    X = np.matmul(multiplier,all_concepts_)\n",
        "    assert X.shape[0]==num_concepts\n",
        "    Y = np.arange(num_concepts)\n",
        "\n",
        "    #             x_train_cp = x_train\n",
        "    #             x_test_cp = x_test\n",
        "    x_train_cp = X\n",
        "    x_test_cp = X\n",
        "    y_train = Y\n",
        "    y_test = Y\n",
        "\n",
        "    for i in range(1):\n",
        "\n",
        "        clf = svm.SVC(kernel=\"linear\")\n",
        "        scr = train_network(clf, x_train_cp, y_train, x_test_cp, y_test)\n",
        "\n",
        "        W = get_weights(clf)\n",
        "        P_i = get_nullspace_projection(W)\n",
        "        x_train_cp = x_train_cp.dot(P_i)\n",
        "        x_test_cp = x_test_cp.dot(P_i)\n",
        "\n",
        "        clf = svm.SVC(kernel=\"linear\")\n",
        "        train_network(clf, x_train_cp, y_train, x_test_cp, y_test)\n",
        "\n",
        "    all_concepts_ = torch.tensor(all_concepts_)\n",
        "    all_concepts_ = all_concepts_.mm(torch.tensor(P_i, dtype = all_concepts_.dtype))\n",
        "\n",
        "    return all_concepts_"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:32:12.061986Z",
          "iopub.execute_input": "2024-09-21T10:32:12.062742Z",
          "iopub.status.idle": "2024-09-21T10:32:12.070965Z",
          "shell.execute_reply.started": "2024-09-21T10:32:12.062709Z",
          "shell.execute_reply": "2024-09-21T10:32:12.070065Z"
        },
        "trusted": true,
        "id": "SaDRhtOdUhlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "#torch.save(vb.state_dict(), '/kaggle/working/final_model_concept_inlp.pt')\n",
        "# check evaluation performance separately on p and q using their classifier: should be less\n",
        "pred_r1, pred_r2 = [],[]\n",
        "pred_both = []\n",
        "act = []\n",
        "if DECONFOUND:\n",
        "    all_concepts_ = get_inlp(all_concepts).to('cuda')\n",
        "else:\n",
        "    all_concepts_ = all_concepts.to('cuda')\n",
        "\n",
        "for ii, batch in enumerate(eval_dataloader):\n",
        "    with torch.no_grad():\n",
        "        batch_inp = {k: v.to(device) for k, v in batch[0].items()}\n",
        "        outputs, r1, r2,_,_ = vb(all_concepts_.to('cuda'),**batch_inp)\n",
        "        act.append(batch[1].detach().cpu().numpy()[0])\n",
        "\n",
        "        pred_r1.append(np.argmax(r1.detach().cpu().numpy()[0]))\n",
        "        pred_r2.append(np.argmax(r2.detach().cpu().numpy()[0]))\n",
        "        pred_both.append(np.argmax(outputs.detach().cpu().numpy()[0]))\n",
        "\n",
        "print('*')\n",
        "print(f1_score(act,pred_r1,average='macro'))\n",
        "print(f1_score(act,pred_r1,average='weighted'))\n",
        "print(accuracy_score(act,pred_r1))\n",
        "print('*'*10)\n",
        "print(f1_score(act,pred_r2,average='macro'))\n",
        "print(f1_score(act,pred_r2,average='weighted'))\n",
        "print(accuracy_score(act,pred_r2))\n",
        "print('*'*10)\n",
        "print(f1_score(act,pred_both,average='macro'))\n",
        "print(f1_score(act,pred_both,average='weighted'))\n",
        "print(accuracy_score(act,pred_both))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:32:14.450877Z",
          "iopub.execute_input": "2024-09-21T10:32:14.451736Z",
          "iopub.status.idle": "2024-09-21T10:32:29.877931Z",
          "shell.execute_reply.started": "2024-09-21T10:32:14.451702Z",
          "shell.execute_reply": "2024-09-21T10:32:29.877023Z"
        },
        "trusted": true,
        "id": "rJh5a_dDUhlZ",
        "outputId": "fa14b4e4-061b-474f-de4e-33f043a8a09e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "*\n0.40647482014388486\n0.5567473294091998\n0.6848484848484848\n**********\n0.40647482014388486\n0.5567473294091998\n0.6848484848484848\n**********\n0.7036565354754966\n0.7469814601650707\n0.75\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('h')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:32:29.879630Z",
          "iopub.execute_input": "2024-09-21T10:32:29.879921Z",
          "iopub.status.idle": "2024-09-21T10:32:29.884686Z",
          "shell.execute_reply.started": "2024-09-21T10:32:29.879896Z",
          "shell.execute_reply": "2024-09-21T10:32:29.883797Z"
        },
        "trusted": true,
        "id": "z6SST_kDUhlZ",
        "outputId": "1d698533-f57b-4b85-959a-80729811311b"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "h\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "def model_pred(input_ids, attention_mask, token_type_ids, visual_embeds, visual_attention_mask, visual_token_type_ids, all_concepts, concept_no=None):\n",
        "\n",
        "\n",
        "\n",
        "    B = visual_embeds.size(0)\n",
        "\n",
        "\n",
        "    #######################################################\n",
        "    input_ids = input_ids.to('cuda')\n",
        "    attention_mask = attention_mask.to('cuda')\n",
        "    token_type_ids = token_type_ids.to('cuda')\n",
        "    visual_embeds = visual_embeds.to('cuda')\n",
        "    visual_attention_mask = visual_attention_mask.to('cuda')\n",
        "    visual_token_type_ids = visual_token_type_ids.to('cuda')\n",
        "\n",
        "    #####################################################################\n",
        "\n",
        "    ##############################################\n",
        "\n",
        "    all_concepts = all_concepts.to('cuda')\n",
        "\n",
        "    # deconfounding step by passing the concepts through INLP (1)\n",
        "    if DECONFOUND:\n",
        "        all_concepts_ = get_inlp(all_concepts).to('cuda')\n",
        "    else:\n",
        "        all_concepts_ = all_concepts.to('cuda')\n",
        "\n",
        "    # this is without deconfounding while testing (2)\n",
        "    # either use (1) or (2) or mean of (1) and (2)\n",
        "\n",
        "    # all_concepts_ = all_concepts.to('cuda')\n",
        "\n",
        "    #all_concepts_ = (get_inlp(all_concepts).to('cuda') + all_concepts.to('cuda')) / 2\n",
        "\n",
        "    all_concepts = vb.proj_conc(all_concepts_)\n",
        "\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        inputs_embeds = vb.vb.embeddings.word_embeddings(input_ids.to('cuda'))\n",
        "\n",
        "    #print(0/0)\n",
        "\n",
        "    inputs_embeds = vb.proj_embed(inputs_embeds)\n",
        "\n",
        "\n",
        "    #####################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #B, 64, 28, #B, 28, 17\n",
        "\n",
        "    b = torch.einsum('blx,bxn->bln',inputs_embeds,all_concepts.repeat(B,1,1).transpose(1,2))\n",
        "\n",
        "\n",
        "\n",
        "    b = b.masked_fill(~attention_mask.unsqueeze(dim=2).bool(), -np.inf)\n",
        "    c = F.softmax(b,dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    einsum_expression = 'bln,clnx->bclx'\n",
        "    inter = torch.einsum(einsum_expression, inputs_embeds, vb.W_ij)\n",
        "\n",
        "    k = torch.einsum('bcsn,bsc->bcn',inter,c)\n",
        "    norm = torch.norm(k,dim=2)\n",
        "\n",
        "\n",
        "\n",
        "    kk1 = ((norm**2)/(1+norm**2)).unsqueeze(dim=2) * (k/norm.unsqueeze(dim=2))\n",
        "\n",
        "    w_dynamic = kk1.norm(dim=2).unsqueeze(2)\n",
        "\n",
        "    #     print(w_dynamic.shape)\n",
        "    #     print(vb.w.shape)\n",
        "\n",
        "\n",
        "    #     print(w_dynamic)\n",
        "    #     print(vb.w)\n",
        "    #     print('*'*10)\n",
        "\n",
        "\n",
        "    #print(concept_no)\n",
        "    if concept_no==None:\n",
        "        k_static = vb.w.data.clone()\n",
        "        k_dynamic = w_dynamic.data.clone()\n",
        "    else:\n",
        "        k_static = vb.w.data.clone()\n",
        "        k_dynamic = w_dynamic.data.clone()\n",
        "        k_static[concept_no][0] = 0\n",
        "        k_dynamic[0][concept_no][0] = 0\n",
        "\n",
        "    #     print(k_dynamic)\n",
        "    #     print(k_static)\n",
        "    #     print('*'*10)\n",
        "\n",
        "    #     print(0/0)\n",
        "\n",
        "    #either use the following line or raw_latent = (1-vb.w1) * all_concepts_.unsqueeze(0).repeat(B,1,1) * k_static.unsqueeze(dim=0).repeat(B,1,1)\n",
        "\n",
        "    if DYN_ROUTE:\n",
        "        raw_latent = (1-vb.w1) * all_concepts_.unsqueeze(0).repeat(B,1,1) * k_static.unsqueeze(dim=0).repeat(B,1,1) + all_concepts_.unsqueeze(0).repeat(B,1,1) * k_dynamic * vb.w1\n",
        "    else:\n",
        "        raw_latent = (1-vb.w1) * all_concepts_.unsqueeze(0).repeat(B,1,1) * k_static.unsqueeze(dim=0).repeat(B,1,1)\n",
        "\n",
        "    #raw_latent = 0.0 * all_concepts_.unsqueeze(0).repeat(B,1,1) * k_static.unsqueeze(dim=0).repeat(B,1,1) + all_concepts_.unsqueeze(0).repeat(B,1,1) * k_dynamic * 1.0\n",
        "\n",
        "    #raw_latent = (1-vb.w1) * all_concepts_.unsqueeze(0).repeat(B,1,1) * k_static.unsqueeze(dim=0).repeat(B,1,1)\n",
        "\n",
        "    raw_latent = torch.mean(raw_latent, dim=1)\n",
        "\n",
        "\n",
        "\n",
        "    latent = vb.project_latent(raw_latent)\n",
        "\n",
        "\n",
        "\n",
        "    latent = latent.unsqueeze(dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    visual_embeds_latent = torch.cat((visual_embeds,latent),dim=1)\n",
        "\n",
        "\n",
        "    ################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Create a tensor of ones with the same number of columns as the original tensor\n",
        "    ones_column_vam = torch.ones((visual_attention_mask.size(0), 1), dtype=visual_attention_mask.dtype)\n",
        "\n",
        "    # Concatenate the original tensor with the tensor of ones along the second dimension (columns)\n",
        "    visual_attention_mask_latent = torch.cat((visual_attention_mask.to('cuda'), ones_column_vam.to('cuda')), dim=1)\n",
        "\n",
        "    # Create a tensor of ones with the same number of columns as the original tensor\n",
        "    ones_column_vtt = torch.ones((visual_token_type_ids.size(0), 1), dtype=visual_token_type_ids.dtype)\n",
        "\n",
        "    # Concatenate the original tensor with the tensor of ones along the second dimension (columns)\n",
        "    visual_token_type_ids_latent = torch.cat((visual_token_type_ids.to('cuda'), ones_column_vtt.to('cuda')), dim=1)\n",
        "\n",
        "\n",
        "\n",
        "    x = vb.vb(input_ids = input_ids.to('cuda'), token_type_ids = token_type_ids.to('cuda'), attention_mask = attention_mask.to('cuda'), visual_embeds=visual_embeds_latent.to('cuda'), visual_attention_mask=visual_attention_mask_latent.to('cuda'), visual_token_type_ids=visual_token_type_ids_latent.to('cuda'))\n",
        "\n",
        "    p = vb.vb(input_ids = input_ids.to('cuda'), token_type_ids = token_type_ids.to('cuda'), attention_mask = attention_mask.to('cuda'), visual_embeds=visual_embeds.to('cuda'), visual_attention_mask=visual_attention_mask.to('cuda'), visual_token_type_ids=visual_token_type_ids.to('cuda'))\n",
        "    p = vb.grad_rev1(p.pooler_output)\n",
        "    q = vb.grad_rev2(raw_latent.repeat(B,1))\n",
        "\n",
        "    logits_p = vb.linear_relu_stack_hidden(p)\n",
        "    logits_q = vb.linear_relu_stack_latent(q.to('cuda'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    logits = vb.linear_relu_stack(x.pooler_output)\n",
        "\n",
        "\n",
        "    if_logits_p = vb.linear_relu_stack(p)\n",
        "    if_logits_q = vb.linear_relu_stack(q.to('cuda'))\n",
        "\n",
        "    return torch.softmax(logits,dim=-1), x.pooler_output"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:32:33.662550Z",
          "iopub.execute_input": "2024-09-21T10:32:33.663194Z",
          "iopub.status.idle": "2024-09-21T10:32:33.689715Z",
          "shell.execute_reply.started": "2024-09-21T10:32:33.663153Z",
          "shell.execute_reply": "2024-09-21T10:32:33.688767Z"
        },
        "trusted": true,
        "id": "zM2jleyhUhlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:32:35.195247Z",
          "iopub.execute_input": "2024-09-21T10:32:35.195628Z",
          "iopub.status.idle": "2024-09-21T10:32:35.200074Z",
          "shell.execute_reply.started": "2024-09-21T10:32:35.195600Z",
          "shell.execute_reply": "2024-09-21T10:32:35.199074Z"
        },
        "trusted": true,
        "id": "z3fM5uMtUhla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(50):\n",
        "    if train_set[i]['label']==1:\n",
        "        print(i, train_set[i])\n",
        "        Image.open('/kaggle/input/facebook-hateful-meme-dataset/data/'+train_set[i]['img'])\n",
        "        inputs = {}\n",
        "        inputs = tokenizer(train_set[i]['text'], padding=\"max_length\", truncation=True, max_length=64, return_tensors='pt')\n",
        "\n",
        "\n",
        "        inputs['input_ids'] = inputs['input_ids']\n",
        "        inputs['token_type_ids'] = inputs['token_type_ids']\n",
        "        inputs['attention_mask'] = inputs['attention_mask']\n",
        "\n",
        "        input_ids = inputs['input_ids']\n",
        "        token_type_ids = inputs['token_type_ids']\n",
        "        attention_mask = inputs['attention_mask']\n",
        "\n",
        "        visual_embeds,visual_token_type_ids,visual_attention_mask = get_visual_embedding('/kaggle/input/facebook-hateful-meme-dataset/data/'+train_set[i]['img'])\n",
        "        # visual_embeds,visual_token_type_ids,visual_attention_mask = visual_feats[train_set[874]['img']]\n",
        "\n",
        "\n",
        "\n",
        "        inputs.update({\n",
        "          \"visual_embeds\":  visual_embeds,\n",
        "          \"visual_token_type_ids\": visual_token_type_ids,\n",
        "          \"visual_attention_mask\": visual_attention_mask\n",
        "        })\n",
        "\n",
        "\n",
        "        class_map = {0:'non offensive', 1: 'offensive'}\n",
        "        #print('original model prediction {}'.format(model_pred(input_ids,attention_mask,token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids)))\n",
        "        orig,_ = model_pred(input_ids,attention_mask,token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids,all_concepts)\n",
        "        orig = orig[0]\n",
        "        print(orig)\n",
        "        print('predicted class {}'.format(class_map[np.argmax(orig.detach().cpu().numpy())]))\n",
        "        pred_class = np.argmax(orig.detach().cpu().numpy())\n",
        "        orig = orig[pred_class]\n",
        "\n",
        "        k = []\n",
        "        for i in range(18):\n",
        "            #print('Removing concept: {}'.format(concept_classes[i]))\n",
        "\n",
        "            after_intervention,_ = model_pred(input_ids,attention_mask,token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids,all_concepts,concept_no=i)\n",
        "            #print('hello')\n",
        "            after_intervention = after_intervention[0][pred_class]\n",
        "            #print(after_intervention, orig)\n",
        "            #print(0/0)\n",
        "            k.append(abs(after_intervention-orig).item())\n",
        "            #print('model prediction after removal {}'.format(model_pred(input_ids,attention_mask,token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids,concept_no=i)))\n",
        "        k = np.asarray(k)\n",
        "        print(k)\n",
        "        k = list(map(lambda x: ((x-np.mean(k))/np.mean(k))*100, k))\n",
        "\n",
        "        conc_scr = []\n",
        "        for i in range(17):\n",
        "            cc = concept_classes[i]\n",
        "            print('relative ICaCE scores for concept {} is {}%'.format(cc,k[i]))\n",
        "            conc_scr.append((cc,k[i]))\n",
        "        conc_scr.sort(key=lambda tup: tup[1], reverse=True)\n",
        "\n",
        "        for i in conc_scr:\n",
        "            if i[1]> 0:\n",
        "                print(i[0], end = \" \")\n",
        "        print(\"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:32:35.887359Z",
          "iopub.execute_input": "2024-09-21T10:32:35.887736Z",
          "iopub.status.idle": "2024-09-21T10:32:48.834387Z",
          "shell.execute_reply.started": "2024-09-21T10:32:35.887708Z",
          "shell.execute_reply": "2024-09-21T10:32:48.832968Z"
        },
        "trusted": true,
        "id": "otR-UB_xUhla",
        "outputId": "c5af5fa2-9e9a-47c6-d408-01886db7d929"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "10 {'text': 'jew mad? get fuhrerious!', 'img': 'img/79351.png', 'label': 1}\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:3526.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "tensor([0.2246, 0.7754], device='cuda:0', grad_fn=<SelectBackward0>)\npredicted class offensive\n[0.00029802 0.00029796 0.00029647 0.0002957  0.0002982  0.00029802\n 0.0002982  0.0002982  0.00029701 0.00029832 0.0002982  0.00029796\n 0.00029808 0.00029701 0.00029755 0.00029844 0.00029582 0.00029677]\nrelative ICaCE scores for concept holocaust is 0.15802710943933357%\nrelative ICaCE scores for concept nazism is 0.1379955040174457%\nrelative ICaCE scores for concept genocide is -0.362794631529751%\nrelative ICaCE scores for concept funny is -0.6232055020142933%\nrelative ICaCE scores for concept anti muslim is 0.21812192570499714%\nrelative ICaCE scores for concept terrorism is 0.15802710943933357%\nrelative ICaCE scores for concept violence is 0.21812192570499714%\nrelative ICaCE scores for concept politics is 0.21812192570499714%\nrelative ICaCE scores for concept racism is -0.18251018273276015%\nrelative ICaCE scores for concept international relation is 0.2581851365487729%\nrelative ICaCE scores for concept adult is 0.21812192570499714%\nrelative ICaCE scores for concept gore is 0.1379955040174457%\nrelative ICaCE scores for concept misogynistic is 0.17805871486122143%\nrelative ICaCE scores for concept immigration is -0.18251018273276015%\nrelative ICaCE scores for concept extremism is -0.002225733935769367%\nrelative ICaCE scores for concept immoral is 0.29824834739254863%\nrelative ICaCE scores for concept white supremacy is -0.5831422911705175%\nimmoral international relation anti muslim violence politics adult misogynistic holocaust terrorism nazism gore \n\n12 {'text': 'brother... a day without a blast is a day wasted', 'img': 'img/25489.png', 'label': 1}\ntensor([0.8937, 0.1063], device='cuda:0', grad_fn=<SelectBackward0>)\npredicted class non offensive\n[0.00020522 0.0002057  0.0002057  0.00020552 0.00020534 0.0002057\n 0.00020558 0.00020558 0.00020534 0.0002057  0.00020552 0.00020558\n 0.00020534 0.0002057  0.00020552 0.0002057  0.00020552 0.00020558]\nrelative ICaCE scores for concept holocaust is -0.15788117025389575%\nrelative ICaCE scores for concept nazism is 0.07410748807836355%\nrelative ICaCE scores for concept genocide is 0.07410748807836355%\nrelative ICaCE scores for concept funny is -0.012888258796233697%\nrelative ICaCE scores for concept anti muslim is -0.09988400567083094%\nrelative ICaCE scores for concept terrorism is 0.07410748807836355%\nrelative ICaCE scores for concept violence is 0.016110323495298718%\nrelative ICaCE scores for concept politics is 0.016110323495298718%\nrelative ICaCE scores for concept racism is -0.09988400567083094%\nrelative ICaCE scores for concept international relation is 0.07410748807836355%\nrelative ICaCE scores for concept adult is -0.012888258796233697%\nrelative ICaCE scores for concept gore is 0.016110323495298718%\nrelative ICaCE scores for concept misogynistic is -0.09988400567083094%\nrelative ICaCE scores for concept immigration is 0.07410748807836355%\nrelative ICaCE scores for concept extremism is -0.012888258796233697%\nrelative ICaCE scores for concept immoral is 0.07410748807836355%\nrelative ICaCE scores for concept white supremacy is -0.012888258796233697%\nnazism genocide terrorism international relation immigration immoral violence politics gore \n\n27 {'text': \"is bribing muslims for liberal votes justin trudeau's only skill? why does justin trudeau love foreigners so much while openly disrespecting canadians, canadian values, our history and traditions, our seniors and veterans? sharia law has no place in canada! never has... never will.\", 'img': 'img/72640.png', 'label': 1}\ntensor([0.1694, 0.8306], device='cuda:0', grad_fn=<SelectBackward0>)\npredicted class offensive\n[7.15255737e-07 7.15255737e-07 7.15255737e-07 7.15255737e-07\n 7.15255737e-07 7.15255737e-07 7.15255737e-07 7.15255737e-07\n 7.15255737e-07 7.74860382e-07 7.15255737e-07 7.15255737e-07\n 7.15255737e-07 7.15255737e-07 7.74860382e-07 7.15255737e-07\n 7.15255737e-07 7.15255737e-07]\nrelative ICaCE scores for concept holocaust is -0.9174311926605472%\nrelative ICaCE scores for concept nazism is -0.9174311926605472%\nrelative ICaCE scores for concept genocide is -0.9174311926605472%\nrelative ICaCE scores for concept funny is -0.9174311926605472%\nrelative ICaCE scores for concept anti muslim is -0.9174311926605472%\nrelative ICaCE scores for concept terrorism is -0.9174311926605472%\nrelative ICaCE scores for concept violence is -0.9174311926605472%\nrelative ICaCE scores for concept politics is -0.9174311926605472%\nrelative ICaCE scores for concept racism is -0.9174311926605472%\nrelative ICaCE scores for concept international relation is 7.339449541284407%\nrelative ICaCE scores for concept adult is -0.9174311926605472%\nrelative ICaCE scores for concept gore is -0.9174311926605472%\nrelative ICaCE scores for concept misogynistic is -0.9174311926605472%\nrelative ICaCE scores for concept immigration is -0.9174311926605472%\nrelative ICaCE scores for concept extremism is 7.339449541284407%\nrelative ICaCE scores for concept immoral is -0.9174311926605472%\nrelative ICaCE scores for concept white supremacy is -0.9174311926605472%\ninternational relation extremism \n\n30 {'text': \"d.j. osama spin laden droppin' beats like the twin towers\", 'img': 'img/93547.png', 'label': 1}\ntensor([0.2857, 0.7143], device='cuda:0', grad_fn=<SelectBackward0>)\npredicted class offensive\n[0.00069511 0.00069517 0.00069755 0.00069517 0.00069827 0.00069535\n 0.00069821 0.00069696 0.00069648 0.00069809 0.00069487 0.00069636\n 0.00069779 0.0006966  0.00069463 0.00069839 0.00069684 0.00069678]\nrelative ICaCE scores for concept holocaust is -0.21248983899260368%\nrelative ICaCE scores for concept nazism is -0.2039332011808212%\nrelative ICaCE scores for concept genocide is 0.1383323112904784%\nrelative ICaCE scores for concept funny is -0.2039332011808212%\nrelative ICaCE scores for concept anti muslim is 0.24101196503186825%\nrelative ICaCE scores for concept terrorism is -0.17826328774547373%\nrelative ICaCE scores for concept violence is 0.23245532722008577%\nrelative ICaCE scores for concept politics is 0.052765933172653504%\nrelative ICaCE scores for concept racism is -0.01568716932160642%\nrelative ICaCE scores for concept international relation is 0.2153420515965208%\nrelative ICaCE scores for concept adult is -0.24671639023973366%\nrelative ICaCE scores for concept gore is -0.0328004449451714%\nrelative ICaCE scores for concept misogynistic is 0.17255886253760835%\nrelative ICaCE scores for concept immigration is 0.00142610630195856%\nrelative ICaCE scores for concept extremism is -0.2809429414868636%\nrelative ICaCE scores for concept immoral is 0.2581252406554333%\nrelative ICaCE scores for concept white supremacy is 0.03565265754908852%\nimmoral anti muslim violence international relation misogynistic genocide politics white supremacy immigration \n\n48 {'text': 'we said we would never forget why are you voting them into our government?', 'img': 'img/74386.png', 'label': 1}\ntensor([0.7734, 0.2266], device='cuda:0', grad_fn=<SelectBackward0>)\npredicted class non offensive\n[0.00176328 0.00176358 0.00176072 0.00176316 0.00176698 0.00176239\n 0.00176471 0.00176406 0.00176191 0.00176257 0.00176316 0.00176817\n 0.00176454 0.00176203 0.00175959 0.00176585 0.00176454 0.00176501]\nrelative ICaCE scores for concept holocaust is -0.022530387860631117%\nrelative ICaCE scores for concept nazism is -0.005632596965160853%\nrelative ICaCE scores for concept genocide is -0.1678513895616754%\nrelative ICaCE scores for concept funny is -0.02928950421881922%\nrelative ICaCE scores for concept anti muslim is 0.18700221924320015%\nrelative ICaCE scores for concept terrorism is -0.07322376054704191%\nrelative ICaCE scores for concept violence is 0.05857900843762615%\nrelative ICaCE scores for concept politics is 0.02140386846759157%\nrelative ICaCE scores for concept racism is -0.10026022597979434%\nrelative ICaCE scores for concept international relation is -0.06308508600975975%\nrelative ICaCE scores for concept adult is -0.02928950421881922%\nrelative ICaCE scores for concept gore is 0.25459338282508126%\nrelative ICaCE scores for concept misogynistic is 0.048440333900343996%\nrelative ICaCE scores for concept immigration is -0.09350110962160622%\nrelative ICaCE scores for concept extremism is -0.2320629949644624%\nrelative ICaCE scores for concept immoral is 0.12279061384041316%\nrelative ICaCE scores for concept white supremacy is 0.048440333900343996%\ngore anti muslim immoral violence misogynistic white supremacy politics \n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from captum.attr import IntegratedGradients\n",
        "\n",
        "# # input1 = torch.tensor([[1.0, 3.0], [3.0, 5.0]], requires_grad=True)\n",
        "# # input2 = torch.tensor([[1.0, 4.0], [0.0, 2.0]], requires_grad=True)\n",
        "# # # Initializing our toy model\n",
        "# # model = ToyModel_With_Additional_Forward_Args()\n",
        "# # # Applying integrated gradients on the input\n",
        "# ig = IntegratedGradients(vb)\n",
        "# (input1_attr, input2_attr), delta = ig.attribute((input1, input2), n_steps=100,\n",
        "#                                     additional_forward_args=1, return_convergence_delta=True)\n",
        "# output\n",
        "# .........\n",
        "# input1_attr: tensor([[0.0000, 0.0000],\n",
        "#                      [0.0000, 3.3428]], grad_fn=<MulBackward0>)\n",
        "# input2_attr:  tensor([[ 0.0000,  0.0000],\n",
        "#                       [0.0000, -1.3371]], grad_fn=<MulBackward0>)\n",
        "# approximation_error (aka delta): 0.005693793296813965"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:32:52.119644Z",
          "iopub.execute_input": "2024-09-21T10:32:52.120291Z",
          "iopub.status.idle": "2024-09-21T10:32:52.126229Z",
          "shell.execute_reply.started": "2024-09-21T10:32:52.120258Z",
          "shell.execute_reply": "2024-09-21T10:32:52.125159Z"
        },
        "trusted": true,
        "id": "s3ZHalJBUhla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "concept_classes"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:32:52.417998Z",
          "iopub.execute_input": "2024-09-21T10:32:52.418377Z",
          "iopub.status.idle": "2024-09-21T10:32:52.424370Z",
          "shell.execute_reply.started": "2024-09-21T10:32:52.418347Z",
          "shell.execute_reply": "2024-09-21T10:32:52.423507Z"
        },
        "trusted": true,
        "id": "9sw9Rg-tUhlb",
        "outputId": "77e014e6-a26a-4888-e9d4-ecd3ed5cdd13"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 64,
          "output_type": "execute_result",
          "data": {
            "text/plain": "['holocaust',\n 'nazism',\n 'genocide',\n 'funny',\n 'anti muslim',\n 'terrorism',\n 'violence',\n 'politics',\n 'racism',\n 'international relation',\n 'adult',\n 'gore',\n 'misogynistic',\n 'immigration',\n 'extremism',\n 'immoral',\n 'white supremacy',\n 'indecency']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vb"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:32:55.055996Z",
          "iopub.execute_input": "2024-09-21T10:32:55.056684Z",
          "iopub.status.idle": "2024-09-21T10:32:55.065131Z",
          "shell.execute_reply.started": "2024-09-21T10:32:55.056652Z",
          "shell.execute_reply": "2024-09-21T10:32:55.064344Z"
        },
        "trusted": true,
        "id": "pv_jwqMyUhlb",
        "outputId": "a2e33eb3-c3cd-4ecd-a83e-d2852d14afa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 65,
          "output_type": "execute_result",
          "data": {
            "text/plain": "VB(\n  (vb): VisualBertModel(\n    (embeddings): VisualBertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=1)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n      (visual_token_type_embeddings): Embedding(2, 768)\n      (visual_position_embeddings): Embedding(512, 768)\n      (visual_projection): Linear(in_features=2048, out_features=768, bias=True)\n    )\n    (encoder): VisualBertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x VisualBertLayer(\n          (attention): VisualBertAttention(\n            (self): VisualBertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): VisualBertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): VisualBertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): VisualBertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): VisualBertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (linear_relu_stack): Sequential(\n    (0): Dropout(p=0.1, inplace=False)\n    (1): Linear(in_features=768, out_features=512, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=512, out_features=128, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=128, out_features=2, bias=True)\n  )\n  (linear_relu_stack_latent): Sequential(\n    (0): Dropout(p=0.1, inplace=False)\n    (1): Linear(in_features=768, out_features=512, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=512, out_features=128, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=128, out_features=2, bias=True)\n  )\n  (linear_relu_stack_hidden): Sequential(\n    (0): Dropout(p=0.1, inplace=False)\n    (1): Linear(in_features=768, out_features=512, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=512, out_features=128, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=128, out_features=2, bias=True)\n  )\n  (project_latent): Linear(in_features=768, out_features=2048, bias=True)\n  (proj_conc): Linear(in_features=768, out_features=28, bias=True)\n  (proj_embed): Linear(in_features=768, out_features=28, bias=True)\n  (grad_rev1): RevGrad()\n  (grad_rev2): RevGrad()\n)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "baselines = (torch.randn(20,18,768).to('cuda'), torch.randn(20,64,768).to('cuda'), torch.randn(20,36,2048).to('cuda'))\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:32:58.004368Z",
          "iopub.execute_input": "2024-09-21T10:32:58.005047Z",
          "iopub.status.idle": "2024-09-21T10:32:58.036025Z",
          "shell.execute_reply.started": "2024-09-21T10:32:58.005013Z",
          "shell.execute_reply": "2024-09-21T10:32:58.034807Z"
        },
        "trusted": true,
        "id": "o3HnfQyqUhlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "from captum.attr import IntegratedGradients,Saliency,DeepLift,DeepLiftShap, GradientShap, InputXGradient, KernelShap, FeatureAblation\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:32:58.905294Z",
          "iopub.execute_input": "2024-09-21T10:32:58.905912Z",
          "iopub.status.idle": "2024-09-21T10:32:58.973400Z",
          "shell.execute_reply.started": "2024-09-21T10:32:58.905881Z",
          "shell.execute_reply": "2024-09-21T10:32:58.972437Z"
        },
        "trusted": true,
        "id": "niLdfOEhUhlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.manual_seed(42)\n",
        "# print(torch.randn(2,3))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:32:59.360814Z",
          "iopub.execute_input": "2024-09-21T10:32:59.361504Z",
          "iopub.status.idle": "2024-09-21T10:32:59.365168Z",
          "shell.execute_reply.started": "2024-09-21T10:32:59.361470Z",
          "shell.execute_reply": "2024-09-21T10:32:59.364227Z"
        },
        "trusted": true,
        "id": "x80Flk3LUhlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "#vb_wrapper = VB_wrapper(vb)\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:32:59.854184Z",
          "iopub.execute_input": "2024-09-21T10:32:59.854564Z",
          "iopub.status.idle": "2024-09-21T10:32:59.858834Z",
          "shell.execute_reply.started": "2024-09-21T10:32:59.854534Z",
          "shell.execute_reply": "2024-09-21T10:32:59.857798Z"
        },
        "trusted": true,
        "id": "vfeeu4ihUhlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "!pip install shutup"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:33:02.312768Z",
          "iopub.execute_input": "2024-09-21T10:33:02.313718Z",
          "iopub.status.idle": "2024-09-21T10:33:14.640370Z",
          "shell.execute_reply.started": "2024-09-21T10:33:02.313681Z",
          "shell.execute_reply": "2024-09-21T10:33:14.639306Z"
        },
        "trusted": true,
        "id": "TnQYfV9LUhlc",
        "outputId": "bbff7942-6d30-44d9-de68-52fcb89740f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting shutup\n  Downloading shutup-0.2.0-py3-none-any.whl.metadata (530 bytes)\nDownloading shutup-0.2.0-py3-none-any.whl (1.5 kB)\nInstalling collected packages: shutup\nSuccessfully installed shutup-0.2.0\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(gc)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:33:17.389306Z",
          "iopub.execute_input": "2024-09-21T10:33:17.390311Z",
          "iopub.status.idle": "2024-09-21T10:33:17.396832Z",
          "shell.execute_reply.started": "2024-09-21T10:33:17.390270Z",
          "shell.execute_reply": "2024-09-21T10:33:17.395819Z"
        },
        "trusted": true,
        "id": "W_xwEXPsUhld",
        "outputId": "638f477d-22ed-469f-927f-f1dea9e65e87"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 71,
          "output_type": "execute_result",
          "data": {
            "text/plain": "208"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def call_sim():\n",
        "\n",
        "    import math\n",
        "    # Quantitative case study (1) Faithfulness on concepts\n",
        "    # Quantitative case study (2) Are they explainable?\n",
        "    test_dataloader=DataLoader(t_p, shuffle=True, batch_size=1)\n",
        "\n",
        "    pred_r1, pred_r2 = [],[]\n",
        "    pred_both = []\n",
        "    act = []\n",
        "\n",
        "    multimodal_repr = []\n",
        "    concept_repr = []\n",
        "\n",
        "    concept_repr1 = []\n",
        "    concept_repr2 = []\n",
        "    y_p = []\n",
        "\n",
        "    cr, rr = [],[]\n",
        "    cr1,rr1 = [],[]\n",
        "\n",
        "\n",
        "    ours_icace = []\n",
        "    ia_icace = []\n",
        "\n",
        "    concept_cat = {}\n",
        "    for i,c in enumerate(concept_classes):\n",
        "        concept_cat[c] = all_concepts[i]\n",
        "\n",
        "    for ii, batch in tqdm(enumerate(eval_dataloader)):\n",
        "\n",
        "        #if batch[1].detach().cpu().numpy()[0] == 1:\n",
        "\n",
        "        ll = batch[1].detach().cpu().numpy()[0]\n",
        "        y_p.append(ll)\n",
        "\n",
        "        #print('*'*10)\n",
        "\n",
        "        input_ids = batch[0]['input_ids'].to('cuda')\n",
        "        #print(tokenizer.batch_decode(batch[0]['input_ids'], skip_special_tokens=True))\n",
        "        token_type_ids = batch[0]['token_type_ids'].to('cuda')\n",
        "        attention_mask = batch[0]['attention_mask'].to('cuda')\n",
        "        visual_embeds = batch[0]['visual_embeds'].to('cuda')\n",
        "        visual_token_type_ids = batch[0]['visual_token_type_ids'].to('cuda')\n",
        "        visual_attention_mask = batch[0]['visual_attention_mask'].to('cuda')\n",
        "\n",
        "        inputs_embeds = vb_wrapper.vb.vb.embeddings.word_embeddings(input_ids)\n",
        "\n",
        "        #     (input1_attr, input2_attr), delta = ig.attribute(inputs = (all_concepts.to('cuda'), inputs_embeds), additional_forward_args = (attention_mask,\n",
        "        #         token_type_ids,\n",
        "        #         None,\n",
        "        #         None,\n",
        "        #         None,\n",
        "        #         visual_embeds,\n",
        "        #         visual_attention_mask,\n",
        "        #         visual_token_type_ids), return_convergence_delta=True)\n",
        "\n",
        "        #print(visual_embeds)\n",
        "\n",
        "        #     (input1_attr, input2_attr, input3_attr) = ig.attribute(inputs = (all_concepts.unsqueeze(0).to('cuda'), inputs_embeds, visual_embeds), additional_forward_args = (attention_mask,\n",
        "        #         token_type_ids,\n",
        "        #         None,\n",
        "        #         None,\n",
        "        #         None,\n",
        "        #         visual_attention_mask,\n",
        "        #         visual_token_type_ids))\n",
        "        #vb(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask, visual_embeds=visual_embeds_latent, visual_attention_mask=visual_attention_mask_latent, visual_token_type_ids=visual_token_type_ids_latent)\n",
        "\n",
        "        if type(ig) in [captum.attr._core.kernel_shap.KernelShap, captum.attr._core.input_x_gradient.InputXGradient, captum.attr._core.deep_lift.DeepLift, captum.attr._core.saliency.Saliency, captum.attr._core.integrated_gradients.IntegratedGradients, captum.attr._core.feature_ablation.FeatureAblation]:\n",
        "                baseline = None\n",
        "                #print('none')\n",
        "        else:\n",
        "            baseline = baselines\n",
        "            #print('baseline')\n",
        "\n",
        "        all_conc = all_concepts.to('cuda')\n",
        "        if type(ig) in [captum.attr._core.input_x_gradient.InputXGradient,captum.attr._core.saliency.Saliency]:\n",
        "            #print('ip_grad')\n",
        "            (input1_attr, input2_attr, input3_attr) = ig.attribute(inputs = (all_conc.unsqueeze(0), inputs_embeds, visual_embeds),\n",
        "\n",
        "                                                                additional_forward_args = (attention_mask,\n",
        "            token_type_ids,\n",
        "            None,\n",
        "            None,\n",
        "            None,\n",
        "            visual_attention_mask,\n",
        "            visual_token_type_ids))\n",
        "\n",
        "        else:\n",
        "            #print('others')\n",
        "\n",
        "            (input1_attr, input2_attr, input3_attr) = ig.attribute(inputs = (all_conc.unsqueeze(0), inputs_embeds, visual_embeds),\n",
        "                                                                  baselines = baseline,\n",
        "                                                                    additional_forward_args = (attention_mask,\n",
        "                token_type_ids,\n",
        "                None,\n",
        "                None,\n",
        "                None,\n",
        "                visual_attention_mask,\n",
        "                visual_token_type_ids))\n",
        "\n",
        "\n",
        "        #print(input1_attr, input2_attr)\n",
        "        #print(input1_attr.shape, input2_attr.shape)\n",
        "        k1=input1_attr.sum(dim=-1).squeeze(0).detach().cpu().numpy()\n",
        "\n",
        "        conc_scr = []\n",
        "        attribution_score = {}\n",
        "        for i in range(18):\n",
        "            cc = concept_classes[i]\n",
        "            #print('relative attribution scores for concept {} is {}'.format(cc,k[i]))\n",
        "            conc_scr.append((cc,k1[i]))\n",
        "            attribution_score[cc] = k1[i]\n",
        "            #conc_scr[cc] = k1[i]\n",
        "        conc_scr.sort(key=lambda tup: tup[1], reverse=True)\n",
        "\n",
        "        d = {}\n",
        "\n",
        "        for cnt,i in enumerate(conc_scr):\n",
        "            d[i[0]] = cnt\n",
        "\n",
        "        #print(d)\n",
        "\n",
        "\n",
        "        orig,pooled_op = model_pred(input_ids,attention_mask,token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids,all_concepts)\n",
        "        orig = orig[0]\n",
        "        pooled_op = pooled_op.detach().cpu()\n",
        "        multimodal_repr.append(pooled_op)\n",
        "        #print('predicted class {}'.format(class_map[np.argmax(orig.detach().cpu().numpy())]))\n",
        "        #print('actual class {}'.format(ll))\n",
        "        #print(0/0)\n",
        "        #orig = orig[0]\n",
        "        k = []\n",
        "        pred_class = np.argmax(orig.detach().cpu().numpy())\n",
        "        causal_score = {}\n",
        "        #print(pred_class)\n",
        "        for i in range(18):\n",
        "            #print('Removing concept: {}'.format(concept_classes[i]))\n",
        "            cc = concept_classes[i]\n",
        "            after_intervention,_ = model_pred(input_ids,attention_mask,token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids,all_concepts,concept_no=i)\n",
        "            after_intervention = after_intervention[0][pred_class]\n",
        "            #print('hello')\n",
        "            #print(orig[pred_class], after_intervention)\n",
        "            #print(0/0)\n",
        "            k.append((cc,abs(after_intervention-orig[pred_class]).item()))\n",
        "            causal_score[cc] = abs(after_intervention-orig[pred_class]).item()\n",
        "            #print('model prediction after removal {}'.format(model_pred(input_ids,attention_mask,token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids,concept_no=i)))\n",
        "        #k = np.asarray(k)\n",
        "        #k = np.asarray(k)\n",
        "        #k = list(map(lambda x: ((x-np.mean(k))/np.mean(k))*100, k))\n",
        "        k.sort(key=lambda tup: tup[1], reverse=True)\n",
        "        #print(k)\n",
        "\n",
        "        #print(list(map(lambda x: x[0], list(k))))\n",
        "\n",
        "        gamma = 0.9\n",
        "        temp = []\n",
        "        for cnt,i in enumerate(list(map(lambda x: x[0], list(k)))):\n",
        "            temp.append((gamma**cnt) * concept_cat[i])\n",
        "            #temp.append((math.e**(-cnt)) * concept_cat[i])\n",
        "            #temp.append((1.1**(-cnt)) * concept_cat[i])\n",
        "\n",
        "        temp = torch.stack(temp).mean(dim=0).unsqueeze(dim=0)\n",
        "        #print(temp)\n",
        "        #print(temp.shape)\n",
        "\n",
        "        concept_repr.append(temp)\n",
        "\n",
        "\n",
        "\n",
        "        temp = []\n",
        "        for cnt,i in enumerate(list(map(lambda x: x[0], list(conc_scr)))):\n",
        "            temp.append((gamma**cnt) * concept_cat[i])\n",
        "            #temp.append((math.e**(-cnt)) * concept_cat[i])\n",
        "            #temp.append((1.1**(-cnt)) * concept_cat[i])\n",
        "\n",
        "        temp = torch.stack(temp).mean(dim=0).unsqueeze(dim=0)\n",
        "        #print(temp)\n",
        "        #print(temp.shape)\n",
        "\n",
        "        concept_repr1.append(temp)\n",
        "\n",
        "\n",
        "        #print(list(map(lambda x: x[0], list(conc_scr))))\n",
        "        #print(0/0)\n",
        "\n",
        "        #considering the k to be the causally sorted rank, we estimate ranks of\n",
        "\n",
        "        ip_attrib_causal = []\n",
        "\n",
        "\n",
        "\n",
        "        for i in range(18):\n",
        "            cc = concept_classes[i]\n",
        "            ip_attrib_causal.append((cc,((2*attribution_score[cc]*causal_score[cc]) / (attribution_score[cc]+causal_score[cc]))))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ip_attrib_causal.sort(key=lambda tup: tup[1], reverse=True)\n",
        "\n",
        "\n",
        "\n",
        "        ra_icace_ours = []\n",
        "        for cnt,i in enumerate(list(map(lambda x: x[0], list(k)))):\n",
        "            ra_icace_ours.append((gamma**cnt) * causal_score[i])\n",
        "            #ra_icace_ours.append((math.e**(-cnt)) * causal_score[i])\n",
        "            #ra_icace_ours.append((1.1**(-cnt)) * causal_score[i])\n",
        "\n",
        "        ra_icace_ia = []\n",
        "        for cnt,i in enumerate(list(map(lambda x: x[0], list(conc_scr)))):\n",
        "            ra_icace_ia.append((gamma**cnt) * causal_score[i])\n",
        "            #ra_icace_ia.append((math.e**(-cnt)) * causal_score[i])\n",
        "            #ra_icace_ia.append((1.1**(-cnt)) * causal_score[i])\n",
        "\n",
        "\n",
        "\n",
        "        ours_icace.append(np.mean(ra_icace_ours))\n",
        "        ia_icace.append(np.mean(ra_icace_ia))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        temp = []\n",
        "        for cnt,i in enumerate(list(map(lambda x: x[0], list(ip_attrib_causal)))):\n",
        "            temp.append((gamma**cnt) * concept_cat[i])\n",
        "            #temp.append((math.e**(-cnt)) * concept_cat[i])\n",
        "            #temp.append((1.1**(-cnt)) * concept_cat[i])\n",
        "\n",
        "        temp = torch.stack(temp).mean(dim=0).unsqueeze(dim=0)\n",
        "\n",
        "        concept_repr2.append(temp)\n",
        "\n",
        "\n",
        "\n",
        "        d1 = {}\n",
        "\n",
        "        for cnt,i in enumerate(ip_attrib_causal):\n",
        "            d1[i[0]] = cnt\n",
        "\n",
        "        r1,r2,r3 = [],[],[]\n",
        "        for cnt,i in enumerate(list(map(lambda x: x[0], list(k)))):\n",
        "            r1.append(cnt)\n",
        "            r2.append(d[i])\n",
        "            r3.append(d1[i])\n",
        "\n",
        "        #print(r1,r2)\n",
        "        corr, _ = stats.kendalltau(r1, r2)\n",
        "        res,_ = stats.spearmanr(r1, r2)\n",
        "\n",
        "        corr1, _ = stats.kendalltau(r1, r3)\n",
        "        res1,_ = stats.spearmanr(r1, r3)\n",
        "\n",
        "        #print('kendall tau', corr)\n",
        "        cr.append(corr)\n",
        "        #print('spearmans rho', res)\n",
        "        rr.append(res)\n",
        "\n",
        "        cr1.append(corr1)\n",
        "        rr1.append(res1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    multimodal_repr = torch.stack(multimodal_repr).squeeze(1)\n",
        "\n",
        "    #causal\n",
        "    concept_repr = torch.stack(concept_repr).squeeze(1)\n",
        "\n",
        "\n",
        "    #attribution\n",
        "    concept_repr1 = torch.stack(concept_repr1).squeeze(1)\n",
        "\n",
        "    #HM\n",
        "    concept_repr2 = torch.stack(concept_repr2).squeeze(1)\n",
        "\n",
        "\n",
        "    return np.mean(cr), np.mean(rr), multimodal_repr, concept_repr, concept_repr1, y_p\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:33:19.501908Z",
          "iopub.execute_input": "2024-09-21T10:33:19.502279Z",
          "iopub.status.idle": "2024-09-21T10:33:19.543417Z",
          "shell.execute_reply.started": "2024-09-21T10:33:19.502251Z",
          "shell.execute_reply": "2024-09-21T10:33:19.542627Z"
        },
        "trusted": true,
        "id": "vijW4BS8Uhld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /kaggle/working"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:33:20.985235Z",
          "iopub.execute_input": "2024-09-21T10:33:20.985632Z",
          "iopub.status.idle": "2024-09-21T10:33:22.073217Z",
          "shell.execute_reply.started": "2024-09-21T10:33:20.985600Z",
          "shell.execute_reply": "2024-09-21T10:33:22.072132Z"
        },
        "trusted": true,
        "id": "TbU1GDbpUhle",
        "outputId": "1d952fef-efd0-408c-eaf4-511df36a024a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "25461.png\t   cr1.pt\t\t\t  mean_comparison_plot.png\n28076.png\t   cr2.pt\t\t\t  mr.pt\n94162.png\t   final_model_concept.pt\t  rr.npy\n96704.png\t   final_model_concept_inlp.pt\t  rr1.npy\ncausal_vb.pt\t   final_model_wo_adversarial.pt  transformers\ncausal_vb_inlp.pt  foo1.png\t\t\t  vs_tensors.pt\ncr.npy\t\t   full_annotation.pkl\t\t  y_p.pt\ncr.pt\t\t   image1.jpg\ncr1.npy\t\t   image2.jpg\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(DECONFOUND, DYN_ROUTE)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:33:23.101301Z",
          "iopub.execute_input": "2024-09-21T10:33:23.102120Z",
          "iopub.status.idle": "2024-09-21T10:33:23.107305Z",
          "shell.execute_reply.started": "2024-09-21T10:33:23.102074Z",
          "shell.execute_reply": "2024-09-21T10:33:23.106130Z"
        },
        "trusted": true,
        "id": "fc_2BSWAUhle",
        "outputId": "abf56864-433e-4e2b-e066-0dc17fb20cd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "True True\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#clf = svm.SVC(kernel='poly', C=2, random_state=42)\n",
        "\n",
        "def get_sim(multimodal_repr, concept_repr1, y_p):\n",
        "\n",
        "    X = np.concatenate([multimodal_repr.numpy(),concept_repr1.numpy()],axis=-1) # inp w/ clip representation w/ exp -> w/ both\n",
        "    #X = (multimodal_repr.numpy()*concept_repr.numpy())\n",
        "    X_ = multimodal_repr.numpy() # inp w/ clip representation w/o exp -> w/ only input\n",
        "    X__ = concept_repr1.numpy() # inp w/o clip representation w/ exp -> w/ only exp\n",
        "\n",
        "    print(X.shape)\n",
        "    print(X_.shape)\n",
        "    print(X__.shape)\n",
        "    print(y_p)\n",
        "    #print(0/0)\n",
        "\n",
        "\n",
        "\n",
        "    kf = KFold(n_splits=5)\n",
        "    y = np.array(y_p)\n",
        "    vals = []\n",
        "    vals_lus = []\n",
        "    k1,k2,k3 = [],[],[]\n",
        "    C,S = [],[]\n",
        "    for train, test in kf.split(X):\n",
        "\n",
        "        X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]\n",
        "        X_train_, X_test_, y_train, y_test = X_[train], X_[test], y[train], y[test]\n",
        "        X_train__, X_test__, y_train, y_test = X__[train], X__[test], y[train], y[test]\n",
        "\n",
        "\n",
        "        clf1 = make_pipeline(StandardScaler(),svm.SVC(probability=True, kernel='rbf', C=2, random_state=42))\n",
        "        clf1.fit(X_train, y_train)\n",
        "        y_pred = clf1.predict(X_test)\n",
        "\n",
        "        clf2 = make_pipeline(StandardScaler(),svm.SVC(probability=True,kernel='rbf', C=2, random_state=42))\n",
        "        clf2.fit(X_train_, y_train)\n",
        "        y_pred_ = clf2.predict(X_test_)\n",
        "\n",
        "        clf3 = make_pipeline(StandardScaler(), svm.SVC(probability=True,kernel='rbf', C=2, random_state=42))\n",
        "        clf3.fit(X_train__, y_train)\n",
        "        y_pred__ = clf3.predict(X_test__)\n",
        "\n",
        "\n",
        "        y_pred_proba_w_both = clf1.predict_proba(X_test)\n",
        "        y_pred_proba_w_inp = clf2.predict_proba(X_test_)\n",
        "        y_pred_proba_w_exp = clf3.predict_proba(X_test__)\n",
        "\n",
        "\n",
        "\n",
        "        comprehensiveness, sufficiency = 0,0\n",
        "\n",
        "        counter = 0\n",
        "        for pred_proba_w_both, pred_proba_w_inp, pred_proba_w_exp, pred_class in zip(y_pred_proba_w_both,y_pred_proba_w_inp,y_pred_proba_w_exp,y_pred):\n",
        "            comprehensiveness += (pred_proba_w_both[pred_class] - pred_proba_w_inp[pred_class])\n",
        "            sufficiency += (pred_proba_w_both[pred_class] - pred_proba_w_exp[pred_class])\n",
        "            counter+=1\n",
        "\n",
        "\n",
        "        comprehensiveness = comprehensiveness / counter\n",
        "        sufficiency = sufficiency / counter\n",
        "\n",
        "        C.append(comprehensiveness)\n",
        "        S.append(sufficiency)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        l, nl = 0,0\n",
        "        us = 0\n",
        "        for i ,j in zip(y_pred__,y_test):\n",
        "            if i==j:\n",
        "                l+=1\n",
        "            else:\n",
        "                nl+=1\n",
        "        a,b=0,0\n",
        "        for i,j,k,x in zip(y_pred,y_pred_,y_test,y_pred__):\n",
        "            us+=int(i==k) - int(j==k)\n",
        "            if k==x:\n",
        "                a += int(i==k) - int(j==k)\n",
        "            elif k!=x:\n",
        "                b += int(i==k) - int(j==k)\n",
        "\n",
        "        #print('LAS ',.5*(a/l)+.5*(b/nl))\n",
        "        #print('Leakage Unadjusted Simulatability (LUS)', (us/(l+nl)))\n",
        "        vals_lus.append(us/(l+nl))\n",
        "        #print('Comprehensiveness ', comprehensiveness)\n",
        "        #print('Sufficiency ', sufficiency)\n",
        "        #print('f1 between prediction of proposed model and SVM w/ both inp and exp {}'.format(f1_score(y_test, y_pred, average='macro')))\n",
        "        #print('f1 between prediction of proposed model and SVM w/ only inp {}'.format(f1_score(y_test, y_pred_, average='macro')))\n",
        "        #print('f1 between prediction of proposed model and SVM w/ only exp {}'.format(f1_score(y_test, y_pred__, average='macro')))\n",
        "        vals.append(.5*(a/l)+.5*(b/nl))\n",
        "        k1.append(f1_score(y_test, y_pred, average='macro'))\n",
        "        k2.append(f1_score(y_test, y_pred_, average='macro'))\n",
        "        k3.append(f1_score(y_test, y_pred__, average='macro'))\n",
        "\n",
        "\n",
        "    #print('Intra mean ', np.mean(ID), 'Intra std ', np.std(ID))\n",
        "    #print('Inter mean ', np.mean(ID1), 'Inter std ', np.std(ID1))\n",
        "    print('LAS mean ', np.mean(vals), 'LAS std ', np.std(vals))\n",
        "    print('LUS mean ', np.mean(vals_lus), 'LUS std ', np.std(vals_lus))\n",
        "    print('Comprehensiveness mean ', np.mean(C), 'Comprehensiveness std ', np.std(C))\n",
        "    print('Sufficiency mean ', np.mean(S), 'Sufficiency std ', np.std(S))\n",
        "    print('Avg. f1 between prediction of proposed model and SVM w/ both inp and exp {}'.format(np.mean(k1)))\n",
        "    print('Avg. f1 between prediction of proposed model and SVM w/ only inp {}'.format(np.mean(k2)))\n",
        "    print('Avg. f1 between prediction of proposed model and SVM w/ only exp {}'.format(np.mean(k3)))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:33:26.269903Z",
          "iopub.execute_input": "2024-09-21T10:33:26.270269Z",
          "iopub.status.idle": "2024-09-21T10:33:26.291761Z",
          "shell.execute_reply.started": "2024-09-21T10:33:26.270242Z",
          "shell.execute_reply": "2024-09-21T10:33:26.290746Z"
        },
        "trusted": true,
        "id": "th7MNpbuUhle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d83Afg8yUhlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "cd = {i:[] for i in concept_classes}\n",
        "import shutup; shutup.please()\n",
        "def call():\n",
        "    import math\n",
        "    # Quantitative case study (1) Faithfulness on concepts\n",
        "    # Quantitative case study (2) Are they explainable?\n",
        "    test_dataloader=DataLoader(t_p, shuffle=True, batch_size=1)\n",
        "\n",
        "    pred_r1, pred_r2 = [],[]\n",
        "    pred_both = []\n",
        "    act = []\n",
        "\n",
        "    multimodal_repr = []\n",
        "    concept_repr = []\n",
        "\n",
        "    concept_repr1 = []\n",
        "    concept_repr2 = []\n",
        "    y_p = []\n",
        "\n",
        "    cr, rr = [],[]\n",
        "    cr1,rr1 = [],[]\n",
        "\n",
        "\n",
        "    ours_icace = []\n",
        "    ia_icace = []\n",
        "\n",
        "    counter = 0\n",
        "\n",
        "\n",
        "    c_p=0\n",
        "    c_r=0\n",
        "\n",
        "    a_p=0\n",
        "    a_r=0\n",
        "\n",
        "    average_precision_causal = 0\n",
        "    average_precision_attrib = 0\n",
        "\n",
        "    concept_cat = {}\n",
        "    for i,c in enumerate(concept_classes):\n",
        "        concept_cat[c] = all_concepts[i]\n",
        "\n",
        "    for ii, batch in tqdm(enumerate(eval_dataloader)):\n",
        "\n",
        "        if ii==0:\n",
        "            print(ig)\n",
        "\n",
        "        #if batch[1].detach().cpu().numpy()[0] == 1:\n",
        "\n",
        "        ll = batch[1].detach().cpu().numpy()[0]\n",
        "        if ll==1:\n",
        "\n",
        "            #ll = batch[1].detach().cpu().numpy()[0]\n",
        "            #print(ll, ii)\n",
        "            y_p.append(ll)\n",
        "\n",
        "            #print('*'*10)\n",
        "\n",
        "            input_ids = batch[0]['input_ids'].to('cuda')\n",
        "            #print(tokenizer.batch_decode(batch[0]['input_ids'], skip_special_tokens=True))\n",
        "            token_type_ids = batch[0]['token_type_ids'].to('cuda')\n",
        "            attention_mask = batch[0]['attention_mask'].to('cuda')\n",
        "            visual_embeds = batch[0]['visual_embeds'].to('cuda')\n",
        "            visual_token_type_ids = batch[0]['visual_token_type_ids'].to('cuda')\n",
        "            visual_attention_mask = batch[0]['visual_attention_mask'].to('cuda')\n",
        "\n",
        "            inputs_embeds = vb_wrapper.vb.vb.embeddings.word_embeddings(input_ids)\n",
        "\n",
        "            # print(inputs_embeds)\n",
        "            #     (input1_attr, input2_attr), delta = ig.attribute(inputs = (all_concepts.to('cuda'), inputs_embeds), additional_forward_args = (attention_mask,\n",
        "            #         token_type_ids,\n",
        "            #         None,\n",
        "            #         None,\n",
        "            #         None,\n",
        "            #         visual_embeds,\n",
        "            #         visual_attention_mask,\n",
        "            #         visual_token_type_ids), return_convergence_delta=True)\n",
        "\n",
        "            #print(visual_embeds)\n",
        "\n",
        "            #     (input1_attr, input2_attr, input3_attr) = ig.attribute(inputs = (all_concepts.unsqueeze(0).to('cuda'), inputs_embeds, visual_embeds), additional_forward_args = (attention_mask,\n",
        "            #         token_type_ids,\n",
        "            #         None,\n",
        "            #         None,\n",
        "            #         None,\n",
        "            #         visual_attention_mask,\n",
        "            #         visual_token_type_ids))\n",
        "            #vb(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask, visual_embeds=visual_embeds_latent, visual_attention_mask=visual_attention_mask_latent, visual_token_type_ids=visual_token_type_ids_latent)\n",
        "\n",
        "            if type(ig) in [captum.attr._core.kernel_shap.KernelShap, captum.attr._core.input_x_gradient.InputXGradient, captum.attr._core.deep_lift.DeepLift, captum.attr._core.saliency.Saliency, captum.attr._core.integrated_gradients.IntegratedGradients, captum.attr._core.feature_ablation.FeatureAblation]:\n",
        "                    baseline = None\n",
        "                    #print('none')\n",
        "            else:\n",
        "                baseline = baselines\n",
        "                #print('baseline')\n",
        "\n",
        "            if DECONFOUND:\n",
        "                all_conc = get_inlp(all_concepts).to('cuda')\n",
        "            else:\n",
        "                all_conc = all_concepts.to('cuda')\n",
        "\n",
        "            #print(all_conc)\n",
        "            #print(all_concepts)\n",
        "\n",
        "            #print(0/0)\n",
        "\n",
        "            #all_conc = all_concepts.to('cuda')\n",
        "\n",
        "            if type(ig) in [captum.attr._core.input_x_gradient.InputXGradient,captum.attr._core.saliency.Saliency]:\n",
        "                #print('ip_grad')\n",
        "                (input1_attr, input2_attr, input3_attr) = ig.attribute(inputs = (all_conc.unsqueeze(0).to('cuda'), inputs_embeds, visual_embeds),\n",
        "\n",
        "                                                                    additional_forward_args = (attention_mask,\n",
        "                token_type_ids,\n",
        "                None,\n",
        "                None,\n",
        "                None,\n",
        "                visual_attention_mask,\n",
        "                visual_token_type_ids))\n",
        "\n",
        "            else:\n",
        "                #print('others')\n",
        "\n",
        "                (input1_attr, input2_attr, input3_attr) = ig.attribute(inputs = (all_conc.unsqueeze(0).to('cuda'), inputs_embeds, visual_embeds),\n",
        "                                                                      baselines = baseline,\n",
        "                                                                        additional_forward_args = (attention_mask,\n",
        "                    token_type_ids,\n",
        "                    None,\n",
        "                    None,\n",
        "                    None,\n",
        "                    visual_attention_mask,\n",
        "                    visual_token_type_ids))\n",
        "\n",
        "\n",
        "            #print(input1_attr, input2_attr)\n",
        "            #print(input1_attr.shape, input2_attr.shape)\n",
        "            k1=input1_attr.sum(dim=-1).squeeze(0).detach().cpu().numpy()\n",
        "\n",
        "            conc_scr = []\n",
        "            attribution_score = {}\n",
        "            for i in range(18):\n",
        "                cc = concept_classes[i]\n",
        "                #print('relative attribution scores for concept {} is {}'.format(cc,k[i]))\n",
        "                conc_scr.append((cc,k1[i]))\n",
        "                attribution_score[cc] = k1[i]\n",
        "                #conc_scr[cc] = k1[i]\n",
        "            conc_scr.sort(key=lambda tup: tup[1], reverse=True)\n",
        "\n",
        "            d = {}\n",
        "\n",
        "            for cnt,i in enumerate(conc_scr):\n",
        "                d[i[0]] = cnt\n",
        "\n",
        "            #print(d)\n",
        "\n",
        "\n",
        "            orig,pooled_op = model_pred(input_ids,attention_mask,token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids,all_concepts)\n",
        "            orig = orig[0]\n",
        "            pooled_op = pooled_op.detach().cpu()\n",
        "            multimodal_repr.append(pooled_op)\n",
        "            #print('predicted class {}'.format(class_map[np.argmax(orig.detach().cpu().numpy())]))\n",
        "            #print('actual class {}'.format(ll))\n",
        "            #print(0/0)\n",
        "            #orig = orig[0]\n",
        "            k = []\n",
        "            pred_class = np.argmax(orig.detach().cpu().numpy())\n",
        "            causal_score = {}\n",
        "            #print(pred_class)\n",
        "            for i in range(18):\n",
        "                #print('Removing concept: {}'.format(concept_classes[i]))\n",
        "                cc = concept_classes[i]\n",
        "                after_intervention,_ = model_pred(input_ids,attention_mask,token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids,all_concepts,concept_no=i)\n",
        "                after_intervention = after_intervention[0][pred_class]\n",
        "                #print('hello')\n",
        "                #print(orig[pred_class], after_intervention)\n",
        "                #print(0/0)\n",
        "                k.append((cc,abs(after_intervention-orig[pred_class]).item()))\n",
        "                causal_score[cc] = abs(after_intervention-orig[pred_class]).item()\n",
        "                #print('model prediction after removal {}'.format(model_pred(input_ids,attention_mask,token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids,concept_no=i)))\n",
        "            #k = np.asarray(k)\n",
        "            #k = np.asarray(k)\n",
        "            #k = list(map(lambda x: ((x-np.mean(k))/np.mean(k))*100, k))\n",
        "            k.sort(key=lambda tup: tup[1], reverse=True)\n",
        "\n",
        "            #print(k)\n",
        "\n",
        "            #random.seed(counter)\n",
        "            #random.shuffle(k)\n",
        "\n",
        "            #print(k)\n",
        "\n",
        "            #print(0/0)\n",
        "\n",
        "\n",
        "            for i in k:\n",
        "                cd[i[0]].append(i[1])\n",
        "\n",
        "\n",
        "\n",
        "            #print(list(map(lambda x: x[0], list(k))))\n",
        "\n",
        "            gamma = 0.9\n",
        "            temp = []\n",
        "            for cnt,i in enumerate(list(map(lambda x: x[0], list(k)))):\n",
        "                temp.append((gamma**cnt) * concept_cat[i])\n",
        "                #temp.append((math.e**(-cnt)) * concept_cat[i])\n",
        "                #temp.append((1.1**(-cnt)) * concept_cat[i])\n",
        "\n",
        "            temp = torch.stack(temp).mean(dim=0).unsqueeze(dim=0)\n",
        "            #print(temp)\n",
        "            #print(temp.shape)\n",
        "\n",
        "            concept_repr.append(temp)\n",
        "\n",
        "\n",
        "\n",
        "            temp = []\n",
        "            for cnt,i in enumerate(list(map(lambda x: x[0], list(conc_scr)))):\n",
        "                temp.append((gamma**cnt) * concept_cat[i])\n",
        "                #temp.append((math.e**(-cnt)) * concept_cat[i])\n",
        "                #temp.append((1.1**(-cnt)) * concept_cat[i])\n",
        "\n",
        "            temp = torch.stack(temp).mean(dim=0).unsqueeze(dim=0)\n",
        "            #print(temp)\n",
        "            #print(temp.shape)\n",
        "\n",
        "            concept_repr1.append(temp)\n",
        "\n",
        "\n",
        "            #print(list(map(lambda x: x[0], list(conc_scr))))\n",
        "            #print(0/0)\n",
        "\n",
        "            #considering the k to be the causally sorted rank, we estimate ranks of\n",
        "\n",
        "            ip_attrib_causal = []\n",
        "\n",
        "\n",
        "\n",
        "            for i in range(18):\n",
        "                cc = concept_classes[i]\n",
        "                ip_attrib_causal.append((cc,((2*attribution_score[cc]*causal_score[cc]) / (attribution_score[cc]+causal_score[cc]))))\n",
        "\n",
        "\n",
        "\n",
        "            ip_attrib = []\n",
        "            for i in range(18):\n",
        "                cc = concept_classes[i]\n",
        "                ip_attrib.append((cc,attribution_score[cc]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            ip_attrib_causal.sort(key=lambda tup: tup[1], reverse=True)\n",
        "            ip_attrib.sort(key=lambda tup: tup[1], reverse=True)\n",
        "\n",
        "\n",
        "\n",
        "            ra_icace_ours = []\n",
        "            for cnt,i in enumerate(list(map(lambda x: x[0], list(k)))):\n",
        "                ra_icace_ours.append((gamma**cnt) * causal_score[i])\n",
        "                #ra_icace_ours.append((math.e**(-cnt)) * causal_score[i])\n",
        "                #ra_icace_ours.append((1.1**(-cnt)) * causal_score[i])\n",
        "\n",
        "            ra_icace_ia = []\n",
        "            for cnt,i in enumerate(list(map(lambda x: x[0], list(conc_scr)))):\n",
        "                ra_icace_ia.append((gamma**cnt) * causal_score[i])\n",
        "                #ra_icace_ia.append((math.e**(-cnt)) * causal_score[i])\n",
        "                #ra_icace_ia.append((1.1**(-cnt)) * causal_score[i])\n",
        "\n",
        "\n",
        "\n",
        "            ours_icace.append(np.mean(ra_icace_ours))\n",
        "            ia_icace.append(np.mean(ra_icace_ia))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            temp = []\n",
        "            for cnt,i in enumerate(list(map(lambda x: x[0], list(ip_attrib_causal)))):\n",
        "                temp.append((gamma**cnt) * concept_cat[i])\n",
        "                #temp.append((math.e**(-cnt)) * concept_cat[i])\n",
        "                #temp.append((1.1**(-cnt)) * concept_cat[i])\n",
        "\n",
        "            temp = torch.stack(temp).mean(dim=0).unsqueeze(dim=0)\n",
        "\n",
        "            concept_repr2.append(temp)\n",
        "\n",
        "\n",
        "\n",
        "            d1 = {}\n",
        "\n",
        "            for cnt,i in enumerate(ip_attrib_causal):\n",
        "                d1[i[0]] = cnt\n",
        "\n",
        "            r1,r2,r3 = [],[],[]\n",
        "            for cnt,i in enumerate(list(map(lambda x: x[0], list(k)))):\n",
        "                r1.append(cnt)\n",
        "                r2.append(d[i])\n",
        "                r3.append(d1[i])\n",
        "\n",
        "            #print(r1,r2)\n",
        "            corr, _ = stats.kendalltau(r1, r2)\n",
        "            res,_ = stats.spearmanr(r1, r2)\n",
        "\n",
        "            corr1, _ = stats.kendalltau(r1, r3)\n",
        "            res1,_ = stats.spearmanr(r1, r3)\n",
        "\n",
        "            #print('kendall tau', corr)\n",
        "            cr.append(corr)\n",
        "            #print('spearmans rho', res)\n",
        "            rr.append(res)\n",
        "\n",
        "            cr1.append(corr1)\n",
        "            rr1.append(res1)\n",
        "\n",
        "\n",
        "            rel_items = gc[counter]\n",
        "            causal = [i[0] for i in k]\n",
        "            #causal = random.sample(concept_classes,5)\n",
        "            attrib = [i[0] for i in ip_attrib]\n",
        "\n",
        "\n",
        "            #         mrr_causal = []\n",
        "            #         for i in range(len(causal)):\n",
        "            #             item = causal[i]\n",
        "            #             if item in rel_items:\n",
        "            #                 mrr_causal.append(1/(i+1))\n",
        "\n",
        "            #         mrr_attrib = []\n",
        "            #         for i in range(len(attrib)):\n",
        "            #             item = attrib[i]\n",
        "            #             if item in rel_items:\n",
        "            #                 mrr_attrib.append(1/(i+1))\n",
        "\n",
        "\n",
        "\n",
        "            #         mrr_causal = np.mean(np.asarray(mrr_causal))\n",
        "\n",
        "            #print('gc', rel_items)\n",
        "            #print('causal', causal)\n",
        "            #print('attrib', attrib)\n",
        "\n",
        "            K = 5\n",
        "\n",
        "            rel_rec_items_at_K = set(causal[:K]) & set(rel_items)\n",
        "            rec_items_at_K = causal[:K]\n",
        "            tot_rel_items = len(rel_items)\n",
        "\n",
        "            #print('Causal P@{} '.format(K), len(rel_rec_items_at_K) / len(rec_items_at_K) )\n",
        "            #print('Causal R@{} '.format(K), len(rel_rec_items_at_K) / tot_rel_items )\n",
        "\n",
        "            c_p+=len(rel_rec_items_at_K) / len(rec_items_at_K)\n",
        "            c_r+=len(rel_rec_items_at_K) / tot_rel_items\n",
        "\n",
        "\n",
        "            rel_rec_items_at_K = set(attrib[:K]) & set(rel_items)\n",
        "            rec_items_at_K = attrib[:K]\n",
        "            tot_rel_items = len(rel_items)\n",
        "\n",
        "            #print('Attrib P@{} '.format(K), len(rel_rec_items_at_K) / len(rec_items_at_K) )\n",
        "            #print('Attrib R@{} '.format(K), len(rel_rec_items_at_K) / tot_rel_items )\n",
        "\n",
        "            a_p+=len(rel_rec_items_at_K) / len(rec_items_at_K)\n",
        "            a_r+=len(rel_rec_items_at_K) / tot_rel_items\n",
        "\n",
        "\n",
        "            tot_precision_causal = []\n",
        "            for k in range(1,K+1):\n",
        "                tot_precision_causal.append(len(set(causal[:k]) & set(rel_items)) / len(causal[:k]))\n",
        "\n",
        "            tot_precision_attrib = []\n",
        "            for k in range(1,K+1):\n",
        "                tot_precision_attrib.append(len(set(attrib[:k]) & set(rel_items)) / len(attrib[:k]))\n",
        "\n",
        "\n",
        "\n",
        "            average_precision_causal += np.mean(np.asarray(tot_precision_causal))\n",
        "            average_precision_attrib += np.mean(np.asarray(tot_precision_attrib))\n",
        "\n",
        "            counter+=1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    multimodal_repr = torch.stack(multimodal_repr).squeeze(1)\n",
        "\n",
        "    #causal\n",
        "    concept_repr = torch.stack(concept_repr).squeeze(1)\n",
        "\n",
        "\n",
        "    #attribution\n",
        "    concept_repr1 = torch.stack(concept_repr1).squeeze(1)\n",
        "\n",
        "    #HM\n",
        "    concept_repr2 = torch.stack(concept_repr2).squeeze(1)\n",
        "\n",
        "    print('mean precision causal@5',c_p/counter)\n",
        "    print('mean recall causal@5', c_r/counter)\n",
        "\n",
        "    print('mean precision attrb@5',a_p/counter)\n",
        "    print('mean recall attrb@5', a_r/counter)\n",
        "\n",
        "    print('MAP@5 Causal ', average_precision_causal/counter)\n",
        "    print('MAP@5 Attrib ', average_precision_attrib/counter)\n",
        "\n",
        "    #     print('kendall tau', np.mean(cr))\n",
        "\n",
        "    #     print('spearmans rho', np.mean(rr))\n",
        "\n",
        "\n",
        "    #     print('-----------CAUSAL----------')\n",
        "    #     get_sim(multimodal_repr, concept_repr)\n",
        "\n",
        "    #     print('-----------Attribution----------')\n",
        "    #     get_sim(multimodal_repr, concept_repr1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:33:28.871788Z",
          "iopub.execute_input": "2024-09-21T10:33:28.872151Z",
          "iopub.status.idle": "2024-09-21T10:33:28.945889Z",
          "shell.execute_reply.started": "2024-09-21T10:33:28.872123Z",
          "shell.execute_reply": "2024-09-21T10:33:28.945103Z"
        },
        "trusted": true,
        "id": "5uz95pkMUhlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baire = ['ig', 'saliency', 'deeplift', 'deepliftshap', 'gradshap', 'ipxgrad']\n",
        "#R\n",
        "#baire = ['ig', 'saliency']\n",
        "import captum\n",
        "from scipy import stats\n",
        "for fu in range(len(baire)):\n",
        "\n",
        "\n",
        "    vb_wrapper = VB_wrapper(vb)\n",
        "    vb_wrapper.eval()\n",
        "    vb_wrapper.zero_grad()\n",
        "\n",
        "    if baire[fu]=='ig':\n",
        "        ig = IntegratedGradients(vb_wrapper)\n",
        "    elif baire[fu]=='saliency':\n",
        "        ig = Saliency(vb_wrapper)\n",
        "    elif baire[fu]=='deeplift':\n",
        "        ig = DeepLift(vb_wrapper)\n",
        "    elif baire[fu]=='deepliftshap':\n",
        "        ig = DeepLiftShap(vb_wrapper)\n",
        "    elif baire[fu]=='gradshap':\n",
        "        ig = GradientShap(vb_wrapper)\n",
        "    elif baire[fu]=='ipxgrad':\n",
        "        ig = InputXGradient(vb_wrapper)\n",
        "\n",
        "    call()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T10:33:45.212462Z",
          "iopub.execute_input": "2024-09-21T10:33:45.213332Z",
          "iopub.status.idle": "2024-09-21T11:23:37.076105Z",
          "shell.execute_reply.started": "2024-09-21T10:33:45.213300Z",
          "shell.execute_reply": "2024-09-21T11:23:37.074730Z"
        },
        "trusted": true,
        "id": "2r48EffoUhlf",
        "outputId": "0a5b9f70-c665-4c1e-c621-d86a7a089e6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "0it [00:00, ?it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "<captum.attr._core.integrated_gradients.IntegratedGradients object at 0x7d44c4105720>\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "660it [09:01,  1.22it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "mean precision causal@5 0.17788461538461525\nmean recall causal@5 0.25987293956043955\nmean precision attrb@5 0.19038461538461535\nmean recall attrb@5 0.26879578754578765\nMAP@5 Causal  0.16939102564102562\nMAP@5 Attrib  0.20506410256410254\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "0it [00:00, ?it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "<captum.attr._core.saliency.Saliency object at 0x7d44ba981cf0>\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "660it [07:55,  1.39it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "mean precision causal@5 0.17788461538461525\nmean recall causal@5 0.25987293956043955\nmean precision attrb@5 0.1653846153846152\nmean recall attrb@5 0.22588141025641026\nMAP@5 Causal  0.16939102564102562\nMAP@5 Attrib  0.1741826923076923\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "0it [00:00, ?it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "<captum.attr._core.deep_lift.DeepLift object at 0x7d44ba980ac0>\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "660it [08:00,  1.37it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "mean precision causal@5 0.17788461538461525\nmean recall causal@5 0.25987293956043955\nmean precision attrb@5 0.17980769230769222\nmean recall attrb@5 0.2536515567765569\nMAP@5 Causal  0.16939102564102562\nMAP@5 Attrib  0.1941346153846155\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "0it [00:00, ?it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "<captum.attr._core.deep_lift.DeepLiftShap object at 0x7d44ba91f190>\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "660it [08:40,  1.27it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "mean precision causal@5 0.17788461538461525\nmean recall causal@5 0.25987293956043955\nmean precision attrb@5 0.216346153846154\nmean recall attrb@5 0.3134214743589745\nMAP@5 Causal  0.16939102564102562\nMAP@5 Attrib  0.21995192307692302\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "0it [00:00, ?it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "<captum.attr._core.gradient_shap.GradientShap object at 0x7d44ba949810>\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "660it [08:08,  1.35it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "mean precision causal@5 0.17788461538461525\nmean recall causal@5 0.25987293956043955\nmean precision attrb@5 0.20192307692307693\nmean recall attrb@5 0.3067135989010991\nMAP@5 Causal  0.16939102564102562\nMAP@5 Attrib  0.22091346153846148\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "0it [00:00, ?it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "<captum.attr._core.input_x_gradient.InputXGradient object at 0x7d44baa32410>\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "660it [08:05,  1.36it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "mean precision causal@5 0.17788461538461525\nmean recall causal@5 0.25987293956043955\nmean precision attrb@5 0.1826923076923076\nmean recall attrb@5 0.2604624542124543\nMAP@5 Causal  0.16939102564102562\nMAP@5 Attrib  0.19823717948717956\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NZDDCIFuUhl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SLxeHrxzUhl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zjRFu7GzUhl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g7Z2T7FhUhl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fnsEMLgcUhmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mNCGIO0xUhmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8O7S96EWUhmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hJ5k_6PwUhmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i1Nx2JXjUhmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SFOcDyl3UhmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fAs8F3lqUhmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6apq1YzEUhme"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}