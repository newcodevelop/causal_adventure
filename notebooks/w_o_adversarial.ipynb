{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 1246182,
          "sourceType": "datasetVersion",
          "datasetId": 715500
        }
      ],
      "dockerImageVersionId": 30648,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!ls /kaggle/working/"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T13:58:36.396640Z",
          "iopub.execute_input": "2024-09-21T13:58:36.396997Z",
          "iopub.status.idle": "2024-09-21T13:58:37.408229Z",
          "shell.execute_reply.started": "2024-09-21T13:58:36.396959Z",
          "shell.execute_reply": "2024-09-21T13:58:37.407146Z"
        },
        "trusted": true,
        "id": "IDx0P60YV9O1",
        "outputId": "651a78c5-1a60-45ca-e6ce-cf7e01bc0f43"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "25461.png\t   cr1.pt\t\t\t  mean_comparison_plot.png\n28076.png\t   cr2.pt\t\t\t  mr.pt\n94162.png\t   final_model_concept.pt\t  rr.npy\n96704.png\t   final_model_concept_inlp.pt\t  rr1.npy\ncausal_vb.pt\t   final_model_wo_adversarial.pt  transformers\ncausal_vb_inlp.pt  foo1.png\t\t\t  vs_tensors.pt\ncr.npy\t\t   full_annotation.pkl\t\t  y_p.pt\ncr.pt\t\t   image1.jpg\ncr1.npy\t\t   image2.jpg\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN\n",
        "\n",
        "import ipywidgets as widgets\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T13:58:37.410159Z",
          "iopub.execute_input": "2024-09-21T13:58:37.410826Z",
          "iopub.status.idle": "2024-09-21T13:58:37.493687Z",
          "shell.execute_reply.started": "2024-09-21T13:58:37.410796Z",
          "shell.execute_reply": "2024-09-21T13:58:37.492879Z"
        },
        "trusted": true,
        "id": "hpUTmO6_V9O3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "9GaO2fY2V9O4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN\n",
        "ff = []"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T13:58:38.534924Z",
          "iopub.execute_input": "2024-09-21T13:58:38.535268Z",
          "iopub.status.idle": "2024-09-21T13:58:38.539597Z",
          "shell.execute_reply.started": "2024-09-21T13:58:38.535240Z",
          "shell.execute_reply": "2024-09-21T13:58:38.538658Z"
        },
        "trusted": true,
        "id": "gh8oe5S3V9O4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN\n",
        "!pip install jsonlines\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T13:58:40.697123Z",
          "iopub.execute_input": "2024-09-21T13:58:40.697770Z",
          "iopub.status.idle": "2024-09-21T13:58:53.945147Z",
          "shell.execute_reply.started": "2024-09-21T13:58:40.697734Z",
          "shell.execute_reply": "2024-09-21T13:58:53.943900Z"
        },
        "trusted": true,
        "id": "DTnfB4ABV9O4",
        "outputId": "5ee53d6e-af38-4e73-a531-0206a0429ca6"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting jsonlines\n  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\nRequirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonlines) (23.2.0)\nDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\nInstalling collected packages: jsonlines\nSuccessfully installed jsonlines-4.0.0\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN\n",
        "!pip install captum==0.7.0\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T13:58:53.947207Z",
          "iopub.execute_input": "2024-09-21T13:58:53.947549Z",
          "iopub.status.idle": "2024-09-21T13:59:06.750898Z",
          "shell.execute_reply.started": "2024-09-21T13:58:53.947520Z",
          "shell.execute_reply": "2024-09-21T13:59:06.749884Z"
        },
        "trusted": true,
        "id": "tg45DjGVV9O5",
        "outputId": "bc022b5c-e43a-470d-aaed-e72ac0638972"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting captum==0.7.0\n  Downloading captum-0.7.0-py3-none-any.whl.metadata (26 kB)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from captum==0.7.0) (3.7.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from captum==0.7.0) (1.24.4)\nRequirement already satisfied: torch>=1.6 in /opt/conda/lib/python3.10/site-packages (from captum==0.7.0) (2.1.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from captum==0.7.0) (4.66.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum==0.7.0) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum==0.7.0) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum==0.7.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum==0.7.0) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum==0.7.0) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum==0.7.0) (2023.12.2)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum==0.7.0) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum==0.7.0) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum==0.7.0) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum==0.7.0) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum==0.7.0) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum==0.7.0) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum==0.7.0) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum==0.7.0) (2.8.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->captum==0.7.0) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6->captum==0.7.0) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6->captum==0.7.0) (1.3.0)\nDownloading captum-0.7.0-py3-none-any.whl (1.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: captum\nSuccessfully installed captum-0.7.0\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "import jsonlines"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T13:59:06.752095Z",
          "iopub.execute_input": "2024-09-21T13:59:06.752393Z",
          "iopub.status.idle": "2024-09-21T13:59:06.791148Z",
          "shell.execute_reply.started": "2024-09-21T13:59:06.752365Z",
          "shell.execute_reply": "2024-09-21T13:59:06.790344Z"
        },
        "trusted": true,
        "id": "c_cFYESqV9O5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "!pip install wget\n",
        "#!pip install transformers"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T13:59:06.793526Z",
          "iopub.execute_input": "2024-09-21T13:59:06.794166Z",
          "iopub.status.idle": "2024-09-21T13:59:21.487857Z",
          "shell.execute_reply.started": "2024-09-21T13:59:06.794131Z",
          "shell.execute_reply": "2024-09-21T13:59:21.486810Z"
        },
        "trusted": true,
        "id": "gsEo9PwQV9O6",
        "outputId": "112a5969-5722-400f-c2c8-d77229179910"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting wget\n  Downloading wget-3.2.zip (10 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: wget\n  Building wheel for wget (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=74f422aeac47b29beefa1e50dc028ae40c2a6a719349bce908c1bae4b8c703f5\n  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\nSuccessfully built wget\nInstalling collected packages: wget\nSuccessfully installed wget-3.2\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "import sys\n",
        "sys.path.append('/kaggle/working/transformers/examples/research_projects/visual_bert')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T13:59:21.489234Z",
          "iopub.execute_input": "2024-09-21T13:59:21.489551Z",
          "iopub.status.idle": "2024-09-21T13:59:21.495107Z",
          "shell.execute_reply.started": "2024-09-21T13:59:21.489523Z",
          "shell.execute_reply": "2024-09-21T13:59:21.494267Z"
        },
        "trusted": true,
        "id": "Vck1H2pmV9O6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "import numpy as np\n",
        "import jsonlines\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import sys\n",
        "import os\n",
        "from IPython.display import Image, display\n",
        "import PIL.Image\n",
        "import io\n",
        "import torch\n",
        "import numpy as np\n",
        "from processing_image import Preprocess\n",
        "from visualizing_image import SingleImageViz\n",
        "from modeling_frcnn import GeneralizedRCNN\n",
        "from utils import Config\n",
        "import utils\n",
        "from transformers import VisualBertForQuestionAnswering, BertTokenizerFast\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
        "from sklearn import svm\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T13:59:21.496167Z",
          "iopub.execute_input": "2024-09-21T13:59:21.496549Z",
          "iopub.status.idle": "2024-09-21T13:59:31.121544Z",
          "shell.execute_reply.started": "2024-09-21T13:59:21.496523Z",
          "shell.execute_reply": "2024-09-21T13:59:31.120520Z"
        },
        "trusted": true,
        "id": "ryqFbrcWV9O6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "train_file = '/kaggle/input/facebook-hateful-meme-dataset/data/train.jsonl'\n",
        "k =0\n",
        "with jsonlines.open(train_file) as reader:\n",
        "    for obj in reader:\n",
        "        print(obj['img'])\n",
        "        if k==9:\n",
        "            break\n",
        "        k+=1"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T13:59:31.122860Z",
          "iopub.execute_input": "2024-09-21T13:59:31.123481Z",
          "iopub.status.idle": "2024-09-21T13:59:31.153189Z",
          "shell.execute_reply.started": "2024-09-21T13:59:31.123441Z",
          "shell.execute_reply": "2024-09-21T13:59:31.152329Z"
        },
        "trusted": true,
        "id": "hJycM-UFV9O7",
        "outputId": "d98b7209-5618-44a1-ce4d-7a951f4a8bd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "img/42953.png\nimg/23058.png\nimg/13894.png\nimg/37408.png\nimg/82403.png\nimg/16952.png\nimg/76932.png\nimg/70914.png\nimg/02973.png\nimg/58306.png\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "visual_feats = {}"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T13:59:31.154327Z",
          "iopub.execute_input": "2024-09-21T13:59:31.154668Z",
          "iopub.status.idle": "2024-09-21T13:59:31.158645Z",
          "shell.execute_reply.started": "2024-09-21T13:59:31.154636Z",
          "shell.execute_reply": "2024-09-21T13:59:31.157644Z"
        },
        "trusted": true,
        "id": "L7rnVuauV9O7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "device = 'cuda'\n",
        "frcnn_cfg = Config.from_pretrained(\"unc-nlp/frcnn-vg-finetuned\")\n",
        "frcnn_cfg.MODEL.device = device\n",
        "frcnn = GeneralizedRCNN.from_pretrained(\"unc-nlp/frcnn-vg-finetuned\", config=frcnn_cfg)\n",
        "image_preprocess = Preprocess(frcnn_cfg)\n",
        "frcnn.eval()\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T13:59:31.160447Z",
          "iopub.execute_input": "2024-09-21T13:59:31.160750Z",
          "iopub.status.idle": "2024-09-21T13:59:38.604939Z",
          "shell.execute_reply.started": "2024-09-21T13:59:31.160726Z",
          "shell.execute_reply": "2024-09-21T13:59:38.604062Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "0e769a1c2fc94296ba2cf2e9e0c8fb96",
            "b905c808d3c14e6cb4b263b638fac470"
          ]
        },
        "id": "YfTaAr72V9O8",
        "outputId": "69c0cb9c-eb40-4b1e-d55f-69f277e52bb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "%s not found in cache or force_download set to True, downloading to %s https://s3.amazonaws.com/models.huggingface.co/bert/unc-nlp/frcnn-vg-finetuned/config.yaml /root/.cache/torch/transformers/tmpup494v11\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/2.13k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0e769a1c2fc94296ba2cf2e9e0c8fb96"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "loading configuration file cache\n%s not found in cache or force_download set to True, downloading to %s https://cdn.huggingface.co/unc-nlp/frcnn-vg-finetuned/pytorch_model.bin /root/.cache/torch/transformers/tmpjy_0xhip\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/262M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b905c808d3c14e6cb4b263b638fac470"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "loading weights file https://cdn.huggingface.co/unc-nlp/frcnn-vg-finetuned/pytorch_model.bin from cache at /root/.cache/torch/transformers/57f6df6abe353be2773f2700159c65615babf39ab5b48114d2b49267672ae10f.77b59256a4cf8343ae0f923246a81489fc8d82f98d082edc2d2037c977c0d9d0\nAll model checkpoint weights were used when initializing GeneralizedRCNN.\n\nAll the weights of GeneralizedRCNN were initialized from the model checkpoint at unc-nlp/frcnn-vg-finetuned.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use GeneralizedRCNN for predictions without further training.\n",
          "output_type": "stream"
        },
        {
          "execution_count": 12,
          "output_type": "execute_result",
          "data": {
            "text/plain": "GeneralizedRCNN(\n  (backbone): ResNet(\n    (stem): BasicStem(\n      (conv1): Conv2d(\n        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n        (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (res2): Sequential(\n      (0): BottleneckBlock(\n        (shortcut): Conv2d(\n          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv1): Conv2d(\n          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BottleneckBlock(\n        (conv1): Conv2d(\n          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (2): BottleneckBlock(\n        (conv1): Conv2d(\n          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (res3): Sequential(\n      (0): BottleneckBlock(\n        (shortcut): Conv2d(\n          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv1): Conv2d(\n          256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BottleneckBlock(\n        (conv1): Conv2d(\n          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (2): BottleneckBlock(\n        (conv1): Conv2d(\n          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (3): BottleneckBlock(\n        (conv1): Conv2d(\n          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (res4): Sequential(\n      (0): BottleneckBlock(\n        (shortcut): Conv2d(\n          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv1): Conv2d(\n          512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (2): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (3): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (4): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (5): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (6): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (7): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (8): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (9): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (10): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (11): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (12): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (13): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (14): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (15): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (16): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (17): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (18): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (19): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (20): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (21): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (22): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n    )\n  )\n  (proposal_generator): RPN(\n    (anchor_generator): AnchorGenerator(\n      (cell_anchors): ParameterList(  (0): Parameter containing: [torch.float32 of size 12x4 (cuda:0)])\n    )\n    (rpn_head): RPNHead(\n      (conv): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (objectness_logits): Conv2d(512, 12, kernel_size=(1, 1), stride=(1, 1))\n      (anchor_deltas): Conv2d(512, 48, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n  (roi_heads): Res5ROIHeads(\n    (pooler): ROIPooler(\n      (level_poolers): ModuleList(\n        (0): RoIPool(output_size=(14, 14), spatial_scale=0.0625)\n      )\n    )\n    (res5): Sequential(\n      (0): BottleneckBlock(\n        (shortcut): Conv2d(\n          1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv1): Conv2d(\n          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False\n          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BottleneckBlock(\n        (conv1): Conv2d(\n          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False\n          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (2): BottleneckBlock(\n        (conv1): Conv2d(\n          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv2): Conv2d(\n          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False\n          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (conv3): Conv2d(\n          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (box_predictor): FastRCNNOutputLayers(\n      (cls_score): Linear(in_features=2048, out_features=1601, bias=True)\n      (bbox_pred): Linear(in_features=2048, out_features=6400, bias=True)\n      (cls_embedding): Embedding(1601, 256)\n      (fc_attr): Linear(in_features=2304, out_features=512, bias=True)\n      (attr_score): Linear(in_features=512, out_features=401, bias=True)\n    )\n  )\n)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "def get_visual_embedding(img_paths):\n",
        "    images, sizes, scales_yx = image_preprocess(img_paths) # img_paths -> list of image paths\n",
        "    output_dict = frcnn(\n",
        "      images,\n",
        "      sizes,\n",
        "      scales_yx=scales_yx,\n",
        "      padding=\"max_detections\",\n",
        "      max_detections=frcnn_cfg.max_detections,\n",
        "      return_tensors=\"pt\",\n",
        "    )\n",
        "    features = output_dict.get(\"roi_features\")\n",
        "    visual_embeds = features\n",
        "    visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\n",
        "    visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n",
        "    return visual_embeds,visual_token_type_ids,visual_attention_mask"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T13:59:38.608739Z",
          "iopub.execute_input": "2024-09-21T13:59:38.609065Z",
          "iopub.status.idle": "2024-09-21T13:59:38.615286Z",
          "shell.execute_reply.started": "2024-09-21T13:59:38.609036Z",
          "shell.execute_reply": "2024-09-21T13:59:38.614341Z"
        },
        "trusted": true,
        "id": "sVZcA3n6V9O8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "visual_feats = torch.load('./vs_tensors.pt')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T13:59:38.616366Z",
          "iopub.execute_input": "2024-09-21T13:59:38.616656Z",
          "iopub.status.idle": "2024-09-21T13:59:41.700161Z",
          "shell.execute_reply.started": "2024-09-21T13:59:38.616632Z",
          "shell.execute_reply": "2024-09-21T13:59:41.699355Z"
        },
        "trusted": true,
        "id": "1BjTD6rKV9O8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "len(visual_feats)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T13:59:41.701418Z",
          "iopub.execute_input": "2024-09-21T13:59:41.702068Z",
          "iopub.status.idle": "2024-09-21T13:59:41.708103Z",
          "shell.execute_reply.started": "2024-09-21T13:59:41.702033Z",
          "shell.execute_reply": "2024-09-21T13:59:41.707212Z"
        },
        "trusted": true,
        "id": "PrSbmVnUV9O9",
        "outputId": "941ad51a-40fa-4182-b924-fec9a12ce08d"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 15,
          "output_type": "execute_result",
          "data": {
            "text/plain": "8500"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# visual_feats"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T13:59:41.709256Z",
          "iopub.execute_input": "2024-09-21T13:59:41.709590Z",
          "iopub.status.idle": "2024-09-21T13:59:42.646852Z",
          "shell.execute_reply.started": "2024-09-21T13:59:41.709566Z",
          "shell.execute_reply": "2024-09-21T13:59:42.645941Z"
        },
        "trusted": true,
        "id": "8sd4owOVV9O9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "from utils import Config\n",
        "import utils\n",
        "from transformers import VisualBertModel, BertTokenizer\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.metrics import accuracy_score,f1_score,roc_auc_score\n",
        "from scipy.special import softmax"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T13:59:42.647983Z",
          "iopub.execute_input": "2024-09-21T13:59:42.648271Z",
          "iopub.status.idle": "2024-09-21T13:59:42.655956Z",
          "shell.execute_reply.started": "2024-09-21T13:59:42.648240Z",
          "shell.execute_reply": "2024-09-21T13:59:42.655331Z"
        },
        "trusted": true,
        "id": "NaSEcD_9V9O9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "train_set = []\n",
        "train_file = '/kaggle/input/facebook-hateful-meme-dataset/data/train.jsonl'\n",
        "k =0\n",
        "with jsonlines.open(train_file) as reader:\n",
        "    for obj in reader:\n",
        "    #         print(obj['img'])\n",
        "\n",
        "        train_set.append({'text': obj['text'],\n",
        "\n",
        "                       'img': obj['img'],\n",
        "\n",
        "                       'label': int(obj['label'])})\n",
        "    #         print(k)\n",
        "    #         if k==1500:\n",
        "    #             break\n",
        "        k+=1\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T13:59:42.657344Z",
          "iopub.execute_input": "2024-09-21T13:59:42.657661Z",
          "iopub.status.idle": "2024-09-21T13:59:42.704062Z",
          "shell.execute_reply.started": "2024-09-21T13:59:42.657633Z",
          "shell.execute_reply": "2024-09-21T13:59:42.703168Z"
        },
        "trusted": true,
        "id": "Eq8_wDKQV9O-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "len(train_set)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T13:59:42.705071Z",
          "iopub.execute_input": "2024-09-21T13:59:42.705344Z",
          "iopub.status.idle": "2024-09-21T13:59:42.711009Z",
          "shell.execute_reply.started": "2024-09-21T13:59:42.705309Z",
          "shell.execute_reply": "2024-09-21T13:59:42.710123Z"
        },
        "trusted": true,
        "id": "ljkFlMUwV9O-",
        "outputId": "7d2e767a-aa9c-475e-d9d7-93ea7e0ec906"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 19,
          "output_type": "execute_result",
          "data": {
            "text/plain": "8500"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, ds, max_len=64):\n",
        "        self.ds = ds\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "        self.model = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "\n",
        "\n",
        "        self.max_len = max_len\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        ds_idx = self.ds[index]\n",
        "        inputs = self.tokenizer(ds_idx['text'], padding=\"max_length\", truncation=True, max_length=self.max_len, return_tensors='pt')\n",
        "\n",
        "\n",
        "        inputs['input_ids'] = inputs['input_ids'].squeeze(0)\n",
        "        inputs['token_type_ids'] = inputs['token_type_ids'].squeeze(0)\n",
        "        inputs['attention_mask'] = inputs['attention_mask'].squeeze(0)\n",
        "\n",
        "        # visual_embeds,visual_token_type_ids,visual_attention_mask = get_visual_embedding('./data/'+ds_idx['img'])\n",
        "        visual_embeds,visual_token_type_ids,visual_attention_mask = visual_feats[ds_idx['img']]\n",
        "\n",
        "        #print(ds_idx['img'])\n",
        "\n",
        "        inputs.update({\n",
        "          \"visual_embeds\": torch.squeeze(visual_embeds),\n",
        "          \"visual_token_type_ids\": torch.squeeze(visual_token_type_ids),\n",
        "          \"visual_attention_mask\": torch.squeeze(visual_attention_mask)\n",
        "        })\n",
        "\n",
        "\n",
        "\n",
        "        #inputs.update({\"path\": ds_idx['img']})\n",
        "\n",
        "        return inputs, int(ds_idx['label'])\n",
        "\n",
        "t = CustomDataset(train_set)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T13:59:42.712369Z",
          "iopub.execute_input": "2024-09-21T13:59:42.712629Z",
          "iopub.status.idle": "2024-09-21T13:59:56.230152Z",
          "shell.execute_reply.started": "2024-09-21T13:59:42.712607Z",
          "shell.execute_reply": "2024-09-21T13:59:56.229248Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "d7120cb322214ba0b3cdc348c329e2cb",
            "50b482d1671e4843b06fc267c37d8428",
            "e29d80c77d844a00884cc97eea4ccca1",
            "44ae8608317b49398749956a61ef9e76",
            "9f5d8d891dc744178bbd8a2f5dbb23cc",
            "38a24595a04643038f0d5be06740954c"
          ]
        },
        "id": "rOYbotKxV9O-",
        "outputId": "05bdb704-fb35-46c6-abe0-17818bf77752"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d7120cb322214ba0b3cdc348c329e2cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "50b482d1671e4843b06fc267c37d8428"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e29d80c77d844a00884cc97eea4ccca1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "44ae8608317b49398749956a61ef9e76"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/631 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f5d8d891dc744178bbd8a2f5dbb23cc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "pytorch_model.bin:   0%|          | 0.00/448M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "38a24595a04643038f0d5be06740954c"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!export CUBLAS_WORKSPACE_CONFIG=:16:8"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T13:59:56.231857Z",
          "iopub.execute_input": "2024-09-21T13:59:56.232482Z",
          "iopub.status.idle": "2024-09-21T13:59:56.236377Z",
          "shell.execute_reply.started": "2024-09-21T13:59:56.232448Z",
          "shell.execute_reply": "2024-09-21T13:59:56.235551Z"
        },
        "trusted": true,
        "id": "JlDiTPjOV9O_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "seed=42\n",
        "random.seed(seed)     # python random generator\n",
        "np.random.seed(seed)  # numpy random generator\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "#torch.use_deterministic_algorithms(True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T13:59:56.237494Z",
          "iopub.execute_input": "2024-09-21T13:59:56.237756Z",
          "iopub.status.idle": "2024-09-21T13:59:56.247765Z",
          "shell.execute_reply.started": "2024-09-21T13:59:56.237734Z",
          "shell.execute_reply": "2024-09-21T13:59:56.246986Z"
        },
        "trusted": true,
        "id": "ECejZZ5UV9O_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(t)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T13:59:56.248656Z",
          "iopub.execute_input": "2024-09-21T13:59:56.248889Z",
          "iopub.status.idle": "2024-09-21T13:59:56.258551Z",
          "shell.execute_reply.started": "2024-09-21T13:59:56.248869Z",
          "shell.execute_reply": "2024-09-21T13:59:56.257622Z"
        },
        "trusted": true,
        "id": "ez3sR4FCV9O_",
        "outputId": "e60694d4-1fe1-48ad-a004-4d8b0c1d9096"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 23,
          "output_type": "execute_result",
          "data": {
            "text/plain": "8500"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "torch.manual_seed(42)\n",
        "t_p,te_p = torch.utils.data.random_split(t,[7840,660])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T13:59:56.259702Z",
          "iopub.execute_input": "2024-09-21T13:59:56.259979Z",
          "iopub.status.idle": "2024-09-21T13:59:56.271040Z",
          "shell.execute_reply.started": "2024-09-21T13:59:56.259956Z",
          "shell.execute_reply": "2024-09-21T13:59:56.270232Z"
        },
        "trusted": true,
        "id": "w3zW34GPV9PA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(t_p, shuffle=True, batch_size=32)\n",
        "eval_dataloader = DataLoader(te_p, batch_size=1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T13:59:56.272212Z",
          "iopub.execute_input": "2024-09-21T13:59:56.272634Z",
          "iopub.status.idle": "2024-09-21T13:59:56.277676Z",
          "shell.execute_reply.started": "2024-09-21T13:59:56.272604Z",
          "shell.execute_reply": "2024-09-21T13:59:56.276775Z"
        },
        "trusted": true,
        "id": "1Sle5lzWV9PA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T13:59:56.278609Z",
          "iopub.execute_input": "2024-09-21T13:59:56.278858Z",
          "iopub.status.idle": "2024-09-21T13:59:56.286182Z",
          "shell.execute_reply.started": "2024-09-21T13:59:56.278837Z",
          "shell.execute_reply": "2024-09-21T13:59:56.285464Z"
        },
        "trusted": true,
        "id": "wZLuoBwyV9PA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('h')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T13:59:56.287303Z",
          "iopub.execute_input": "2024-09-21T13:59:56.287551Z",
          "iopub.status.idle": "2024-09-21T13:59:56.295350Z",
          "shell.execute_reply.started": "2024-09-21T13:59:56.287529Z",
          "shell.execute_reply": "2024-09-21T13:59:56.294472Z"
        },
        "trusted": true,
        "id": "j8CRuVr4V9PB",
        "outputId": "bc3f301e-859e-471a-ac53-32cbe3beb7aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "h\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "concept_classes = [\"holocaust\", \"nazism\", \"genocide\", \"funny\", \"anti muslim\", \"terrorism\", \"violence\", \"politics\", \"racism\", \"international relation\", \"adult\", \"gore\", \"misogynistic\", \"immigration\", \"extremism\", \"immoral\", \"white supremacy\", \"indecency\"]\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T13:59:56.296411Z",
          "iopub.execute_input": "2024-09-21T13:59:56.296793Z",
          "iopub.status.idle": "2024-09-21T13:59:56.303116Z",
          "shell.execute_reply.started": "2024-09-21T13:59:56.296769Z",
          "shell.execute_reply": "2024-09-21T13:59:56.302108Z"
        },
        "trusted": true,
        "id": "aQH8t-t0V9PB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(concept_classes)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T13:59:56.304090Z",
          "iopub.execute_input": "2024-09-21T13:59:56.304423Z",
          "iopub.status.idle": "2024-09-21T13:59:56.316786Z",
          "shell.execute_reply.started": "2024-09-21T13:59:56.304390Z",
          "shell.execute_reply": "2024-09-21T13:59:56.315859Z"
        },
        "trusted": true,
        "id": "A68-q9aNV9PB",
        "outputId": "0ecf7825-0233-4ca9-b6c8-9479182b2bf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 29,
          "output_type": "execute_result",
          "data": {
            "text/plain": "18"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cnt"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T13:59:56.317839Z",
          "iopub.execute_input": "2024-09-21T13:59:56.318165Z",
          "iopub.status.idle": "2024-09-21T13:59:56.324983Z",
          "shell.execute_reply.started": "2024-09-21T13:59:56.318142Z",
          "shell.execute_reply": "2024-09-21T13:59:56.324233Z"
        },
        "trusted": true,
        "id": "Fsophh1lV9PC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dP2W0erUV9PC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ylGAR6-mV9PC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aW2tyoTpV9PC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L2eNcomhV9PD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CU_7PJXTV9PD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "gpWYdvU9V9PD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "with open('/kaggle/working/full_annotation.pkl', 'rb') as f:\n",
        "    mynewlist = pickle.load(f)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T13:59:56.331637Z",
          "iopub.execute_input": "2024-09-21T13:59:56.332030Z",
          "iopub.status.idle": "2024-09-21T13:59:56.336939Z",
          "shell.execute_reply.started": "2024-09-21T13:59:56.331986Z",
          "shell.execute_reply": "2024-09-21T13:59:56.336106Z"
        },
        "trusted": true,
        "id": "-NdmZaTNV9PD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(mynewlist)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T13:59:56.338074Z",
          "iopub.execute_input": "2024-09-21T13:59:56.338369Z",
          "iopub.status.idle": "2024-09-21T13:59:56.346257Z",
          "shell.execute_reply.started": "2024-09-21T13:59:56.338342Z",
          "shell.execute_reply": "2024-09-21T13:59:56.345350Z"
        },
        "trusted": true,
        "id": "O0k9OaIXV9PD",
        "outputId": "82ab3820-4034-4312-a371-06922e56dc0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 32,
          "output_type": "execute_result",
          "data": {
            "text/plain": "208"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "ZqyMGZGnV9PE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "gc=mynewlist"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T13:59:56.347405Z",
          "iopub.execute_input": "2024-09-21T13:59:56.347683Z",
          "iopub.status.idle": "2024-09-21T13:59:56.355596Z",
          "shell.execute_reply.started": "2024-09-21T13:59:56.347654Z",
          "shell.execute_reply": "2024-09-21T13:59:56.354683Z"
        },
        "trusted": true,
        "id": "dneWoe82V9PE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "device='cuda'"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T13:59:56.356878Z",
          "iopub.execute_input": "2024-09-21T13:59:56.357213Z",
          "iopub.status.idle": "2024-09-21T13:59:56.363829Z",
          "shell.execute_reply.started": "2024-09-21T13:59:56.357184Z",
          "shell.execute_reply": "2024-09-21T13:59:56.362843Z"
        },
        "trusted": true,
        "id": "0RfX8f0xV9PE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "elJks-jlV9PF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T13:59:56.365155Z",
          "iopub.execute_input": "2024-09-21T13:59:56.365597Z",
          "iopub.status.idle": "2024-09-21T13:59:56.374260Z",
          "shell.execute_reply.started": "2024-09-21T13:59:56.365567Z",
          "shell.execute_reply": "2024-09-21T13:59:56.373329Z"
        },
        "trusted": true,
        "id": "dAft3UFtV9PF",
        "outputId": "ee78e973-3d12-4b2c-a580-efdb88db05fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 35,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'cuda'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T13:59:56.375541Z",
          "iopub.execute_input": "2024-09-21T13:59:56.375848Z",
          "iopub.status.idle": "2024-09-21T13:59:56.526083Z",
          "shell.execute_reply.started": "2024-09-21T13:59:56.375824Z",
          "shell.execute_reply": "2024-09-21T13:59:56.525344Z"
        },
        "trusted": true,
        "id": "QwOvQUcIV9PG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(\"jew\")[1:-1]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T13:59:56.527173Z",
          "iopub.execute_input": "2024-09-21T13:59:56.527431Z",
          "iopub.status.idle": "2024-09-21T13:59:56.534453Z",
          "shell.execute_reply.started": "2024-09-21T13:59:56.527408Z",
          "shell.execute_reply": "2024-09-21T13:59:56.533340Z"
        },
        "trusted": true,
        "id": "51_YKWIiV9PG",
        "outputId": "fa168c44-3978-41a8-fd1c-12a6d5a03659"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 37,
          "output_type": "execute_result",
          "data": {
            "text/plain": "[16522]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "DYN_ROUTE = True\n",
        "DECONFOUND = True"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T13:59:56.535611Z",
          "iopub.execute_input": "2024-09-21T13:59:56.535907Z",
          "iopub.status.idle": "2024-09-21T13:59:56.542868Z",
          "shell.execute_reply.started": "2024-09-21T13:59:56.535884Z",
          "shell.execute_reply": "2024-09-21T13:59:56.541933Z"
        },
        "trusted": true,
        "id": "YC9QIhp2V9PH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "!pip install pytorch_revgrad\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T13:59:56.543827Z",
          "iopub.execute_input": "2024-09-21T13:59:56.544157Z",
          "iopub.status.idle": "2024-09-21T14:00:09.159917Z",
          "shell.execute_reply.started": "2024-09-21T13:59:56.544126Z",
          "shell.execute_reply": "2024-09-21T14:00:09.158926Z"
        },
        "trusted": true,
        "id": "L0lCbvZEV9PH",
        "outputId": "9321bcf2-55c5-440e-f4ee-a395b50f2bd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting pytorch_revgrad\n  Downloading pytorch_revgrad-0.2.0-py3-none-any.whl.metadata (1.7 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pytorch_revgrad) (1.24.4)\nRequirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_revgrad) (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->pytorch_revgrad) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->pytorch_revgrad) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->pytorch_revgrad) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->pytorch_revgrad) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->pytorch_revgrad) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->pytorch_revgrad) (2023.12.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->pytorch_revgrad) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->pytorch_revgrad) (1.3.0)\nDownloading pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\nInstalling collected packages: pytorch_revgrad\nSuccessfully installed pytorch_revgrad-0.2.0\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.nn.Parameter(torch.tensor(0.1))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T14:00:09.161472Z",
          "iopub.execute_input": "2024-09-21T14:00:09.161821Z",
          "iopub.status.idle": "2024-09-21T14:00:09.166217Z",
          "shell.execute_reply.started": "2024-09-21T14:00:09.161789Z",
          "shell.execute_reply": "2024-09-21T14:00:09.165344Z"
        },
        "trusted": true,
        "id": "gD-QFPtPV9PH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "num_concepts = 18\n",
        "#BS = 8\n",
        "import torch\n",
        "from torch import nn\n",
        "from pytorch_revgrad import RevGrad\n",
        "class VB(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VB, self).__init__()\n",
        "        self.vb = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(768, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 2),\n",
        "        )\n",
        "\n",
        "\n",
        "        self.linear_relu_stack_latent = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(768, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 2),\n",
        "        )\n",
        "\n",
        "\n",
        "        self.linear_relu_stack_hidden = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(768, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 2),\n",
        "        )\n",
        "\n",
        "        #self.w = torch.nn.Parameter(torch.ones(11,1)/11)\n",
        "        self.w = torch.nn.Parameter(torch.ones(num_concepts,1))\n",
        "\n",
        "        self.project_latent = nn.Linear(768,2048)\n",
        "\n",
        "        #self.W_ij = [[nn.Linear(768,768) for i in range(64)] for j in range(17)]\n",
        "\n",
        "        self.W_ij = torch.nn.Parameter(torch.randn(18,64,28,28))\n",
        "\n",
        "        self.proj_conc = nn.Linear(768,28)\n",
        "        self.proj_embed = nn.Linear(768,28)\n",
        "\n",
        "\n",
        "\n",
        "        self.grad_rev1 = RevGrad()\n",
        "        self.grad_rev2 = RevGrad()\n",
        "\n",
        "        self.w1 = torch.nn.Parameter(torch.tensor(0.2))\n",
        "\n",
        "\n",
        "    def forward(self,\n",
        "        all_concepts_,\n",
        "        input_ids = None,\n",
        "        attention_mask = None,\n",
        "        token_type_ids = None,\n",
        "        position_ids = None,\n",
        "        head_mask = None,\n",
        "        inputs_embeds = None,\n",
        "        visual_embeds = None,\n",
        "        visual_attention_mask = None,\n",
        "        visual_token_type_ids = None,\n",
        "        image_text_alignment = None,\n",
        "        output_attentions = None,\n",
        "        output_hidden_states = None,\n",
        "        return_dict = None):\n",
        "\n",
        "        #         print(all_concepts.shape)\n",
        "\n",
        "        B = visual_embeds.size(0)\n",
        "\n",
        "        #         print(B)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ##############################################\n",
        "\n",
        "        all_concepts = self.proj_conc(all_concepts_)\n",
        "\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs_embeds = self.vb.embeddings.word_embeddings(input_ids)\n",
        "\n",
        "        #print(0/0)\n",
        "\n",
        "        inputs_embeds = self.proj_embed(inputs_embeds)\n",
        "\n",
        "\n",
        "        #####################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #b1 = torch.bmm(inputs_embeds,all_concepts.repeat(B,1,1).transpose(1,2))\n",
        "\n",
        "        #print(inputs_embeds.shape)\n",
        "        #print(all_concepts.repeat(B,1,1).transpose(1,2).shape)\n",
        "\n",
        "        #b,64,28\n",
        "        #b,28,17\n",
        "\n",
        "\n",
        "        b = torch.einsum('blx,bxn->bln',inputs_embeds,all_concepts.repeat(B,1,1).transpose(1,2))\n",
        "\n",
        "        #         print(b1)\n",
        "\n",
        "        #         print('********')\n",
        "\n",
        "        #         print(b2)\n",
        "\n",
        "        #         print(torch.equal(b1,b2))\n",
        "\n",
        "\n",
        "\n",
        "        b = b.masked_fill(~attention_mask.unsqueeze(dim=2).bool(), -np.inf)\n",
        "        c = F.softmax(b,dim=1)\n",
        "\n",
        "\n",
        "\n",
        "        #inter = torch.einsum('bcsn,csnn->bcsn',inputs_embeds.unsqueeze(1).repeat(1,17,1,1), self.W_ij)\n",
        "\n",
        "        #print(inter[:,0,0,:])\n",
        "\n",
        "        einsum_expression = 'bln,clnx->bclx'\n",
        "        inter = torch.einsum(einsum_expression, inputs_embeds, self.W_ij)\n",
        "\n",
        "        k = torch.einsum('bcsn,bsc->bcn',inter,c)\n",
        "        norm = torch.norm(k,dim=2)\n",
        "\n",
        "\n",
        "\n",
        "        kk1 = ((norm**2)/(1+norm**2)).unsqueeze(dim=2) * (k/norm.unsqueeze(dim=2))\n",
        "\n",
        "        w_dynamic = kk1.norm(dim=2).unsqueeze(2)\n",
        "\n",
        "\n",
        "        raw_latent = (1-self.w1) * all_concepts_.unsqueeze(0).repeat(B,1,1) * self.w.unsqueeze(dim=0).repeat(B,1,1) + all_concepts_.unsqueeze(0).repeat(B,1,1) * w_dynamic * self.w1\n",
        "\n",
        "        raw_latent = torch.mean(raw_latent, dim=1)\n",
        "\n",
        "        #raw_latent = torch.mean(all_concepts*self.w,dim=0)\n",
        "        #raw_latent = raw_latent.unsqueeze(dim=0)\n",
        "\n",
        "        latent = self.project_latent(raw_latent)\n",
        "\n",
        "\n",
        "\n",
        "        latent = latent.unsqueeze(dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        visual_embeds_latent = torch.cat((visual_embeds,latent),dim=1)\n",
        "\n",
        "\n",
        "        ################################################################\n",
        "\n",
        "\n",
        "        #print(b)\n",
        "\n",
        "        #######################################################\n",
        "        #         print(b.shape)\n",
        "\n",
        "        #         b = []\n",
        "        #         for i in range(64):\n",
        "        #             bi = []\n",
        "        #             for j in range(17):\n",
        "        #                 #print(inputs_embeds[:,i,:].shape)\n",
        "        #                 ci = all_concepts[j].unsqueeze(0).repeat(32,1,1).squeeze()\n",
        "        #                 uj = inputs_embeds[:,i,:]\n",
        "        #                 #print(ci.shape)\n",
        "        #                 bij = torch.bmm(uj.unsqueeze(dim=1), ci.unsqueeze(dim=2))\n",
        "        #                 #print(bij.shape)\n",
        "        #                 #print(bij)\n",
        "        #                 bi.append(bij.squeeze())\n",
        "        #             bi = torch.stack(bi)\n",
        "\n",
        "        #             bi = bi.T\n",
        "        #             #print(bi.shape)\n",
        "        #             #print(bi)\n",
        "        #             b.append(bi)\n",
        "\n",
        "\n",
        "\n",
        "        #         b = torch.stack(b)\n",
        "        #         b = b.transpose(0,1)\n",
        "\n",
        "        #         print(b)\n",
        "        #         print(b.shape)\n",
        "\n",
        "\n",
        "        #print(0/0)\n",
        "\n",
        "        ####################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #c = F.softmax(b,dim=1)\n",
        "\n",
        "        #print(c)\n",
        "        #print(c.shape)\n",
        "        #print(0/0)\n",
        "        ############################################################\n",
        "\n",
        "        #print\n",
        "\n",
        "        #c = 32*64*17\n",
        "\n",
        "        #u_j_i = 32*768\n",
        "\n",
        "\n",
        "        #cij = 32*1\n",
        "\n",
        "\n",
        "        #         kk = []\n",
        "        #         for i in range(17):\n",
        "        #             si = 0\n",
        "        #             for j in range(64):\n",
        "        #                 #print(inputs_embeds[:,j,:].shape)\n",
        "        #                 #print(self.W_ij)\n",
        "        #                 #print(self.W_ij[i,j,:,:].shape)\n",
        "\n",
        "\n",
        "\n",
        "        #                 u_j_i = torch.mm(inputs_embeds[:,j,:],self.W_ij[i,j,:,:])\n",
        "\n",
        "\n",
        "        #                 #print(j,i,u_j_i)\n",
        "        #                 #print(0/0)\n",
        "\n",
        "        #                 cij = c[:,j,i].unsqueeze(dim=1)\n",
        "        #                 #print('cij ',cij)\n",
        "        #                 #print(cij.shape)\n",
        "        #                 #print(u_j_i)\n",
        "        #                 #print(u_j_i.shape)\n",
        "        #                 si+= u_j_i*cij\n",
        "\n",
        "        #                 #print(u_j_i*cij)\n",
        "        #             #print(si)\n",
        "        #             #print(0/0)\n",
        "\n",
        "        #             norm = torch.norm(si, dim=1)\n",
        "\n",
        "        #             #print(norm)\n",
        "        #             #print(norm.shape)\n",
        "\n",
        "        #             vi = ((norm**2)/(1+norm**2)).unsqueeze(dim=1) * (si/norm.unsqueeze(dim=1))\n",
        "\n",
        "        #             #print(vi)\n",
        "        #             kk.append(torch.norm(vi,dim=1))\n",
        "        #             #kk.append(vi)\n",
        "        #             #print(kk)\n",
        "\n",
        "\n",
        "        #         kk = torch.stack(kk)\n",
        "\n",
        "        #         print('kk', kk)\n",
        "        #         print('kk1', kk1)\n",
        "        #         print(kk.shape)\n",
        "        #         print(kk1.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Create a tensor of ones with the same number of columns as the original tensor\n",
        "        ones_column_vam = torch.ones((visual_attention_mask.size(0), 1), dtype=visual_attention_mask.dtype)\n",
        "\n",
        "        # Concatenate the original tensor with the tensor of ones along the second dimension (columns)\n",
        "        visual_attention_mask_latent = torch.cat((visual_attention_mask, ones_column_vam.to('cuda')), dim=1)\n",
        "\n",
        "        # Create a tensor of ones with the same number of columns as the original tensor\n",
        "        ones_column_vtt = torch.ones((visual_token_type_ids.size(0), 1), dtype=visual_token_type_ids.dtype)\n",
        "\n",
        "        # Concatenate the original tensor with the tensor of ones along the second dimension (columns)\n",
        "        visual_token_type_ids_latent = torch.cat((visual_token_type_ids, ones_column_vtt.to('cuda')), dim=1)\n",
        "\n",
        "        #         print(latent.shape)\n",
        "\n",
        "        #         print(visual_embeds.shape)\n",
        "        #         print(visual_attention_mask.shape)\n",
        "        #         print(visual_token_type_ids.shape)\n",
        "\n",
        "        #         print(self.w)\n",
        "\n",
        "        x = self.vb(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask, visual_embeds=visual_embeds_latent, visual_attention_mask=visual_attention_mask_latent, visual_token_type_ids=visual_token_type_ids_latent)\n",
        "\n",
        "        p = self.vb(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask, visual_embeds=visual_embeds, visual_attention_mask=visual_attention_mask, visual_token_type_ids=visual_token_type_ids)\n",
        "        p = self.grad_rev1(p.pooler_output)\n",
        "        #q = self.grad_rev2(raw_latent.repeat(B,1))\n",
        "        q = self.grad_rev2(raw_latent)\n",
        "\n",
        "        logits_p = self.linear_relu_stack_hidden(p)\n",
        "        logits_q = self.linear_relu_stack_latent(q)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        logits = self.linear_relu_stack(x.pooler_output)\n",
        "\n",
        "\n",
        "        if_logits_p = self.linear_relu_stack(p)\n",
        "        if_logits_q = self.linear_relu_stack(q)\n",
        "\n",
        "        return logits, logits_p, logits_q, if_logits_p, if_logits_q\n",
        "        #return logits, logits_p, logits_q, if_logits_p, if_logits_p\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T14:00:09.167649Z",
          "iopub.execute_input": "2024-09-21T14:00:09.167945Z",
          "iopub.status.idle": "2024-09-21T14:00:09.204890Z",
          "shell.execute_reply.started": "2024-09-21T14:00:09.167921Z",
          "shell.execute_reply": "2024-09-21T14:00:09.204105Z"
        },
        "trusted": true,
        "id": "P1chTUX2V9PI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mv4ONLZsV9PJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zjZeEdUvV9PJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xQTdCXVIV9PK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AHAoCGMjV9Pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oePq5I05V9Pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "vb = VB()\n",
        "\n",
        "# vb = nn.DataParallel(vb)\n",
        "vb.to(device)\n",
        "concept_classes = [\"holocaust\", \"nazism\", \"genocide\", \"funny\", \"anti muslim\", \"terrorism\", \"violence\", \"politics\", \"racism\", \"international relation\", \"adult\", \"gore\", \"misogynistic\", \"immigration\", \"extremism\", \"immoral\", \"white supremacy\", \"indecency\"]\n",
        "\n",
        "\n",
        "concept_embedding_map = {}\n",
        "\n",
        "all_concepts = []\n",
        "\n",
        "\n",
        "for c in concept_classes:\n",
        "    conc = torch.tensor(tokenizer.encode(c)[1:-1])\n",
        "    e = vb.vb.embeddings.word_embeddings(conc.to('cuda'))\n",
        "    x = e.detach().cpu().mean(dim=0)\n",
        "\n",
        "\n",
        "\n",
        "    all_concepts.append(x)\n",
        "\n",
        "    concept_embedding_map[c] = x\n",
        "\n",
        "\n",
        "all_concepts = torch.stack(all_concepts)\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T14:00:09.206086Z",
          "iopub.execute_input": "2024-09-21T14:00:09.206396Z",
          "iopub.status.idle": "2024-09-21T14:00:10.125802Z",
          "shell.execute_reply.started": "2024-09-21T14:00:09.206371Z",
          "shell.execute_reply": "2024-09-21T14:00:10.124758Z"
        },
        "trusted": true,
        "id": "3OSMoGDJV9Pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "all_concepts.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T14:00:10.127257Z",
          "iopub.execute_input": "2024-09-21T14:00:10.128098Z",
          "iopub.status.idle": "2024-09-21T14:00:10.134333Z",
          "shell.execute_reply.started": "2024-09-21T14:00:10.128063Z",
          "shell.execute_reply": "2024-09-21T14:00:10.133457Z"
        },
        "trusted": true,
        "id": "3Wya613dV9Pb",
        "outputId": "9958fef6-6ad5-4659-f99f-b650fa4529fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 43,
          "output_type": "execute_result",
          "data": {
            "text/plain": "torch.Size([18, 768])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "#'/kaggle/working/final_model_concept.pt'\n",
        "\n",
        "vb = VB()\n",
        "vb.load_state_dict(torch.load('/kaggle/working/final_model_wo_adversarial.pt'))\n",
        "vb.to('cuda')\n",
        "vb.eval()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T14:00:10.135511Z",
          "iopub.execute_input": "2024-09-21T14:00:10.135797Z",
          "iopub.status.idle": "2024-09-21T14:00:11.142911Z",
          "shell.execute_reply.started": "2024-09-21T14:00:10.135773Z",
          "shell.execute_reply": "2024-09-21T14:00:11.141729Z"
        },
        "trusted": true,
        "id": "chiLfzcJV9Pb",
        "outputId": "6c9a5cc1-e4a6-4413-f5c2-a3789c9c9d0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 44,
          "output_type": "execute_result",
          "data": {
            "text/plain": "VB(\n  (vb): VisualBertModel(\n    (embeddings): VisualBertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=1)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n      (visual_token_type_embeddings): Embedding(2, 768)\n      (visual_position_embeddings): Embedding(512, 768)\n      (visual_projection): Linear(in_features=2048, out_features=768, bias=True)\n    )\n    (encoder): VisualBertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x VisualBertLayer(\n          (attention): VisualBertAttention(\n            (self): VisualBertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): VisualBertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): VisualBertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): VisualBertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): VisualBertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (linear_relu_stack): Sequential(\n    (0): Dropout(p=0.1, inplace=False)\n    (1): Linear(in_features=768, out_features=512, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=512, out_features=128, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=128, out_features=2, bias=True)\n  )\n  (linear_relu_stack_latent): Sequential(\n    (0): Dropout(p=0.1, inplace=False)\n    (1): Linear(in_features=768, out_features=512, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=512, out_features=128, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=128, out_features=2, bias=True)\n  )\n  (linear_relu_stack_hidden): Sequential(\n    (0): Dropout(p=0.1, inplace=False)\n    (1): Linear(in_features=768, out_features=512, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=512, out_features=128, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=128, out_features=2, bias=True)\n  )\n  (project_latent): Linear(in_features=768, out_features=2048, bias=True)\n  (proj_conc): Linear(in_features=768, out_features=28, bias=True)\n  (proj_embed): Linear(in_features=768, out_features=28, bias=True)\n  (grad_rev1): RevGrad()\n  (grad_rev2): RevGrad()\n)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# all_concepts.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T14:00:11.144062Z",
          "iopub.execute_input": "2024-09-21T14:00:11.146540Z",
          "iopub.status.idle": "2024-09-21T14:00:11.150178Z",
          "shell.execute_reply.started": "2024-09-21T14:00:11.146511Z",
          "shell.execute_reply": "2024-09-21T14:00:11.149338Z"
        },
        "trusted": true,
        "id": "EFW4-jv-V9Pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all_concepts[0].unsqueeze(0).shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T14:00:11.151278Z",
          "iopub.execute_input": "2024-09-21T14:00:11.151670Z",
          "iopub.status.idle": "2024-09-21T14:00:11.159040Z",
          "shell.execute_reply.started": "2024-09-21T14:00:11.151635Z",
          "shell.execute_reply": "2024-09-21T14:00:11.158267Z"
        },
        "trusted": true,
        "id": "QrNrnj_xV9Pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all_concepts[0].unsqueeze(0).repeat(32,1,1).squeeze().shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T13:56:08.123402Z",
          "iopub.execute_input": "2024-09-21T13:56:08.124279Z",
          "iopub.status.idle": "2024-09-21T13:56:08.127967Z",
          "shell.execute_reply.started": "2024-09-21T13:56:08.124246Z",
          "shell.execute_reply": "2024-09-21T13:56:08.127054Z"
        },
        "trusted": true,
        "id": "NUHFmHV1V9Pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "# check evaluation performance separately on p and q using their classifier: should be less\n",
        "pred_r1, pred_r2 = [],[]\n",
        "pred_both = []\n",
        "act = []\n",
        "for ii, batch in enumerate(eval_dataloader):\n",
        "    with torch.no_grad():\n",
        "        batch_inp = {k: v.to(device) for k, v in batch[0].items()}\n",
        "        outputs, r1, r2,_,_ = vb(all_concepts.to('cuda'),**batch_inp)\n",
        "        act.append(batch[1].detach().cpu().numpy()[0])\n",
        "\n",
        "        pred_r1.append(np.argmax(r1.detach().cpu().numpy()[0]))\n",
        "        pred_r2.append(np.argmax(r2.detach().cpu().numpy()[0]))\n",
        "        pred_both.append(np.argmax(outputs.detach().cpu().numpy()[0]))\n",
        "\n",
        "print('*')\n",
        "print(f1_score(act,pred_r1,average='macro'))\n",
        "print(f1_score(act,pred_r1,average='weighted'))\n",
        "print(accuracy_score(act,pred_r1))\n",
        "print('*'*10)\n",
        "print(f1_score(act,pred_r2,average='macro'))\n",
        "print(f1_score(act,pred_r2,average='weighted'))\n",
        "print(accuracy_score(act,pred_r2))\n",
        "print('*'*10)\n",
        "print(f1_score(act,pred_both,average='macro'))\n",
        "print(f1_score(act,pred_both,average='weighted'))\n",
        "print(accuracy_score(act,pred_both))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T14:00:11.161995Z",
          "iopub.execute_input": "2024-09-21T14:00:11.162326Z",
          "iopub.status.idle": "2024-09-21T14:00:26.887927Z",
          "shell.execute_reply.started": "2024-09-21T14:00:11.162275Z",
          "shell.execute_reply": "2024-09-21T14:00:26.886837Z"
        },
        "trusted": true,
        "id": "qFEjoBa0V9Pc",
        "outputId": "8c7d44fa-eb36-4afd-cfcc-6ba1040f2661"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "*\n0.23349522892932548\n0.1549232109361029\n0.2924242424242424\n**********\n0.23963133640552994\n0.1510403574919704\n0.3151515151515151\n**********\n0.6793982390463974\n0.73381221836417\n0.746969696969697\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check evaluation performance separately on p and q using parent classifier: should be also less\n",
        "pred_r1, pred_r2 = [],[]\n",
        "pred_both = []\n",
        "act = []\n",
        "for ii, batch in enumerate(eval_dataloader):\n",
        "    with torch.no_grad():\n",
        "        batch_inp = {k: v.to(device) for k, v in batch[0].items()}\n",
        "        outputs, _,_, r1, r2 = vb(all_concepts.to('cuda'),**batch_inp)\n",
        "        act.append(batch[1].detach().cpu().numpy()[0])\n",
        "\n",
        "        pred_r1.append(np.argmax(r1.detach().cpu().numpy()[0]))\n",
        "        pred_r2.append(np.argmax(r2.detach().cpu().numpy()[0]))\n",
        "        pred_both.append(np.argmax(outputs.detach().cpu().numpy()[0]))\n",
        "\n",
        "print('*'*10)\n",
        "print(f1_score(act,pred_r1,average='macro'))\n",
        "print(f1_score(act,pred_r1,average='weighted'))\n",
        "print(accuracy_score(act,pred_r1))\n",
        "print('*'*10)\n",
        "print(f1_score(act,pred_r2,average='macro'))\n",
        "print(f1_score(act,pred_r2,average='weighted'))\n",
        "print(accuracy_score(act,pred_r2))\n",
        "print('*'*10)\n",
        "print(f1_score(act,pred_both,average='macro'))\n",
        "print(f1_score(act,pred_both,average='weighted'))\n",
        "print(accuracy_score(act,pred_both))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T14:00:26.891846Z",
          "iopub.execute_input": "2024-09-21T14:00:26.892136Z",
          "iopub.status.idle": "2024-09-21T14:00:42.179965Z",
          "shell.execute_reply.started": "2024-09-21T14:00:26.892111Z",
          "shell.execute_reply": "2024-09-21T14:00:42.179069Z"
        },
        "trusted": true,
        "id": "eZE8P7gUV9Pc",
        "outputId": "fc927250-5c30-48b6-9073-daa8e26e8030"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "**********\n0.6913720146309976\n0.7423707345273722\n0.753030303030303\n**********\n0.23963133640552994\n0.1510403574919704\n0.3151515151515151\n**********\n0.6793982390463974\n0.73381221836417\n0.746969696969697\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.save(vb.state_dict(), '/kaggle/working/final_model_wo_adversarial.pt')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T14:04:38.684318Z",
          "iopub.execute_input": "2024-09-21T14:04:38.684957Z",
          "iopub.status.idle": "2024-09-21T14:04:38.689270Z",
          "shell.execute_reply.started": "2024-09-21T14:04:38.684922Z",
          "shell.execute_reply": "2024-09-21T14:04:38.688249Z"
        },
        "trusted": true,
        "id": "ZFFchej2V9Pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "from typing import Dict\n",
        "\n",
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "\n",
        "\n",
        "# import svm_classifier\n",
        "\n",
        "REGRESSION = False\n",
        "from typing import List\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "def get_weights(model) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        :return: final weights of the model, as np array\n",
        "        \"\"\"\n",
        "\n",
        "        w = model.coef_\n",
        "        if len(w.shape) == 1:\n",
        "                w = np.expand_dims(w, 0)\n",
        "\n",
        "        return w\n",
        "\n",
        "\n",
        "def train_network(model, X_train: np.ndarray, Y_train: np.ndarray, X_dev: np.ndarray, Y_dev: np.ndarray) -> float:\n",
        "\n",
        "        \"\"\"\n",
        "        :param X_train:\n",
        "        :param Y_train:\n",
        "        :param X_dev:\n",
        "        :param Y_dev:\n",
        "        :return: accuracy score on the dev set / Person's R in the case of regression\n",
        "        \"\"\"\n",
        "\n",
        "        model.fit(X_train, Y_train)\n",
        "        score = model.score(X_dev, Y_dev)\n",
        "        return score\n",
        "\n",
        "\n",
        "from typing import Dict\n",
        "\n",
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "\n",
        "# import svm_classifier\n",
        "\n",
        "REGRESSION = False\n",
        "from typing import List\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "\n",
        "def get_nullspace_projection(W: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    :param W: the matrix over its nullspace to project\n",
        "    :return: the projection matrix\n",
        "    \"\"\"\n",
        "    nullspace_basis = scipy.linalg.null_space(W)  # orthogonal basis\n",
        "    nullspace_basis = nullspace_basis * np.sign(nullspace_basis[0][0])  # handle sign ambiguity\n",
        "    projection_matrix = nullspace_basis.dot(nullspace_basis.T)\n",
        "\n",
        "    return projection_matrix\n",
        "\n",
        "from sklearn import svm"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T14:04:39.129789Z",
          "iopub.execute_input": "2024-09-21T14:04:39.130639Z",
          "iopub.status.idle": "2024-09-21T14:04:39.140607Z",
          "shell.execute_reply.started": "2024-09-21T14:04:39.130605Z",
          "shell.execute_reply": "2024-09-21T14:04:39.139724Z"
        },
        "trusted": true,
        "id": "GKNRJYH4V9Pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#R\n",
        "num_concepts = 18\n",
        "BS = 8\n",
        "import torch\n",
        "from torch import nn\n",
        "from pytorch_revgrad import RevGrad\n",
        "class VB(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VB, self).__init__()\n",
        "        self.vb = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(768, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 2),\n",
        "        )\n",
        "\n",
        "\n",
        "        self.linear_relu_stack_latent = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(768, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 2),\n",
        "        )\n",
        "\n",
        "\n",
        "        self.linear_relu_stack_hidden = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(768, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 2),\n",
        "        )\n",
        "\n",
        "        #self.w = torch.nn.Parameter(torch.ones(11,1)/11)\n",
        "        self.w = torch.nn.Parameter(torch.ones(num_concepts,1))\n",
        "\n",
        "        self.project_latent = nn.Linear(768,2048)\n",
        "\n",
        "        #self.W_ij = [[nn.Linear(768,768) for i in range(64)] for j in range(17)]\n",
        "\n",
        "        self.W_ij = torch.nn.Parameter(torch.randn(18,64,28,28))\n",
        "\n",
        "        self.proj_conc = nn.Linear(768,28)\n",
        "        self.proj_embed = nn.Linear(768,28)\n",
        "\n",
        "\n",
        "\n",
        "        self.grad_rev1 = RevGrad()\n",
        "        self.grad_rev2 = RevGrad()\n",
        "\n",
        "        self.w1 = torch.nn.Parameter(torch.tensor(0.2))\n",
        "\n",
        "\n",
        "    def forward(self,\n",
        "        all_concepts_,\n",
        "        input_ids = None,\n",
        "        attention_mask = None,\n",
        "        token_type_ids = None,\n",
        "        position_ids = None,\n",
        "        head_mask = None,\n",
        "        inputs_embeds = None,\n",
        "        visual_embeds = None,\n",
        "        visual_attention_mask = None,\n",
        "        visual_token_type_ids = None,\n",
        "        image_text_alignment = None,\n",
        "        output_attentions = None,\n",
        "        output_hidden_states = None,\n",
        "        return_dict = None):\n",
        "\n",
        "        #         print(all_concepts.shape)\n",
        "\n",
        "        B = visual_embeds.size(0)\n",
        "\n",
        "        #         print(B)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ##############################################\n",
        "\n",
        "        all_concepts = self.proj_conc(all_concepts_)\n",
        "\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs_embeds = self.vb.embeddings.word_embeddings(input_ids)\n",
        "\n",
        "        #print(0/0)\n",
        "\n",
        "        inputs_embeds = self.proj_embed(inputs_embeds)\n",
        "\n",
        "\n",
        "        #####################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #b1 = torch.bmm(inputs_embeds,all_concepts.repeat(B,1,1).transpose(1,2))\n",
        "\n",
        "        #print(inputs_embeds.shape)\n",
        "        #print(all_concepts.repeat(B,1,1).transpose(1,2).shape)\n",
        "\n",
        "        #b,64,28\n",
        "        #b,28,17\n",
        "\n",
        "\n",
        "        b = torch.einsum('blx,bxn->bln',inputs_embeds,all_concepts.repeat(B,1,1).transpose(1,2))\n",
        "\n",
        "        #         print(b1)\n",
        "\n",
        "        #         print('********')\n",
        "\n",
        "        #         print(b2)\n",
        "\n",
        "        #         print(torch.equal(b1,b2))\n",
        "\n",
        "\n",
        "\n",
        "        b = b.masked_fill(~attention_mask.unsqueeze(dim=2).bool(), -np.inf)\n",
        "        c = F.softmax(b,dim=1)\n",
        "\n",
        "\n",
        "\n",
        "        #inter = torch.einsum('bcsn,csnn->bcsn',inputs_embeds.unsqueeze(1).repeat(1,17,1,1), self.W_ij)\n",
        "\n",
        "        #print(inter[:,0,0,:])\n",
        "\n",
        "        einsum_expression = 'bln,clnx->bclx'\n",
        "        inter = torch.einsum(einsum_expression, inputs_embeds, self.W_ij)\n",
        "\n",
        "        k = torch.einsum('bcsn,bsc->bcn',inter,c)\n",
        "        norm = torch.norm(k,dim=2)\n",
        "\n",
        "\n",
        "\n",
        "        kk1 = ((norm**2)/(1+norm**2)).unsqueeze(dim=2) * (k/norm.unsqueeze(dim=2))\n",
        "\n",
        "        w_dynamic = kk1.norm(dim=2).unsqueeze(2)\n",
        "\n",
        "\n",
        "\n",
        "        raw_latent = (1-self.w1) * all_concepts_.unsqueeze(0).repeat(B,1,1) * self.w.unsqueeze(dim=0).repeat(B,1,1) + all_concepts_.unsqueeze(0).repeat(B,1,1) * w_dynamic * self.w1\n",
        "\n",
        "\n",
        "        #raw_latent = 0 * all_concepts_.unsqueeze(0).repeat(B,1,1) * self.w.unsqueeze(dim=0).repeat(B,1,1) + all_concepts_.unsqueeze(0).repeat(B,1,1) * w_dynamic *1\n",
        "\n",
        "\n",
        "        raw_latent = torch.mean(raw_latent, dim=1)\n",
        "\n",
        "        #raw_latent = torch.mean(all_concepts*self.w,dim=0)\n",
        "        #raw_latent = raw_latent.unsqueeze(dim=0)\n",
        "\n",
        "        latent = self.project_latent(raw_latent)\n",
        "\n",
        "\n",
        "\n",
        "        latent = latent.unsqueeze(dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        visual_embeds_latent = torch.cat((visual_embeds,latent),dim=1)\n",
        "\n",
        "\n",
        "        ################################################################\n",
        "\n",
        "\n",
        "        #print(b)\n",
        "\n",
        "        #######################################################\n",
        "        #         print(b.shape)\n",
        "\n",
        "        #         b = []\n",
        "        #         for i in range(64):\n",
        "        #             bi = []\n",
        "        #             for j in range(17):\n",
        "        #                 #print(inputs_embeds[:,i,:].shape)\n",
        "        #                 ci = all_concepts[j].unsqueeze(0).repeat(32,1,1).squeeze()\n",
        "        #                 uj = inputs_embeds[:,i,:]\n",
        "        #                 #print(ci.shape)\n",
        "        #                 bij = torch.bmm(uj.unsqueeze(dim=1), ci.unsqueeze(dim=2))\n",
        "        #                 #print(bij.shape)\n",
        "        #                 #print(bij)\n",
        "        #                 bi.append(bij.squeeze())\n",
        "        #             bi = torch.stack(bi)\n",
        "\n",
        "        #             bi = bi.T\n",
        "        #             #print(bi.shape)\n",
        "        #             #print(bi)\n",
        "        #             b.append(bi)\n",
        "\n",
        "\n",
        "\n",
        "        #         b = torch.stack(b)\n",
        "        #         b = b.transpose(0,1)\n",
        "\n",
        "        #         print(b)\n",
        "        #         print(b.shape)\n",
        "\n",
        "\n",
        "        #print(0/0)\n",
        "\n",
        "        ####################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #c = F.softmax(b,dim=1)\n",
        "\n",
        "        #print(c)\n",
        "        #print(c.shape)\n",
        "        #print(0/0)\n",
        "        ############################################################\n",
        "\n",
        "        #print\n",
        "\n",
        "        #c = 32*64*17\n",
        "\n",
        "        #u_j_i = 32*768\n",
        "\n",
        "\n",
        "        #cij = 32*1\n",
        "\n",
        "\n",
        "        #         kk = []\n",
        "        #         for i in range(17):\n",
        "        #             si = 0\n",
        "        #             for j in range(64):\n",
        "        #                 #print(inputs_embeds[:,j,:].shape)\n",
        "        #                 #print(self.W_ij)\n",
        "        #                 #print(self.W_ij[i,j,:,:].shape)\n",
        "\n",
        "\n",
        "\n",
        "        #                 u_j_i = torch.mm(inputs_embeds[:,j,:],self.W_ij[i,j,:,:])\n",
        "\n",
        "\n",
        "        #                 #print(j,i,u_j_i)\n",
        "        #                 #print(0/0)\n",
        "\n",
        "        #                 cij = c[:,j,i].unsqueeze(dim=1)\n",
        "        #                 #print('cij ',cij)\n",
        "        #                 #print(cij.shape)\n",
        "        #                 #print(u_j_i)\n",
        "        #                 #print(u_j_i.shape)\n",
        "        #                 si+= u_j_i*cij\n",
        "\n",
        "        #                 #print(u_j_i*cij)\n",
        "        #             #print(si)\n",
        "        #             #print(0/0)\n",
        "\n",
        "        #             norm = torch.norm(si, dim=1)\n",
        "\n",
        "        #             #print(norm)\n",
        "        #             #print(norm.shape)\n",
        "\n",
        "        #             vi = ((norm**2)/(1+norm**2)).unsqueeze(dim=1) * (si/norm.unsqueeze(dim=1))\n",
        "\n",
        "        #             #print(vi)\n",
        "        #             kk.append(torch.norm(vi,dim=1))\n",
        "        #             #kk.append(vi)\n",
        "        #             #print(kk)\n",
        "\n",
        "\n",
        "        #         kk = torch.stack(kk)\n",
        "\n",
        "        #         print('kk', kk)\n",
        "        #         print('kk1', kk1)\n",
        "        #         print(kk.shape)\n",
        "        #         print(kk1.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Create a tensor of ones with the same number of columns as the original tensor\n",
        "        ones_column_vam = torch.ones((visual_attention_mask.size(0), 1), dtype=visual_attention_mask.dtype)\n",
        "\n",
        "        # Concatenate the original tensor with the tensor of ones along the second dimension (columns)\n",
        "        visual_attention_mask_latent = torch.cat((visual_attention_mask, ones_column_vam.to('cuda')), dim=1)\n",
        "\n",
        "        # Create a tensor of ones with the same number of columns as the original tensor\n",
        "        ones_column_vtt = torch.ones((visual_token_type_ids.size(0), 1), dtype=visual_token_type_ids.dtype)\n",
        "\n",
        "        # Concatenate the original tensor with the tensor of ones along the second dimension (columns)\n",
        "        visual_token_type_ids_latent = torch.cat((visual_token_type_ids, ones_column_vtt.to('cuda')), dim=1)\n",
        "\n",
        "        #         print(latent.shape)\n",
        "\n",
        "        #         print(visual_embeds.shape)\n",
        "        #         print(visual_attention_mask.shape)\n",
        "        #         print(visual_token_type_ids.shape)\n",
        "\n",
        "        #         print(self.w)\n",
        "\n",
        "        x = self.vb(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask, visual_embeds=visual_embeds_latent, visual_attention_mask=visual_attention_mask_latent, visual_token_type_ids=visual_token_type_ids_latent)\n",
        "\n",
        "        p = self.vb(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask, visual_embeds=visual_embeds, visual_attention_mask=visual_attention_mask, visual_token_type_ids=visual_token_type_ids)\n",
        "        p = self.grad_rev1(p.pooler_output)\n",
        "        #q = self.grad_rev2(raw_latent.repeat(B,1))\n",
        "        q = self.grad_rev2(raw_latent)\n",
        "\n",
        "        logits_p = self.linear_relu_stack_hidden(p)\n",
        "        logits_q = self.linear_relu_stack_latent(q)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        logits = self.linear_relu_stack(x.pooler_output)\n",
        "\n",
        "\n",
        "        if_logits_p = self.linear_relu_stack(p)\n",
        "        if_logits_q = self.linear_relu_stack(q)\n",
        "\n",
        "        return logits, logits_p, logits_q, if_logits_p, if_logits_q\n",
        "        #return logits, logits_p, logits_q, if_logits_p, if_logits_p\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T14:04:41.548529Z",
          "iopub.execute_input": "2024-09-21T14:04:41.548887Z",
          "iopub.status.idle": "2024-09-21T14:04:41.578636Z",
          "shell.execute_reply.started": "2024-09-21T14:04:41.548860Z",
          "shell.execute_reply": "2024-09-21T14:04:41.577519Z"
        },
        "trusted": true,
        "id": "9eKSTEftV9Pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "def get_inlp(all_concepts):\n",
        "    print('INLP step')\n",
        "    num_concepts = 18\n",
        "    all_concepts_ = all_concepts.numpy()\n",
        "    multiplier = np.logical_xor(1, np.eye(num_concepts)).astype(np.int32)/ (num_concepts-1)\n",
        "    X = np.matmul(multiplier,all_concepts_)\n",
        "    assert X.shape[0]==num_concepts\n",
        "    Y = np.arange(num_concepts)\n",
        "\n",
        "    #             x_train_cp = x_train\n",
        "    #             x_test_cp = x_test\n",
        "    x_train_cp = X\n",
        "    x_test_cp = X\n",
        "    y_train = Y\n",
        "    y_test = Y\n",
        "    print('******')\n",
        "    for i in range(1):\n",
        "\n",
        "        clf = svm.SVC(kernel=\"linear\")\n",
        "        scr = train_network(clf, x_train_cp, y_train, x_test_cp, y_test)\n",
        "        print('initial {}'.format(scr))\n",
        "        W = get_weights(clf)\n",
        "        P_i = get_nullspace_projection(W)\n",
        "        x_train_cp = x_train_cp.dot(P_i)\n",
        "        x_test_cp = x_test_cp.dot(P_i)\n",
        "        print(clf.score(x_test_cp, y_test))\n",
        "        print(clf.score(x_train_cp, y_train))\n",
        "        clf = svm.SVC(kernel=\"linear\")\n",
        "        train_network(clf, x_train_cp, y_train, x_test_cp, y_test)\n",
        "    print('******')\n",
        "    all_concepts_ = torch.tensor(all_concepts_)\n",
        "    all_concepts_ = all_concepts_.mm(torch.tensor(P_i, dtype = all_concepts_.dtype))\n",
        "\n",
        "    return all_concepts_"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T14:04:43.547477Z",
          "iopub.execute_input": "2024-09-21T14:04:43.548230Z",
          "iopub.status.idle": "2024-09-21T14:04:43.558489Z",
          "shell.execute_reply.started": "2024-09-21T14:04:43.548198Z",
          "shell.execute_reply": "2024-09-21T14:04:43.557521Z"
        },
        "trusted": true,
        "id": "hEMEjYcUV9Pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "import torch\n",
        "from torch import nn\n",
        "from pytorch_revgrad import RevGrad\n",
        "class VB_wrapper(nn.Module):\n",
        "    def __init__(self, vb):\n",
        "        super(VB_wrapper, self).__init__()\n",
        "\n",
        "        self.vb = vb\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,\n",
        "        all_concepts_,\n",
        "        input_embeds = None,\n",
        "        visual_embeds = None,\n",
        "        attention_mask = None,\n",
        "        token_type_ids = None,\n",
        "        position_ids = None,\n",
        "        head_mask = None,\n",
        "        inputs_embeds = None,\n",
        "\n",
        "        visual_attention_mask = None,\n",
        "        visual_token_type_ids = None,\n",
        "        image_text_alignment = None,\n",
        "        output_attentions = None,\n",
        "        output_hidden_states = None,\n",
        "        return_dict = None):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #         print('ac', all_concepts_.shape)\n",
        "        #         print('ii', input_embeds.shape)\n",
        "        #         print('am', attention_mask)\n",
        "        #         print('tti', token_type_ids)\n",
        "        #         print('pi', position_ids)\n",
        "        #         print('hm', head_mask)\n",
        "        #         print('ie', inputs_embeds)\n",
        "        #         print('ve', visual_embeds)\n",
        "        #         print('vam', visual_attention_mask)\n",
        "        #         print('vtti', visual_token_type_ids)\n",
        "        #         print(image_text_alignment)\n",
        "        #         print(output_attentions)\n",
        "        #         print(output_hidden_states)\n",
        "        #         print(return_dict)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        B = visual_embeds.size(0)\n",
        "\n",
        "\n",
        "        all_concepts = self.vb.proj_conc(all_concepts_)\n",
        "\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            inputs_embeds = self.vb.vb.embeddings.word_embeddings(input_ids.to('cuda'))\n",
        "        \"\"\"\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs_embeds = self.vb.vb.embeddings.word_embeddings(input_ids.to('cuda'))\n",
        "\n",
        "        #print(0/0)\n",
        "\n",
        "        inputs_embeds = self.vb.proj_embed(inputs_embeds)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        b = torch.einsum('blx,bxn->bln',inputs_embeds,all_concepts.transpose(1,2))\n",
        "\n",
        "\n",
        "        #print(b.shape)\n",
        "\n",
        "\n",
        "\n",
        "        b = b.masked_fill(~attention_mask.unsqueeze(dim=2).bool(), -np.inf)\n",
        "        c = F.softmax(b,dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        einsum_expression = 'bln,clnx->bclx'\n",
        "        inter = torch.einsum(einsum_expression, inputs_embeds, self.vb.W_ij)\n",
        "\n",
        "        k = torch.einsum('bcsn,bsc->bcn',inter,c)\n",
        "        norm = torch.norm(k,dim=2)\n",
        "\n",
        "\n",
        "\n",
        "        kk1 = ((norm**2)/(1+norm**2)).unsqueeze(dim=2) * (k/norm.unsqueeze(dim=2))\n",
        "\n",
        "        w_dynamic = kk1.norm(dim=2).unsqueeze(2)\n",
        "\n",
        "        num_reps = int(all_concepts.shape[0])\n",
        "        w_static = self.vb.w.unsqueeze(0).repeat(num_reps,1,1)\n",
        "\n",
        "\n",
        "        #         print('*'*10)\n",
        "\n",
        "        #         print(self.vb.w1.shape)\n",
        "        #         print(all_concepts_.shape)\n",
        "        #         print(w_static.shape)\n",
        "        #         print(w_dynamic.shape)\n",
        "\n",
        "\n",
        "\n",
        "        if DYN_ROUTE:\n",
        "            raw_latent = (1-self.vb.w1) * all_concepts_ * w_static + all_concepts_ * w_dynamic * self.vb.w1\n",
        "\n",
        "        else:\n",
        "            raw_latent = (1-self.vb.w1) * all_concepts_ * w_static\n",
        "\n",
        "\n",
        "\n",
        "        #raw_latent = (1-self.vb.w1) * all_concepts_ * w_static\n",
        "        raw_latent = torch.mean(raw_latent, dim=1)\n",
        "\n",
        "\n",
        "        latent = self.vb.project_latent(raw_latent)\n",
        "\n",
        "\n",
        "\n",
        "        latent = latent.unsqueeze(dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        visual_embeds_latent = torch.cat((visual_embeds,latent),dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #         num_reps = int(all_concepts.shape[0]/17)\n",
        "        #         w_static = self.vb.w.unsqueeze(0).repeat(num_reps,1,1).reshape(17*num_reps,1)\n",
        "        #         raw_latent = torch.mean(all_concepts*w_static,dim=0)\n",
        "        #         raw_latent = raw_latent.unsqueeze(dim=0)\n",
        "\n",
        "        #         latent = self.vb.project_latent(raw_latent)\n",
        "\n",
        "\n",
        "\n",
        "        #         latent = latent.repeat(B,1).unsqueeze(dim=1)\n",
        "\n",
        "\n",
        "        #         visual_embeds_latent = torch.cat((visual_embeds,latent),dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Create a tensor of ones with the same number of columns as the original tensor\n",
        "        ones_column_vam = torch.ones((visual_attention_mask.size(0), 1), dtype=visual_attention_mask.dtype)\n",
        "\n",
        "        # Concatenate the original tensor with the tensor of ones along the second dimension (columns)\n",
        "        visual_attention_mask_latent = torch.cat((visual_attention_mask, ones_column_vam.to('cuda')), dim=1)\n",
        "\n",
        "        # Create a tensor of ones with the same number of columns as the original tensor\n",
        "        ones_column_vtt = torch.ones((visual_token_type_ids.size(0), 1), dtype=visual_token_type_ids.dtype)\n",
        "\n",
        "        # Concatenate the original tensor with the tensor of ones along the second dimension (columns)\n",
        "        visual_token_type_ids_latent = torch.cat((visual_token_type_ids, ones_column_vtt.to('cuda')), dim=1)\n",
        "\n",
        "        #         print(latent.shape)\n",
        "\n",
        "        #         print(visual_embeds.shape)\n",
        "        #         print(visual_attention_mask.shape)\n",
        "        #         print(visual_token_type_ids.shape)\n",
        "\n",
        "        #         print(self.w)\n",
        "\n",
        "        x = self.vb.vb(inputs_embeds = input_embeds, token_type_ids = token_type_ids, attention_mask = attention_mask, visual_embeds=visual_embeds_latent, visual_attention_mask=visual_attention_mask_latent, visual_token_type_ids=visual_token_type_ids_latent)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        logits = self.vb.linear_relu_stack(x.pooler_output)\n",
        "        #print('softmax logit', torch.softmax(logits, dim=1))\n",
        "        #print(0/0)\n",
        "\n",
        "        #logits = torch.softmax(logits, dim=1)[:, 1].unsqueeze(1)\n",
        "\n",
        "        #print('previous',torch.softmax(logits, dim=1)[:, 1].unsqueeze(1) )\n",
        "\n",
        "        logits = torch.softmax(logits, dim=1).max(dim=1).values.unsqueeze(1)\n",
        "\n",
        "        #print('logits', logits)\n",
        "\n",
        "        #print(0/0)\n",
        "\n",
        "        #print(logits)\n",
        "        #print(logits.shape)\n",
        "\n",
        "\n",
        "\n",
        "        return logits\n",
        "\n",
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T14:04:44.140633Z",
          "iopub.execute_input": "2024-09-21T14:04:44.140973Z",
          "iopub.status.idle": "2024-09-21T14:04:44.162711Z",
          "shell.execute_reply.started": "2024-09-21T14:04:44.140946Z",
          "shell.execute_reply": "2024-09-21T14:04:44.161518Z"
        },
        "trusted": true,
        "id": "B_YFFcdjV9Pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "#'/kaggle/working/final_model_concept.pt'\n",
        "\n",
        "vb = VB()\n",
        "\n",
        "vb.load_state_dict(torch.load('/kaggle/working/final_model_wo_adversarial.pt'))\n",
        "vb.to('cuda')\n",
        "vb.eval()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T14:04:44.681223Z",
          "iopub.execute_input": "2024-09-21T14:04:44.681999Z",
          "iopub.status.idle": "2024-09-21T14:04:45.767379Z",
          "shell.execute_reply.started": "2024-09-21T14:04:44.681957Z",
          "shell.execute_reply": "2024-09-21T14:04:45.766428Z"
        },
        "trusted": true,
        "id": "Z9OiCLAvV9Pf",
        "outputId": "5a2fd9d1-ca98-4469-ba20-ca942a9997b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 54,
          "output_type": "execute_result",
          "data": {
            "text/plain": "VB(\n  (vb): VisualBertModel(\n    (embeddings): VisualBertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=1)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n      (visual_token_type_embeddings): Embedding(2, 768)\n      (visual_position_embeddings): Embedding(512, 768)\n      (visual_projection): Linear(in_features=2048, out_features=768, bias=True)\n    )\n    (encoder): VisualBertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x VisualBertLayer(\n          (attention): VisualBertAttention(\n            (self): VisualBertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): VisualBertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): VisualBertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): VisualBertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): VisualBertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (linear_relu_stack): Sequential(\n    (0): Dropout(p=0.1, inplace=False)\n    (1): Linear(in_features=768, out_features=512, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=512, out_features=128, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=128, out_features=2, bias=True)\n  )\n  (linear_relu_stack_latent): Sequential(\n    (0): Dropout(p=0.1, inplace=False)\n    (1): Linear(in_features=768, out_features=512, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=512, out_features=128, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=128, out_features=2, bias=True)\n  )\n  (linear_relu_stack_hidden): Sequential(\n    (0): Dropout(p=0.1, inplace=False)\n    (1): Linear(in_features=768, out_features=512, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=512, out_features=128, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=128, out_features=2, bias=True)\n  )\n  (project_latent): Linear(in_features=768, out_features=2048, bias=True)\n  (proj_conc): Linear(in_features=768, out_features=28, bias=True)\n  (proj_embed): Linear(in_features=768, out_features=28, bias=True)\n  (grad_rev1): RevGrad()\n  (grad_rev2): RevGrad()\n)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "vb_c = VB()\n",
        "vb_c.to(device)\n",
        "# concept_classes = [\"jew\", \"holocaust\", \"hitler\", \"genocide\", \"funny\", \"muslim\", \"terrorism\", \"violence\", \"politics\", \"naive\", \"international relation\"]\n",
        "\n",
        "\n",
        "# concept_embedding_map = {}\n",
        "\n",
        "# all_concepts = []\n",
        "\n",
        "\n",
        "# for c in concept_classes:\n",
        "#     conc = torch.tensor(tokenizer.encode(c)[1:-1])\n",
        "#     e = vb_c.vb.embeddings.word_embeddings(conc.to('cuda'))\n",
        "#     x = e.detach().cpu().mean(dim=0)\n",
        "\n",
        "#     all_concepts.append(x)\n",
        "\n",
        "#     concept_embedding_map[c] = x\n",
        "\n",
        "\n",
        "# all_concepts = torch.stack(all_concepts)\n",
        "\n",
        "# concept_classes = [\"jew\", \"holocaust\", \"hitler\", \"genocide\", \"funny\", \"muslim\", \"terrorism\", \"violence\", \"politics\", \"racism\", \"international relation\", \"sex\", \"gore\", \"violence\", \"immigration\", \"extremism\", \"immoral\"]\n",
        "\n",
        "concept_classes = [\"holocaust\", \"nazism\", \"genocide\", \"funny\", \"anti muslim\", \"terrorism\", \"violence\", \"politics\", \"racism\", \"international relation\", \"adult\", \"gore\", \"misogynistic\", \"immigration\", \"extremism\", \"immoral\", \"white supremacy\", \"indecency\"]\n",
        "\n",
        "\n",
        "concept_embedding_map = {}\n",
        "\n",
        "all_concepts = []\n",
        "\n",
        "\n",
        "for c in concept_classes:\n",
        "    conc = torch.tensor(tokenizer.encode(c)[1:-1])\n",
        "    e = vb_c.vb.embeddings.word_embeddings(conc.to('cuda'))\n",
        "    x = e.detach().cpu().mean(dim=0)\n",
        "\n",
        "\n",
        "    all_concepts.append(x)\n",
        "\n",
        "    concept_embedding_map[c] = x\n",
        "\n",
        "\n",
        "all_concepts = torch.stack(all_concepts)\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T14:04:47.861173Z",
          "iopub.execute_input": "2024-09-21T14:04:47.861559Z",
          "iopub.status.idle": "2024-09-21T14:04:48.534273Z",
          "shell.execute_reply.started": "2024-09-21T14:04:47.861528Z",
          "shell.execute_reply": "2024-09-21T14:04:48.533487Z"
        },
        "trusted": true,
        "id": "kBIcR2CxV9Pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "def get_inlp(all_concepts):\n",
        "\n",
        "    num_concepts = 18\n",
        "    all_concepts_ = all_concepts.cpu().numpy()\n",
        "    multiplier = np.logical_xor(1, np.eye(num_concepts)).astype(np.int32)/ (num_concepts-1)\n",
        "    X = np.matmul(multiplier,all_concepts_)\n",
        "    assert X.shape[0]==num_concepts\n",
        "    Y = np.arange(num_concepts)\n",
        "\n",
        "    #             x_train_cp = x_train\n",
        "    #             x_test_cp = x_test\n",
        "    x_train_cp = X\n",
        "    x_test_cp = X\n",
        "    y_train = Y\n",
        "    y_test = Y\n",
        "\n",
        "    for i in range(1):\n",
        "\n",
        "        clf = svm.SVC(kernel=\"linear\")\n",
        "        scr = train_network(clf, x_train_cp, y_train, x_test_cp, y_test)\n",
        "\n",
        "        W = get_weights(clf)\n",
        "        P_i = get_nullspace_projection(W)\n",
        "        x_train_cp = x_train_cp.dot(P_i)\n",
        "        x_test_cp = x_test_cp.dot(P_i)\n",
        "\n",
        "        clf = svm.SVC(kernel=\"linear\")\n",
        "        train_network(clf, x_train_cp, y_train, x_test_cp, y_test)\n",
        "\n",
        "    all_concepts_ = torch.tensor(all_concepts_)\n",
        "    all_concepts_ = all_concepts_.mm(torch.tensor(P_i, dtype = all_concepts_.dtype))\n",
        "\n",
        "    return all_concepts_"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T14:04:49.441307Z",
          "iopub.execute_input": "2024-09-21T14:04:49.442034Z",
          "iopub.status.idle": "2024-09-21T14:04:49.450994Z",
          "shell.execute_reply.started": "2024-09-21T14:04:49.441999Z",
          "shell.execute_reply": "2024-09-21T14:04:49.449913Z"
        },
        "trusted": true,
        "id": "lmiUkYnhV9Pg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "#torch.save(vb.state_dict(), '/kaggle/working/final_model_concept_inlp.pt')\n",
        "# check evaluation performance separately on p and q using their classifier: should be less\n",
        "pred_r1, pred_r2 = [],[]\n",
        "pred_both = []\n",
        "act = []\n",
        "if DECONFOUND:\n",
        "    all_concepts_ = get_inlp(all_concepts).to('cuda')\n",
        "else:\n",
        "    all_concepts_ = all_concepts.to('cuda')\n",
        "\n",
        "for ii, batch in enumerate(eval_dataloader):\n",
        "    with torch.no_grad():\n",
        "        batch_inp = {k: v.to(device) for k, v in batch[0].items()}\n",
        "        outputs, r1, r2,_,_ = vb(all_concepts_.to('cuda'),**batch_inp)\n",
        "        act.append(batch[1].detach().cpu().numpy()[0])\n",
        "\n",
        "        pred_r1.append(np.argmax(r1.detach().cpu().numpy()[0]))\n",
        "        pred_r2.append(np.argmax(r2.detach().cpu().numpy()[0]))\n",
        "        pred_both.append(np.argmax(outputs.detach().cpu().numpy()[0]))\n",
        "\n",
        "print('*')\n",
        "print(f1_score(act,pred_r1,average='macro'))\n",
        "print(f1_score(act,pred_r1,average='weighted'))\n",
        "print(accuracy_score(act,pred_r1))\n",
        "print('*'*10)\n",
        "print(f1_score(act,pred_r2,average='macro'))\n",
        "print(f1_score(act,pred_r2,average='weighted'))\n",
        "print(accuracy_score(act,pred_r2))\n",
        "print('*'*10)\n",
        "print(f1_score(act,pred_both,average='macro'))\n",
        "print(f1_score(act,pred_both,average='weighted'))\n",
        "print(accuracy_score(act,pred_both))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T14:04:54.613208Z",
          "iopub.execute_input": "2024-09-21T14:04:54.613558Z",
          "iopub.status.idle": "2024-09-21T14:05:10.230038Z",
          "shell.execute_reply.started": "2024-09-21T14:04:54.613532Z",
          "shell.execute_reply": "2024-09-21T14:05:10.229170Z"
        },
        "trusted": true,
        "id": "BlfPlG8FV9Pg",
        "outputId": "9b5a5a31-3ba5-4e74-bf5d-e8e6c4d5b108"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "*\n0.23349522892932548\n0.1549232109361029\n0.2924242424242424\n**********\n0.23963133640552994\n0.1510403574919704\n0.3151515151515151\n**********\n0.6793982390463974\n0.73381221836417\n0.746969696969697\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('h')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T14:05:14.886373Z",
          "iopub.execute_input": "2024-09-21T14:05:14.887315Z",
          "iopub.status.idle": "2024-09-21T14:05:14.891863Z",
          "shell.execute_reply.started": "2024-09-21T14:05:14.887256Z",
          "shell.execute_reply": "2024-09-21T14:05:14.890801Z"
        },
        "trusted": true,
        "id": "93KLdnBFV9Pg",
        "outputId": "4f7ba6a0-f654-4cac-df82-e7d582e82360"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "h\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "def model_pred(input_ids, attention_mask, token_type_ids, visual_embeds, visual_attention_mask, visual_token_type_ids, all_concepts, concept_no=None):\n",
        "\n",
        "\n",
        "\n",
        "    B = visual_embeds.size(0)\n",
        "\n",
        "\n",
        "    #######################################################\n",
        "    input_ids = input_ids.to('cuda')\n",
        "    attention_mask = attention_mask.to('cuda')\n",
        "    token_type_ids = token_type_ids.to('cuda')\n",
        "    visual_embeds = visual_embeds.to('cuda')\n",
        "    visual_attention_mask = visual_attention_mask.to('cuda')\n",
        "    visual_token_type_ids = visual_token_type_ids.to('cuda')\n",
        "\n",
        "    #####################################################################\n",
        "\n",
        "    ##############################################\n",
        "\n",
        "    all_concepts = all_concepts.to('cuda')\n",
        "\n",
        "    # deconfounding step by passing the concepts through INLP (1)\n",
        "    if DECONFOUND:\n",
        "        all_concepts_ = get_inlp(all_concepts).to('cuda')\n",
        "    else:\n",
        "        all_concepts_ = all_concepts.to('cuda')\n",
        "\n",
        "    # this is without deconfounding while testing (2)\n",
        "    # either use (1) or (2) or mean of (1) and (2)\n",
        "\n",
        "    # all_concepts_ = all_concepts.to('cuda')\n",
        "\n",
        "    #all_concepts_ = (get_inlp(all_concepts).to('cuda') + all_concepts.to('cuda')) / 2\n",
        "\n",
        "    all_concepts = vb.proj_conc(all_concepts_)\n",
        "\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        inputs_embeds = vb.vb.embeddings.word_embeddings(input_ids.to('cuda'))\n",
        "\n",
        "    #print(0/0)\n",
        "\n",
        "    inputs_embeds = vb.proj_embed(inputs_embeds)\n",
        "\n",
        "\n",
        "    #####################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #B, 64, 28, #B, 28, 17\n",
        "\n",
        "    b = torch.einsum('blx,bxn->bln',inputs_embeds,all_concepts.repeat(B,1,1).transpose(1,2))\n",
        "\n",
        "\n",
        "\n",
        "    b = b.masked_fill(~attention_mask.unsqueeze(dim=2).bool(), -np.inf)\n",
        "    c = F.softmax(b,dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    einsum_expression = 'bln,clnx->bclx'\n",
        "    inter = torch.einsum(einsum_expression, inputs_embeds, vb.W_ij)\n",
        "\n",
        "    k = torch.einsum('bcsn,bsc->bcn',inter,c)\n",
        "    norm = torch.norm(k,dim=2)\n",
        "\n",
        "\n",
        "\n",
        "    kk1 = ((norm**2)/(1+norm**2)).unsqueeze(dim=2) * (k/norm.unsqueeze(dim=2))\n",
        "\n",
        "    w_dynamic = kk1.norm(dim=2).unsqueeze(2)\n",
        "\n",
        "    #     print(w_dynamic.shape)\n",
        "    #     print(vb.w.shape)\n",
        "\n",
        "\n",
        "    #     print(w_dynamic)\n",
        "    #     print(vb.w)\n",
        "    #     print('*'*10)\n",
        "\n",
        "\n",
        "    #print(concept_no)\n",
        "    if concept_no==None:\n",
        "        k_static = vb.w.data.clone()\n",
        "        k_dynamic = w_dynamic.data.clone()\n",
        "    else:\n",
        "        k_static = vb.w.data.clone()\n",
        "        k_dynamic = w_dynamic.data.clone()\n",
        "        k_static[concept_no][0] = 0\n",
        "        k_dynamic[0][concept_no][0] = 0\n",
        "\n",
        "    #     print(k_dynamic)\n",
        "    #     print(k_static)\n",
        "    #     print('*'*10)\n",
        "\n",
        "    #     print(0/0)\n",
        "\n",
        "    #either use the following line or raw_latent = (1-vb.w1) * all_concepts_.unsqueeze(0).repeat(B,1,1) * k_static.unsqueeze(dim=0).repeat(B,1,1)\n",
        "\n",
        "    if DYN_ROUTE:\n",
        "        raw_latent = (1-vb.w1) * all_concepts_.unsqueeze(0).repeat(B,1,1) * k_static.unsqueeze(dim=0).repeat(B,1,1) + all_concepts_.unsqueeze(0).repeat(B,1,1) * k_dynamic * vb.w1\n",
        "    else:\n",
        "        raw_latent = (1-vb.w1) * all_concepts_.unsqueeze(0).repeat(B,1,1) * k_static.unsqueeze(dim=0).repeat(B,1,1)\n",
        "\n",
        "    #raw_latent = 0.0 * all_concepts_.unsqueeze(0).repeat(B,1,1) * k_static.unsqueeze(dim=0).repeat(B,1,1) + all_concepts_.unsqueeze(0).repeat(B,1,1) * k_dynamic * 1.0\n",
        "\n",
        "    #raw_latent = (1-vb.w1) * all_concepts_.unsqueeze(0).repeat(B,1,1) * k_static.unsqueeze(dim=0).repeat(B,1,1)\n",
        "\n",
        "    raw_latent = torch.mean(raw_latent, dim=1)\n",
        "\n",
        "\n",
        "\n",
        "    latent = vb.project_latent(raw_latent)\n",
        "\n",
        "\n",
        "\n",
        "    latent = latent.unsqueeze(dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    visual_embeds_latent = torch.cat((visual_embeds,latent),dim=1)\n",
        "\n",
        "\n",
        "    ################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Create a tensor of ones with the same number of columns as the original tensor\n",
        "    ones_column_vam = torch.ones((visual_attention_mask.size(0), 1), dtype=visual_attention_mask.dtype)\n",
        "\n",
        "    # Concatenate the original tensor with the tensor of ones along the second dimension (columns)\n",
        "    visual_attention_mask_latent = torch.cat((visual_attention_mask.to('cuda'), ones_column_vam.to('cuda')), dim=1)\n",
        "\n",
        "    # Create a tensor of ones with the same number of columns as the original tensor\n",
        "    ones_column_vtt = torch.ones((visual_token_type_ids.size(0), 1), dtype=visual_token_type_ids.dtype)\n",
        "\n",
        "    # Concatenate the original tensor with the tensor of ones along the second dimension (columns)\n",
        "    visual_token_type_ids_latent = torch.cat((visual_token_type_ids.to('cuda'), ones_column_vtt.to('cuda')), dim=1)\n",
        "\n",
        "\n",
        "\n",
        "    x = vb.vb(input_ids = input_ids.to('cuda'), token_type_ids = token_type_ids.to('cuda'), attention_mask = attention_mask.to('cuda'), visual_embeds=visual_embeds_latent.to('cuda'), visual_attention_mask=visual_attention_mask_latent.to('cuda'), visual_token_type_ids=visual_token_type_ids_latent.to('cuda'))\n",
        "\n",
        "    p = vb.vb(input_ids = input_ids.to('cuda'), token_type_ids = token_type_ids.to('cuda'), attention_mask = attention_mask.to('cuda'), visual_embeds=visual_embeds.to('cuda'), visual_attention_mask=visual_attention_mask.to('cuda'), visual_token_type_ids=visual_token_type_ids.to('cuda'))\n",
        "    p = vb.grad_rev1(p.pooler_output)\n",
        "    q = vb.grad_rev2(raw_latent.repeat(B,1))\n",
        "\n",
        "    logits_p = vb.linear_relu_stack_hidden(p)\n",
        "    logits_q = vb.linear_relu_stack_latent(q.to('cuda'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    logits = vb.linear_relu_stack(x.pooler_output)\n",
        "\n",
        "\n",
        "    if_logits_p = vb.linear_relu_stack(p)\n",
        "    if_logits_q = vb.linear_relu_stack(q.to('cuda'))\n",
        "\n",
        "    return torch.softmax(logits,dim=-1), x.pooler_output"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T14:05:29.109047Z",
          "iopub.execute_input": "2024-09-21T14:05:29.109426Z",
          "iopub.status.idle": "2024-09-21T14:05:29.133645Z",
          "shell.execute_reply.started": "2024-09-21T14:05:29.109393Z",
          "shell.execute_reply": "2024-09-21T14:05:29.132424Z"
        },
        "trusted": true,
        "id": "CO2z1P_MV9Ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T14:05:30.283209Z",
          "iopub.execute_input": "2024-09-21T14:05:30.283916Z",
          "iopub.status.idle": "2024-09-21T14:05:30.288251Z",
          "shell.execute_reply.started": "2024-09-21T14:05:30.283884Z",
          "shell.execute_reply": "2024-09-21T14:05:30.287186Z"
        },
        "trusted": true,
        "id": "TIt1ETp5V9Ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(50):\n",
        "    if train_set[i]['label']==1:\n",
        "        print(i, train_set[i])\n",
        "        Image.open('/kaggle/input/facebook-hateful-meme-dataset/data/'+train_set[i]['img'])\n",
        "        inputs = {}\n",
        "        inputs = tokenizer(train_set[i]['text'], padding=\"max_length\", truncation=True, max_length=64, return_tensors='pt')\n",
        "\n",
        "\n",
        "        inputs['input_ids'] = inputs['input_ids']\n",
        "        inputs['token_type_ids'] = inputs['token_type_ids']\n",
        "        inputs['attention_mask'] = inputs['attention_mask']\n",
        "\n",
        "        input_ids = inputs['input_ids']\n",
        "        token_type_ids = inputs['token_type_ids']\n",
        "        attention_mask = inputs['attention_mask']\n",
        "\n",
        "        visual_embeds,visual_token_type_ids,visual_attention_mask = get_visual_embedding('/kaggle/input/facebook-hateful-meme-dataset/data/'+train_set[i]['img'])\n",
        "        # visual_embeds,visual_token_type_ids,visual_attention_mask = visual_feats[train_set[874]['img']]\n",
        "\n",
        "\n",
        "\n",
        "        inputs.update({\n",
        "          \"visual_embeds\":  visual_embeds,\n",
        "          \"visual_token_type_ids\": visual_token_type_ids,\n",
        "          \"visual_attention_mask\": visual_attention_mask\n",
        "        })\n",
        "\n",
        "\n",
        "        class_map = {0:'non offensive', 1: 'offensive'}\n",
        "        #print('original model prediction {}'.format(model_pred(input_ids,attention_mask,token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids)))\n",
        "        orig,_ = model_pred(input_ids,attention_mask,token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids,all_concepts)\n",
        "        orig = orig[0]\n",
        "        print(orig)\n",
        "        print('predicted class {}'.format(class_map[np.argmax(orig.detach().cpu().numpy())]))\n",
        "        pred_class = np.argmax(orig.detach().cpu().numpy())\n",
        "        orig = orig[pred_class]\n",
        "\n",
        "        k = []\n",
        "        for i in range(18):\n",
        "            #print('Removing concept: {}'.format(concept_classes[i]))\n",
        "\n",
        "            after_intervention,_ = model_pred(input_ids,attention_mask,token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids,all_concepts,concept_no=i)\n",
        "            #print('hello')\n",
        "            after_intervention = after_intervention[0][pred_class]\n",
        "            #print(after_intervention, orig)\n",
        "            #print(0/0)\n",
        "            k.append(abs(after_intervention-orig).item())\n",
        "            #print('model prediction after removal {}'.format(model_pred(input_ids,attention_mask,token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids,concept_no=i)))\n",
        "        k = np.asarray(k)\n",
        "        print(k)\n",
        "        k = list(map(lambda x: ((x-np.mean(k))/np.mean(k))*100, k))\n",
        "\n",
        "        conc_scr = []\n",
        "        for i in range(17):\n",
        "            cc = concept_classes[i]\n",
        "            print('relative ICaCE scores for concept {} is {}%'.format(cc,k[i]))\n",
        "            conc_scr.append((cc,k[i]))\n",
        "        conc_scr.sort(key=lambda tup: tup[1], reverse=True)\n",
        "\n",
        "        for i in conc_scr:\n",
        "            if i[1]> 0:\n",
        "                print(i[0], end = \" \")\n",
        "        print(\"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T14:05:30.827616Z",
          "iopub.execute_input": "2024-09-21T14:05:30.827983Z",
          "iopub.status.idle": "2024-09-21T14:05:44.095850Z",
          "shell.execute_reply.started": "2024-09-21T14:05:30.827954Z",
          "shell.execute_reply": "2024-09-21T14:05:44.094358Z"
        },
        "trusted": true,
        "id": "hrrsMvhOV9Ph",
        "outputId": "31321eca-da2e-4784-f8e7-e2e963da9cb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "10 {'text': 'jew mad? get fuhrerious!', 'img': 'img/79351.png', 'label': 1}\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:3526.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "tensor([0.4120, 0.5880], device='cuda:0', grad_fn=<SelectBackward0>)\npredicted class offensive\n[0.00223088 0.00223607 0.00224173 0.00224119 0.00223291 0.00222516\n 0.00224775 0.00224119 0.00224531 0.00225747 0.00225258 0.0022521\n 0.00224489 0.00225341 0.00223196 0.00226617 0.00225627 0.00221962]\nrelative ICaCE scores for concept holocaust is -0.5467901176248373%\nrelative ICaCE scores for concept nazism is -0.315614814114988%\nrelative ICaCE scores for concept genocide is -0.06318201143181926%\nrelative ICaCE scores for concept funny is -0.08709669800180365%\nrelative ICaCE scores for concept anti muslim is -0.4564457461382295%\nrelative ICaCE scores for concept terrorism is -0.801880107704671%\nrelative ICaCE scores for concept violence is 0.2051939156313391%\nrelative ICaCE scores for concept politics is -0.08709669800180365%\nrelative ICaCE scores for concept racism is 0.09624923236807681%\nrelative ICaCE scores for concept international relation is 0.6383154612877234%\nrelative ICaCE scores for concept adult is 0.42042609476119874%\nrelative ICaCE scores for concept gore is 0.39916859558787926%\nrelative ICaCE scores for concept misogynistic is 0.07764892059142227%\nrelative ICaCE scores for concept immigration is 0.4576267183145079%\nrelative ICaCE scores for concept extremism is -0.4989607444848685%\nrelative ICaCE scores for concept immoral is 1.0262648212008039%\nrelative ICaCE scores for concept white supremacy is 0.5851717133544247%\nimmoral international relation white supremacy immigration adult gore violence racism misogynistic \n\n12 {'text': 'brother... a day without a blast is a day wasted', 'img': 'img/25489.png', 'label': 1}\ntensor([0.8952, 0.1048], device='cuda:0', grad_fn=<SelectBackward0>)\npredicted class non offensive\n[0.00021398 0.00021333 0.00021392 0.00021362 0.00021374 0.0002138\n 0.00021476 0.00021392 0.00021422 0.00021422 0.0002138  0.0002141\n 0.00021392 0.0002135  0.00021362 0.0002147  0.00021333 0.00021493]\nrelative ICaCE scores for concept holocaust is 0.006190417234120171%\nrelative ICaCE scores for concept nazism is -0.30023523585489803%\nrelative ICaCE scores for concept genocide is -0.021666460319426935%\nrelative ICaCE scores for concept funny is -0.16095084808716248%\nrelative ICaCE scores for concept anti muslim is -0.10523709298006825%\nrelative ICaCE scores for concept terrorism is -0.07738021542652115%\nrelative ICaCE scores for concept violence is 0.36832982543023257%\nrelative ICaCE scores for concept politics is -0.021666460319426935%\nrelative ICaCE scores for concept racism is 0.11761792744830861%\nrelative ICaCE scores for concept international relation is 0.11761792744830861%\nrelative ICaCE scores for concept adult is -0.07738021542652115%\nrelative ICaCE scores for concept gore is 0.061904172341214384%\nrelative ICaCE scores for concept misogynistic is -0.021666460319426935%\nrelative ICaCE scores for concept immigration is -0.2166646031942567%\nrelative ICaCE scores for concept extremism is -0.16095084808716248%\nrelative ICaCE scores for concept immoral is 0.34047294787668547%\nrelative ICaCE scores for concept white supremacy is -0.30023523585489803%\nviolence immoral racism international relation gore holocaust \n\n27 {'text': \"is bribing muslims for liberal votes justin trudeau's only skill? why does justin trudeau love foreigners so much while openly disrespecting canadians, canadian values, our history and traditions, our seniors and veterans? sharia law has no place in canada! never has... never will.\", 'img': 'img/72640.png', 'label': 1}\ntensor([0.2539, 0.7461], device='cuda:0', grad_fn=<SelectBackward0>)\npredicted class offensive\n[3.09944153e-06 3.09944153e-06 3.15904617e-06 3.09944153e-06\n 3.09944153e-06 3.09944153e-06 3.09944153e-06 3.09944153e-06\n 3.09944153e-06 3.09944153e-06 3.09944153e-06 3.09944153e-06\n 3.15904617e-06 3.09944153e-06 3.09944153e-06 3.09944153e-06\n 3.09944153e-06 3.09944153e-06]\nrelative ICaCE scores for concept holocaust is -0.2132196162046969%\nrelative ICaCE scores for concept nazism is -0.2132196162046969%\nrelative ICaCE scores for concept genocide is 1.7057569296375203%\nrelative ICaCE scores for concept funny is -0.2132196162046969%\nrelative ICaCE scores for concept anti muslim is -0.2132196162046969%\nrelative ICaCE scores for concept terrorism is -0.2132196162046969%\nrelative ICaCE scores for concept violence is -0.2132196162046969%\nrelative ICaCE scores for concept politics is -0.2132196162046969%\nrelative ICaCE scores for concept racism is -0.2132196162046969%\nrelative ICaCE scores for concept international relation is -0.2132196162046969%\nrelative ICaCE scores for concept adult is -0.2132196162046969%\nrelative ICaCE scores for concept gore is -0.2132196162046969%\nrelative ICaCE scores for concept misogynistic is 1.7057569296375203%\nrelative ICaCE scores for concept immigration is -0.2132196162046969%\nrelative ICaCE scores for concept extremism is -0.2132196162046969%\nrelative ICaCE scores for concept immoral is -0.2132196162046969%\nrelative ICaCE scores for concept white supremacy is -0.2132196162046969%\ngenocide misogynistic \n\n30 {'text': \"d.j. osama spin laden droppin' beats like the twin towers\", 'img': 'img/93547.png', 'label': 1}\ntensor([0.3520, 0.6480], device='cuda:0', grad_fn=<SelectBackward0>)\npredicted class offensive\n[0.00067502 0.00067383 0.00067616 0.00067848 0.00067359 0.0006727\n 0.00067407 0.00067383 0.00067496 0.00067455 0.0006727  0.00067484\n 0.00067413 0.00067598 0.00067425 0.00067455 0.00067276 0.00067359]\nrelative ICaCE scores for concept holocaust is 0.08592119798699266%\nrelative ICaCE scores for concept nazism is -0.09083098072909918%\nrelative ICaCE scores for concept genocide is 0.2538357677672799%\nrelative ICaCE scores for concept funny is 0.598502516263659%\nrelative ICaCE scores for concept anti muslim is -0.12618141647231754%\nrelative ICaCE scores for concept terrorism is -0.2587455505093864%\nrelative ICaCE scores for concept violence is -0.05548054498588081%\nrelative ICaCE scores for concept politics is -0.09083098072909918%\nrelative ICaCE scores for concept racism is 0.07708358905118806%\nrelative ICaCE scores for concept international relation is 0.015220326500555922%\nrelative ICaCE scores for concept adult is -0.2587455505093864%\nrelative ICaCE scores for concept gore is 0.05940837117957888%\nrelative ICaCE scores for concept misogynistic is -0.04664293605007622%\nrelative ICaCE scores for concept immigration is 0.22732294095986608%\nrelative ICaCE scores for concept extremism is -0.028967718178467033%\nrelative ICaCE scores for concept immoral is 0.015220326500555922%\nrelative ICaCE scores for concept white supremacy is -0.24990794157358182%\nfunny genocide immigration holocaust racism gore international relation immoral \n\n48 {'text': 'we said we would never forget why are you voting them into our government?', 'img': 'img/74386.png', 'label': 1}\ntensor([0.7578, 0.2422], device='cuda:0', grad_fn=<SelectBackward0>)\npredicted class non offensive\n[0.00019342 0.00019312 0.00019324 0.00019366 0.00019336 0.0001933\n 0.00019407 0.00019366 0.00019324 0.00019282 0.00019288 0.00019324\n 0.00019342 0.00019383 0.00019318 0.0001933  0.00019324 0.00019312]\nrelative ICaCE scores for concept holocaust is 0.04110574452780243%\nrelative ICaCE scores for concept nazism is -0.11304079745143916%\nrelative ICaCE scores for concept genocide is -0.051382180659742524%\nrelative ICaCE scores for concept funny is 0.16442297811119572%\nrelative ICaCE scores for concept anti muslim is 0.010276436131954114%\nrelative ICaCE scores for concept terrorism is -0.02055287226389421%\nrelative ICaCE scores for concept violence is 0.38022813688213397%\nrelative ICaCE scores for concept politics is 0.16442297811119572%\nrelative ICaCE scores for concept racism is -0.051382180659742524%\nrelative ICaCE scores for concept international relation is -0.26718733943068074%\nrelative ICaCE scores for concept adult is -0.23635803103483244%\nrelative ICaCE scores for concept gore is -0.051382180659742524%\nrelative ICaCE scores for concept misogynistic is 0.04110574452780243%\nrelative ICaCE scores for concept immigration is 0.25691090329874067%\nrelative ICaCE scores for concept extremism is -0.08221148905559085%\nrelative ICaCE scores for concept immoral is -0.02055287226389421%\nrelative ICaCE scores for concept white supremacy is -0.051382180659742524%\nviolence immigration funny politics holocaust misogynistic anti muslim \n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from captum.attr import IntegratedGradients\n",
        "\n",
        "# # input1 = torch.tensor([[1.0, 3.0], [3.0, 5.0]], requires_grad=True)\n",
        "# # input2 = torch.tensor([[1.0, 4.0], [0.0, 2.0]], requires_grad=True)\n",
        "# # # Initializing our toy model\n",
        "# # model = ToyModel_With_Additional_Forward_Args()\n",
        "# # # Applying integrated gradients on the input\n",
        "# ig = IntegratedGradients(vb)\n",
        "# (input1_attr, input2_attr), delta = ig.attribute((input1, input2), n_steps=100,\n",
        "#                                     additional_forward_args=1, return_convergence_delta=True)\n",
        "# output\n",
        "# .........\n",
        "# input1_attr: tensor([[0.0000, 0.0000],\n",
        "#                      [0.0000, 3.3428]], grad_fn=<MulBackward0>)\n",
        "# input2_attr:  tensor([[ 0.0000,  0.0000],\n",
        "#                       [0.0000, -1.3371]], grad_fn=<MulBackward0>)\n",
        "# approximation_error (aka delta): 0.005693793296813965"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T14:06:02.683779Z",
          "iopub.execute_input": "2024-09-21T14:06:02.684443Z",
          "iopub.status.idle": "2024-09-21T14:06:02.689267Z",
          "shell.execute_reply.started": "2024-09-21T14:06:02.684412Z",
          "shell.execute_reply": "2024-09-21T14:06:02.688289Z"
        },
        "trusted": true,
        "id": "dPmS6qPWV9Pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "concept_classes"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T14:06:03.936550Z",
          "iopub.execute_input": "2024-09-21T14:06:03.937507Z",
          "iopub.status.idle": "2024-09-21T14:06:03.943343Z",
          "shell.execute_reply.started": "2024-09-21T14:06:03.937473Z",
          "shell.execute_reply": "2024-09-21T14:06:03.942357Z"
        },
        "trusted": true,
        "id": "nWeOHbYfV9Pi",
        "outputId": "ce6b268b-dd58-4620-af13-a82a2722af14"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 63,
          "output_type": "execute_result",
          "data": {
            "text/plain": "['holocaust',\n 'nazism',\n 'genocide',\n 'funny',\n 'anti muslim',\n 'terrorism',\n 'violence',\n 'politics',\n 'racism',\n 'international relation',\n 'adult',\n 'gore',\n 'misogynistic',\n 'immigration',\n 'extremism',\n 'immoral',\n 'white supremacy',\n 'indecency']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vb"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T14:06:06.562646Z",
          "iopub.execute_input": "2024-09-21T14:06:06.563314Z",
          "iopub.status.idle": "2024-09-21T14:06:06.571719Z",
          "shell.execute_reply.started": "2024-09-21T14:06:06.563265Z",
          "shell.execute_reply": "2024-09-21T14:06:06.570853Z"
        },
        "trusted": true,
        "id": "N5PFs06eV9Pi",
        "outputId": "bb7340ed-fa42-4b98-9d46-5ac758168ed3"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 64,
          "output_type": "execute_result",
          "data": {
            "text/plain": "VB(\n  (vb): VisualBertModel(\n    (embeddings): VisualBertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=1)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n      (visual_token_type_embeddings): Embedding(2, 768)\n      (visual_position_embeddings): Embedding(512, 768)\n      (visual_projection): Linear(in_features=2048, out_features=768, bias=True)\n    )\n    (encoder): VisualBertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x VisualBertLayer(\n          (attention): VisualBertAttention(\n            (self): VisualBertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): VisualBertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): VisualBertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): VisualBertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): VisualBertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (linear_relu_stack): Sequential(\n    (0): Dropout(p=0.1, inplace=False)\n    (1): Linear(in_features=768, out_features=512, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=512, out_features=128, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=128, out_features=2, bias=True)\n  )\n  (linear_relu_stack_latent): Sequential(\n    (0): Dropout(p=0.1, inplace=False)\n    (1): Linear(in_features=768, out_features=512, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=512, out_features=128, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=128, out_features=2, bias=True)\n  )\n  (linear_relu_stack_hidden): Sequential(\n    (0): Dropout(p=0.1, inplace=False)\n    (1): Linear(in_features=768, out_features=512, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=512, out_features=128, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=128, out_features=2, bias=True)\n  )\n  (project_latent): Linear(in_features=768, out_features=2048, bias=True)\n  (proj_conc): Linear(in_features=768, out_features=28, bias=True)\n  (proj_embed): Linear(in_features=768, out_features=28, bias=True)\n  (grad_rev1): RevGrad()\n  (grad_rev2): RevGrad()\n)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "baselines = (torch.randn(20,18,768).to('cuda'), torch.randn(20,64,768).to('cuda'), torch.randn(20,36,2048).to('cuda'))\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T14:06:14.516804Z",
          "iopub.execute_input": "2024-09-21T14:06:14.517766Z",
          "iopub.status.idle": "2024-09-21T14:06:14.549210Z",
          "shell.execute_reply.started": "2024-09-21T14:06:14.517732Z",
          "shell.execute_reply": "2024-09-21T14:06:14.548485Z"
        },
        "trusted": true,
        "id": "rj8dxDt8V9Pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "from captum.attr import IntegratedGradients,Saliency,DeepLift,DeepLiftShap, GradientShap, InputXGradient, KernelShap, FeatureAblation\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T14:06:15.297510Z",
          "iopub.execute_input": "2024-09-21T14:06:15.298303Z",
          "iopub.status.idle": "2024-09-21T14:06:15.367940Z",
          "shell.execute_reply.started": "2024-09-21T14:06:15.298252Z",
          "shell.execute_reply": "2024-09-21T14:06:15.367100Z"
        },
        "trusted": true,
        "id": "qEb4S-lPV9Pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.manual_seed(42)\n",
        "# print(torch.randn(2,3))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T14:06:17.524787Z",
          "iopub.execute_input": "2024-09-21T14:06:17.525914Z",
          "iopub.status.idle": "2024-09-21T14:06:17.530529Z",
          "shell.execute_reply.started": "2024-09-21T14:06:17.525871Z",
          "shell.execute_reply": "2024-09-21T14:06:17.529172Z"
        },
        "trusted": true,
        "id": "PGKhIRubV9Pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "#vb_wrapper = VB_wrapper(vb)\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T14:06:18.334319Z",
          "iopub.execute_input": "2024-09-21T14:06:18.334684Z",
          "iopub.status.idle": "2024-09-21T14:06:18.339036Z",
          "shell.execute_reply.started": "2024-09-21T14:06:18.334657Z",
          "shell.execute_reply": "2024-09-21T14:06:18.337794Z"
        },
        "trusted": true,
        "id": "DWjSMU2GV9Pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "!pip install shutup"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T14:06:19.192235Z",
          "iopub.execute_input": "2024-09-21T14:06:19.192631Z",
          "iopub.status.idle": "2024-09-21T14:06:31.802966Z",
          "shell.execute_reply.started": "2024-09-21T14:06:19.192601Z",
          "shell.execute_reply": "2024-09-21T14:06:31.801768Z"
        },
        "trusted": true,
        "id": "P8UCNl5GV9Pk",
        "outputId": "c2e09b31-68ea-416d-eedf-41851b71faf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting shutup\n  Downloading shutup-0.2.0-py3-none-any.whl.metadata (530 bytes)\nDownloading shutup-0.2.0-py3-none-any.whl (1.5 kB)\nInstalling collected packages: shutup\nSuccessfully installed shutup-0.2.0\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(gc)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T14:06:31.805341Z",
          "iopub.execute_input": "2024-09-21T14:06:31.805745Z",
          "iopub.status.idle": "2024-09-21T14:06:31.813162Z",
          "shell.execute_reply.started": "2024-09-21T14:06:31.805707Z",
          "shell.execute_reply": "2024-09-21T14:06:31.812310Z"
        },
        "trusted": true,
        "id": "1EUt-bqrV9Pk",
        "outputId": "db1a0b85-3ad5-40a0-b056-7b041f5d468c"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 70,
          "output_type": "execute_result",
          "data": {
            "text/plain": "208"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def call_sim():\n",
        "\n",
        "    import math\n",
        "    # Quantitative case study (1) Faithfulness on concepts\n",
        "    # Quantitative case study (2) Are they explainable?\n",
        "    test_dataloader=DataLoader(t_p, shuffle=True, batch_size=1)\n",
        "\n",
        "    pred_r1, pred_r2 = [],[]\n",
        "    pred_both = []\n",
        "    act = []\n",
        "\n",
        "    multimodal_repr = []\n",
        "    concept_repr = []\n",
        "\n",
        "    concept_repr1 = []\n",
        "    concept_repr2 = []\n",
        "    y_p = []\n",
        "\n",
        "    cr, rr = [],[]\n",
        "    cr1,rr1 = [],[]\n",
        "\n",
        "\n",
        "    ours_icace = []\n",
        "    ia_icace = []\n",
        "\n",
        "    concept_cat = {}\n",
        "    for i,c in enumerate(concept_classes):\n",
        "        concept_cat[c] = all_concepts[i]\n",
        "\n",
        "    for ii, batch in tqdm(enumerate(eval_dataloader)):\n",
        "\n",
        "        #if batch[1].detach().cpu().numpy()[0] == 1:\n",
        "\n",
        "        ll = batch[1].detach().cpu().numpy()[0]\n",
        "        y_p.append(ll)\n",
        "\n",
        "        #print('*'*10)\n",
        "\n",
        "        input_ids = batch[0]['input_ids'].to('cuda')\n",
        "        #print(tokenizer.batch_decode(batch[0]['input_ids'], skip_special_tokens=True))\n",
        "        token_type_ids = batch[0]['token_type_ids'].to('cuda')\n",
        "        attention_mask = batch[0]['attention_mask'].to('cuda')\n",
        "        visual_embeds = batch[0]['visual_embeds'].to('cuda')\n",
        "        visual_token_type_ids = batch[0]['visual_token_type_ids'].to('cuda')\n",
        "        visual_attention_mask = batch[0]['visual_attention_mask'].to('cuda')\n",
        "\n",
        "        inputs_embeds = vb_wrapper.vb.vb.embeddings.word_embeddings(input_ids)\n",
        "\n",
        "        #     (input1_attr, input2_attr), delta = ig.attribute(inputs = (all_concepts.to('cuda'), inputs_embeds), additional_forward_args = (attention_mask,\n",
        "        #         token_type_ids,\n",
        "        #         None,\n",
        "        #         None,\n",
        "        #         None,\n",
        "        #         visual_embeds,\n",
        "        #         visual_attention_mask,\n",
        "        #         visual_token_type_ids), return_convergence_delta=True)\n",
        "\n",
        "        #print(visual_embeds)\n",
        "\n",
        "        #     (input1_attr, input2_attr, input3_attr) = ig.attribute(inputs = (all_concepts.unsqueeze(0).to('cuda'), inputs_embeds, visual_embeds), additional_forward_args = (attention_mask,\n",
        "        #         token_type_ids,\n",
        "        #         None,\n",
        "        #         None,\n",
        "        #         None,\n",
        "        #         visual_attention_mask,\n",
        "        #         visual_token_type_ids))\n",
        "        #vb(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask, visual_embeds=visual_embeds_latent, visual_attention_mask=visual_attention_mask_latent, visual_token_type_ids=visual_token_type_ids_latent)\n",
        "\n",
        "        if type(ig) in [captum.attr._core.kernel_shap.KernelShap, captum.attr._core.input_x_gradient.InputXGradient, captum.attr._core.deep_lift.DeepLift, captum.attr._core.saliency.Saliency, captum.attr._core.integrated_gradients.IntegratedGradients, captum.attr._core.feature_ablation.FeatureAblation]:\n",
        "                baseline = None\n",
        "                #print('none')\n",
        "        else:\n",
        "            baseline = baselines\n",
        "            #print('baseline')\n",
        "\n",
        "        all_conc = all_concepts.to('cuda')\n",
        "        if type(ig) in [captum.attr._core.input_x_gradient.InputXGradient,captum.attr._core.saliency.Saliency]:\n",
        "            #print('ip_grad')\n",
        "            (input1_attr, input2_attr, input3_attr) = ig.attribute(inputs = (all_conc.unsqueeze(0), inputs_embeds, visual_embeds),\n",
        "\n",
        "                                                                additional_forward_args = (attention_mask,\n",
        "            token_type_ids,\n",
        "            None,\n",
        "            None,\n",
        "            None,\n",
        "            visual_attention_mask,\n",
        "            visual_token_type_ids))\n",
        "\n",
        "        else:\n",
        "            #print('others')\n",
        "\n",
        "            (input1_attr, input2_attr, input3_attr) = ig.attribute(inputs = (all_conc.unsqueeze(0), inputs_embeds, visual_embeds),\n",
        "                                                                  baselines = baseline,\n",
        "                                                                    additional_forward_args = (attention_mask,\n",
        "                token_type_ids,\n",
        "                None,\n",
        "                None,\n",
        "                None,\n",
        "                visual_attention_mask,\n",
        "                visual_token_type_ids))\n",
        "\n",
        "\n",
        "        #print(input1_attr, input2_attr)\n",
        "        #print(input1_attr.shape, input2_attr.shape)\n",
        "        k1=input1_attr.sum(dim=-1).squeeze(0).detach().cpu().numpy()\n",
        "\n",
        "        conc_scr = []\n",
        "        attribution_score = {}\n",
        "        for i in range(18):\n",
        "            cc = concept_classes[i]\n",
        "            #print('relative attribution scores for concept {} is {}'.format(cc,k[i]))\n",
        "            conc_scr.append((cc,k1[i]))\n",
        "            attribution_score[cc] = k1[i]\n",
        "            #conc_scr[cc] = k1[i]\n",
        "        conc_scr.sort(key=lambda tup: tup[1], reverse=True)\n",
        "\n",
        "        d = {}\n",
        "\n",
        "        for cnt,i in enumerate(conc_scr):\n",
        "            d[i[0]] = cnt\n",
        "\n",
        "        #print(d)\n",
        "\n",
        "\n",
        "        orig,pooled_op = model_pred(input_ids,attention_mask,token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids,all_concepts)\n",
        "        orig = orig[0]\n",
        "        pooled_op = pooled_op.detach().cpu()\n",
        "        multimodal_repr.append(pooled_op)\n",
        "        #print('predicted class {}'.format(class_map[np.argmax(orig.detach().cpu().numpy())]))\n",
        "        #print('actual class {}'.format(ll))\n",
        "        #print(0/0)\n",
        "        #orig = orig[0]\n",
        "        k = []\n",
        "        pred_class = np.argmax(orig.detach().cpu().numpy())\n",
        "        causal_score = {}\n",
        "        #print(pred_class)\n",
        "        for i in range(18):\n",
        "            #print('Removing concept: {}'.format(concept_classes[i]))\n",
        "            cc = concept_classes[i]\n",
        "            after_intervention,_ = model_pred(input_ids,attention_mask,token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids,all_concepts,concept_no=i)\n",
        "            after_intervention = after_intervention[0][pred_class]\n",
        "            #print('hello')\n",
        "            #print(orig[pred_class], after_intervention)\n",
        "            #print(0/0)\n",
        "            k.append((cc,abs(after_intervention-orig[pred_class]).item()))\n",
        "            causal_score[cc] = abs(after_intervention-orig[pred_class]).item()\n",
        "            #print('model prediction after removal {}'.format(model_pred(input_ids,attention_mask,token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids,concept_no=i)))\n",
        "        #k = np.asarray(k)\n",
        "        #k = np.asarray(k)\n",
        "        #k = list(map(lambda x: ((x-np.mean(k))/np.mean(k))*100, k))\n",
        "        k.sort(key=lambda tup: tup[1], reverse=True)\n",
        "        #print(k)\n",
        "\n",
        "        #print(list(map(lambda x: x[0], list(k))))\n",
        "\n",
        "        gamma = 0.9\n",
        "        temp = []\n",
        "        for cnt,i in enumerate(list(map(lambda x: x[0], list(k)))):\n",
        "            temp.append((gamma**cnt) * concept_cat[i])\n",
        "            #temp.append((math.e**(-cnt)) * concept_cat[i])\n",
        "            #temp.append((1.1**(-cnt)) * concept_cat[i])\n",
        "\n",
        "        temp = torch.stack(temp).mean(dim=0).unsqueeze(dim=0)\n",
        "        #print(temp)\n",
        "        #print(temp.shape)\n",
        "\n",
        "        concept_repr.append(temp)\n",
        "\n",
        "\n",
        "\n",
        "        temp = []\n",
        "        for cnt,i in enumerate(list(map(lambda x: x[0], list(conc_scr)))):\n",
        "            temp.append((gamma**cnt) * concept_cat[i])\n",
        "            #temp.append((math.e**(-cnt)) * concept_cat[i])\n",
        "            #temp.append((1.1**(-cnt)) * concept_cat[i])\n",
        "\n",
        "        temp = torch.stack(temp).mean(dim=0).unsqueeze(dim=0)\n",
        "        #print(temp)\n",
        "        #print(temp.shape)\n",
        "\n",
        "        concept_repr1.append(temp)\n",
        "\n",
        "\n",
        "        #print(list(map(lambda x: x[0], list(conc_scr))))\n",
        "        #print(0/0)\n",
        "\n",
        "        #considering the k to be the causally sorted rank, we estimate ranks of\n",
        "\n",
        "        ip_attrib_causal = []\n",
        "\n",
        "\n",
        "\n",
        "        for i in range(18):\n",
        "            cc = concept_classes[i]\n",
        "            ip_attrib_causal.append((cc,((2*attribution_score[cc]*causal_score[cc]) / (attribution_score[cc]+causal_score[cc]))))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ip_attrib_causal.sort(key=lambda tup: tup[1], reverse=True)\n",
        "\n",
        "\n",
        "\n",
        "        ra_icace_ours = []\n",
        "        for cnt,i in enumerate(list(map(lambda x: x[0], list(k)))):\n",
        "            ra_icace_ours.append((gamma**cnt) * causal_score[i])\n",
        "            #ra_icace_ours.append((math.e**(-cnt)) * causal_score[i])\n",
        "            #ra_icace_ours.append((1.1**(-cnt)) * causal_score[i])\n",
        "\n",
        "        ra_icace_ia = []\n",
        "        for cnt,i in enumerate(list(map(lambda x: x[0], list(conc_scr)))):\n",
        "            ra_icace_ia.append((gamma**cnt) * causal_score[i])\n",
        "            #ra_icace_ia.append((math.e**(-cnt)) * causal_score[i])\n",
        "            #ra_icace_ia.append((1.1**(-cnt)) * causal_score[i])\n",
        "\n",
        "\n",
        "\n",
        "        ours_icace.append(np.mean(ra_icace_ours))\n",
        "        ia_icace.append(np.mean(ra_icace_ia))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        temp = []\n",
        "        for cnt,i in enumerate(list(map(lambda x: x[0], list(ip_attrib_causal)))):\n",
        "            temp.append((gamma**cnt) * concept_cat[i])\n",
        "            #temp.append((math.e**(-cnt)) * concept_cat[i])\n",
        "            #temp.append((1.1**(-cnt)) * concept_cat[i])\n",
        "\n",
        "        temp = torch.stack(temp).mean(dim=0).unsqueeze(dim=0)\n",
        "\n",
        "        concept_repr2.append(temp)\n",
        "\n",
        "\n",
        "\n",
        "        d1 = {}\n",
        "\n",
        "        for cnt,i in enumerate(ip_attrib_causal):\n",
        "            d1[i[0]] = cnt\n",
        "\n",
        "        r1,r2,r3 = [],[],[]\n",
        "        for cnt,i in enumerate(list(map(lambda x: x[0], list(k)))):\n",
        "            r1.append(cnt)\n",
        "            r2.append(d[i])\n",
        "            r3.append(d1[i])\n",
        "\n",
        "        #print(r1,r2)\n",
        "        corr, _ = stats.kendalltau(r1, r2)\n",
        "        res,_ = stats.spearmanr(r1, r2)\n",
        "\n",
        "        corr1, _ = stats.kendalltau(r1, r3)\n",
        "        res1,_ = stats.spearmanr(r1, r3)\n",
        "\n",
        "        #print('kendall tau', corr)\n",
        "        cr.append(corr)\n",
        "        #print('spearmans rho', res)\n",
        "        rr.append(res)\n",
        "\n",
        "        cr1.append(corr1)\n",
        "        rr1.append(res1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    multimodal_repr = torch.stack(multimodal_repr).squeeze(1)\n",
        "\n",
        "    #causal\n",
        "    concept_repr = torch.stack(concept_repr).squeeze(1)\n",
        "\n",
        "\n",
        "    #attribution\n",
        "    concept_repr1 = torch.stack(concept_repr1).squeeze(1)\n",
        "\n",
        "    #HM\n",
        "    concept_repr2 = torch.stack(concept_repr2).squeeze(1)\n",
        "\n",
        "\n",
        "    return np.mean(cr), np.mean(rr), multimodal_repr, concept_repr, concept_repr1, y_p\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T14:06:38.574607Z",
          "iopub.execute_input": "2024-09-21T14:06:38.574997Z",
          "iopub.status.idle": "2024-09-21T14:06:38.617846Z",
          "shell.execute_reply.started": "2024-09-21T14:06:38.574964Z",
          "shell.execute_reply": "2024-09-21T14:06:38.616740Z"
        },
        "trusted": true,
        "id": "8ejyjdqNV9Pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /kaggle/working"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T14:06:41.703552Z",
          "iopub.execute_input": "2024-09-21T14:06:41.703906Z",
          "iopub.status.idle": "2024-09-21T14:06:42.809615Z",
          "shell.execute_reply.started": "2024-09-21T14:06:41.703879Z",
          "shell.execute_reply": "2024-09-21T14:06:42.808377Z"
        },
        "trusted": true,
        "id": "m6uq-kZiV9Pl",
        "outputId": "7ba7d8bc-898a-42f9-f094-34fc1472d87c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "25461.png\t   cr1.pt\t\t\t  mean_comparison_plot.png\n28076.png\t   cr2.pt\t\t\t  mr.pt\n94162.png\t   final_model_concept.pt\t  rr.npy\n96704.png\t   final_model_concept_inlp.pt\t  rr1.npy\ncausal_vb.pt\t   final_model_wo_adversarial.pt  transformers\ncausal_vb_inlp.pt  foo1.png\t\t\t  vs_tensors.pt\ncr.npy\t\t   full_annotation.pkl\t\t  y_p.pt\ncr.pt\t\t   image1.jpg\ncr1.npy\t\t   image2.jpg\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(DECONFOUND, DYN_ROUTE)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T14:06:45.291827Z",
          "iopub.execute_input": "2024-09-21T14:06:45.292195Z",
          "iopub.status.idle": "2024-09-21T14:06:45.297769Z",
          "shell.execute_reply.started": "2024-09-21T14:06:45.292166Z",
          "shell.execute_reply": "2024-09-21T14:06:45.296892Z"
        },
        "trusted": true,
        "id": "UA9A8AvCV9Pl",
        "outputId": "96b94620-9284-4c70-f381-ad11440ba26d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "True True\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#clf = svm.SVC(kernel='poly', C=2, random_state=42)\n",
        "\n",
        "def get_sim(multimodal_repr, concept_repr1, y_p):\n",
        "\n",
        "    X = np.concatenate([multimodal_repr.numpy(),concept_repr1.numpy()],axis=-1) # inp w/ clip representation w/ exp -> w/ both\n",
        "    #X = (multimodal_repr.numpy()*concept_repr.numpy())\n",
        "    X_ = multimodal_repr.numpy() # inp w/ clip representation w/o exp -> w/ only input\n",
        "    X__ = concept_repr1.numpy() # inp w/o clip representation w/ exp -> w/ only exp\n",
        "\n",
        "    print(X.shape)\n",
        "    print(X_.shape)\n",
        "    print(X__.shape)\n",
        "    print(y_p)\n",
        "    #print(0/0)\n",
        "\n",
        "\n",
        "\n",
        "    kf = KFold(n_splits=5)\n",
        "    y = np.array(y_p)\n",
        "    vals = []\n",
        "    vals_lus = []\n",
        "    k1,k2,k3 = [],[],[]\n",
        "    C,S = [],[]\n",
        "    for train, test in kf.split(X):\n",
        "\n",
        "        X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]\n",
        "        X_train_, X_test_, y_train, y_test = X_[train], X_[test], y[train], y[test]\n",
        "        X_train__, X_test__, y_train, y_test = X__[train], X__[test], y[train], y[test]\n",
        "\n",
        "\n",
        "        clf1 = make_pipeline(StandardScaler(),svm.SVC(probability=True, kernel='rbf', C=2, random_state=42))\n",
        "        clf1.fit(X_train, y_train)\n",
        "        y_pred = clf1.predict(X_test)\n",
        "\n",
        "        clf2 = make_pipeline(StandardScaler(),svm.SVC(probability=True,kernel='rbf', C=2, random_state=42))\n",
        "        clf2.fit(X_train_, y_train)\n",
        "        y_pred_ = clf2.predict(X_test_)\n",
        "\n",
        "        clf3 = make_pipeline(StandardScaler(), svm.SVC(probability=True,kernel='rbf', C=2, random_state=42))\n",
        "        clf3.fit(X_train__, y_train)\n",
        "        y_pred__ = clf3.predict(X_test__)\n",
        "\n",
        "\n",
        "        y_pred_proba_w_both = clf1.predict_proba(X_test)\n",
        "        y_pred_proba_w_inp = clf2.predict_proba(X_test_)\n",
        "        y_pred_proba_w_exp = clf3.predict_proba(X_test__)\n",
        "\n",
        "\n",
        "\n",
        "        comprehensiveness, sufficiency = 0,0\n",
        "\n",
        "        counter = 0\n",
        "        for pred_proba_w_both, pred_proba_w_inp, pred_proba_w_exp, pred_class in zip(y_pred_proba_w_both,y_pred_proba_w_inp,y_pred_proba_w_exp,y_pred):\n",
        "            comprehensiveness += (pred_proba_w_both[pred_class] - pred_proba_w_inp[pred_class])\n",
        "            sufficiency += (pred_proba_w_both[pred_class] - pred_proba_w_exp[pred_class])\n",
        "            counter+=1\n",
        "\n",
        "\n",
        "        comprehensiveness = comprehensiveness / counter\n",
        "        sufficiency = sufficiency / counter\n",
        "\n",
        "        C.append(comprehensiveness)\n",
        "        S.append(sufficiency)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        l, nl = 0,0\n",
        "        us = 0\n",
        "        for i ,j in zip(y_pred__,y_test):\n",
        "            if i==j:\n",
        "                l+=1\n",
        "            else:\n",
        "                nl+=1\n",
        "        a,b=0,0\n",
        "        for i,j,k,x in zip(y_pred,y_pred_,y_test,y_pred__):\n",
        "            us+=int(i==k) - int(j==k)\n",
        "            if k==x:\n",
        "                a += int(i==k) - int(j==k)\n",
        "            elif k!=x:\n",
        "                b += int(i==k) - int(j==k)\n",
        "\n",
        "        #print('LAS ',.5*(a/l)+.5*(b/nl))\n",
        "        #print('Leakage Unadjusted Simulatability (LUS)', (us/(l+nl)))\n",
        "        vals_lus.append(us/(l+nl))\n",
        "        #print('Comprehensiveness ', comprehensiveness)\n",
        "        #print('Sufficiency ', sufficiency)\n",
        "        #print('f1 between prediction of proposed model and SVM w/ both inp and exp {}'.format(f1_score(y_test, y_pred, average='macro')))\n",
        "        #print('f1 between prediction of proposed model and SVM w/ only inp {}'.format(f1_score(y_test, y_pred_, average='macro')))\n",
        "        #print('f1 between prediction of proposed model and SVM w/ only exp {}'.format(f1_score(y_test, y_pred__, average='macro')))\n",
        "        vals.append(.5*(a/l)+.5*(b/nl))\n",
        "        k1.append(f1_score(y_test, y_pred, average='macro'))\n",
        "        k2.append(f1_score(y_test, y_pred_, average='macro'))\n",
        "        k3.append(f1_score(y_test, y_pred__, average='macro'))\n",
        "\n",
        "\n",
        "    #print('Intra mean ', np.mean(ID), 'Intra std ', np.std(ID))\n",
        "    #print('Inter mean ', np.mean(ID1), 'Inter std ', np.std(ID1))\n",
        "    print('LAS mean ', np.mean(vals), 'LAS std ', np.std(vals))\n",
        "    print('LUS mean ', np.mean(vals_lus), 'LUS std ', np.std(vals_lus))\n",
        "    print('Comprehensiveness mean ', np.mean(C), 'Comprehensiveness std ', np.std(C))\n",
        "    print('Sufficiency mean ', np.mean(S), 'Sufficiency std ', np.std(S))\n",
        "    print('Avg. f1 between prediction of proposed model and SVM w/ both inp and exp {}'.format(np.mean(k1)))\n",
        "    print('Avg. f1 between prediction of proposed model and SVM w/ only inp {}'.format(np.mean(k2)))\n",
        "    print('Avg. f1 between prediction of proposed model and SVM w/ only exp {}'.format(np.mean(k3)))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T14:06:47.529536Z",
          "iopub.execute_input": "2024-09-21T14:06:47.530233Z",
          "iopub.status.idle": "2024-09-21T14:06:47.552213Z",
          "shell.execute_reply.started": "2024-09-21T14:06:47.530198Z",
          "shell.execute_reply": "2024-09-21T14:06:47.551111Z"
        },
        "trusted": true,
        "id": "XERHYojYV9Pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9nKZTbkBV9Pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#R\n",
        "cd = {i:[] for i in concept_classes}\n",
        "import shutup; shutup.please()\n",
        "def call():\n",
        "    import math\n",
        "    # Quantitative case study (1) Faithfulness on concepts\n",
        "    # Quantitative case study (2) Are they explainable?\n",
        "    test_dataloader=DataLoader(t_p, shuffle=True, batch_size=1)\n",
        "\n",
        "    pred_r1, pred_r2 = [],[]\n",
        "    pred_both = []\n",
        "    act = []\n",
        "\n",
        "    multimodal_repr = []\n",
        "    concept_repr = []\n",
        "\n",
        "    concept_repr1 = []\n",
        "    concept_repr2 = []\n",
        "    y_p = []\n",
        "\n",
        "    cr, rr = [],[]\n",
        "    cr1,rr1 = [],[]\n",
        "\n",
        "\n",
        "    ours_icace = []\n",
        "    ia_icace = []\n",
        "\n",
        "    counter = 0\n",
        "\n",
        "\n",
        "    c_p=0\n",
        "    c_r=0\n",
        "\n",
        "    a_p=0\n",
        "    a_r=0\n",
        "\n",
        "    average_precision_causal = 0\n",
        "    average_precision_attrib = 0\n",
        "\n",
        "    concept_cat = {}\n",
        "    for i,c in enumerate(concept_classes):\n",
        "        concept_cat[c] = all_concepts[i]\n",
        "\n",
        "    for ii, batch in tqdm(enumerate(eval_dataloader)):\n",
        "\n",
        "        if ii==0:\n",
        "            print(ig)\n",
        "\n",
        "        #if batch[1].detach().cpu().numpy()[0] == 1:\n",
        "\n",
        "        ll = batch[1].detach().cpu().numpy()[0]\n",
        "        if ll==1:\n",
        "\n",
        "            #ll = batch[1].detach().cpu().numpy()[0]\n",
        "            #print(ll, ii)\n",
        "            y_p.append(ll)\n",
        "\n",
        "            #print('*'*10)\n",
        "\n",
        "            input_ids = batch[0]['input_ids'].to('cuda')\n",
        "            #print(tokenizer.batch_decode(batch[0]['input_ids'], skip_special_tokens=True))\n",
        "            token_type_ids = batch[0]['token_type_ids'].to('cuda')\n",
        "            attention_mask = batch[0]['attention_mask'].to('cuda')\n",
        "            visual_embeds = batch[0]['visual_embeds'].to('cuda')\n",
        "            visual_token_type_ids = batch[0]['visual_token_type_ids'].to('cuda')\n",
        "            visual_attention_mask = batch[0]['visual_attention_mask'].to('cuda')\n",
        "\n",
        "            inputs_embeds = vb_wrapper.vb.vb.embeddings.word_embeddings(input_ids)\n",
        "\n",
        "            # print(inputs_embeds)\n",
        "            #     (input1_attr, input2_attr), delta = ig.attribute(inputs = (all_concepts.to('cuda'), inputs_embeds), additional_forward_args = (attention_mask,\n",
        "            #         token_type_ids,\n",
        "            #         None,\n",
        "            #         None,\n",
        "            #         None,\n",
        "            #         visual_embeds,\n",
        "            #         visual_attention_mask,\n",
        "            #         visual_token_type_ids), return_convergence_delta=True)\n",
        "\n",
        "            #print(visual_embeds)\n",
        "\n",
        "            #     (input1_attr, input2_attr, input3_attr) = ig.attribute(inputs = (all_concepts.unsqueeze(0).to('cuda'), inputs_embeds, visual_embeds), additional_forward_args = (attention_mask,\n",
        "            #         token_type_ids,\n",
        "            #         None,\n",
        "            #         None,\n",
        "            #         None,\n",
        "            #         visual_attention_mask,\n",
        "            #         visual_token_type_ids))\n",
        "            #vb(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask, visual_embeds=visual_embeds_latent, visual_attention_mask=visual_attention_mask_latent, visual_token_type_ids=visual_token_type_ids_latent)\n",
        "\n",
        "            if type(ig) in [captum.attr._core.kernel_shap.KernelShap, captum.attr._core.input_x_gradient.InputXGradient, captum.attr._core.deep_lift.DeepLift, captum.attr._core.saliency.Saliency, captum.attr._core.integrated_gradients.IntegratedGradients, captum.attr._core.feature_ablation.FeatureAblation]:\n",
        "                    baseline = None\n",
        "                    #print('none')\n",
        "            else:\n",
        "                baseline = baselines\n",
        "                #print('baseline')\n",
        "\n",
        "            if DECONFOUND:\n",
        "                all_conc = get_inlp(all_concepts).to('cuda')\n",
        "            else:\n",
        "                all_conc = all_concepts.to('cuda')\n",
        "\n",
        "            #print(all_conc)\n",
        "            #print(all_concepts)\n",
        "\n",
        "            #print(0/0)\n",
        "\n",
        "            #all_conc = all_concepts.to('cuda')\n",
        "\n",
        "            if type(ig) in [captum.attr._core.input_x_gradient.InputXGradient,captum.attr._core.saliency.Saliency]:\n",
        "                #print('ip_grad')\n",
        "                (input1_attr, input2_attr, input3_attr) = ig.attribute(inputs = (all_conc.unsqueeze(0).to('cuda'), inputs_embeds, visual_embeds),\n",
        "\n",
        "                                                                    additional_forward_args = (attention_mask,\n",
        "                token_type_ids,\n",
        "                None,\n",
        "                None,\n",
        "                None,\n",
        "                visual_attention_mask,\n",
        "                visual_token_type_ids))\n",
        "\n",
        "            else:\n",
        "                #print('others')\n",
        "\n",
        "                (input1_attr, input2_attr, input3_attr) = ig.attribute(inputs = (all_conc.unsqueeze(0).to('cuda'), inputs_embeds, visual_embeds),\n",
        "                                                                      baselines = baseline,\n",
        "                                                                        additional_forward_args = (attention_mask,\n",
        "                    token_type_ids,\n",
        "                    None,\n",
        "                    None,\n",
        "                    None,\n",
        "                    visual_attention_mask,\n",
        "                    visual_token_type_ids))\n",
        "\n",
        "\n",
        "            #print(input1_attr, input2_attr)\n",
        "            #print(input1_attr.shape, input2_attr.shape)\n",
        "            k1=input1_attr.sum(dim=-1).squeeze(0).detach().cpu().numpy()\n",
        "\n",
        "            conc_scr = []\n",
        "            attribution_score = {}\n",
        "            for i in range(18):\n",
        "                cc = concept_classes[i]\n",
        "                #print('relative attribution scores for concept {} is {}'.format(cc,k[i]))\n",
        "                conc_scr.append((cc,k1[i]))\n",
        "                attribution_score[cc] = k1[i]\n",
        "                #conc_scr[cc] = k1[i]\n",
        "            conc_scr.sort(key=lambda tup: tup[1], reverse=True)\n",
        "\n",
        "            d = {}\n",
        "\n",
        "            for cnt,i in enumerate(conc_scr):\n",
        "                d[i[0]] = cnt\n",
        "\n",
        "            #print(d)\n",
        "\n",
        "\n",
        "            orig,pooled_op = model_pred(input_ids,attention_mask,token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids,all_concepts)\n",
        "            orig = orig[0]\n",
        "            pooled_op = pooled_op.detach().cpu()\n",
        "            multimodal_repr.append(pooled_op)\n",
        "            #print('predicted class {}'.format(class_map[np.argmax(orig.detach().cpu().numpy())]))\n",
        "            #print('actual class {}'.format(ll))\n",
        "            #print(0/0)\n",
        "            #orig = orig[0]\n",
        "            k = []\n",
        "            pred_class = np.argmax(orig.detach().cpu().numpy())\n",
        "            causal_score = {}\n",
        "            #print(pred_class)\n",
        "            for i in range(18):\n",
        "                #print('Removing concept: {}'.format(concept_classes[i]))\n",
        "                cc = concept_classes[i]\n",
        "                after_intervention,_ = model_pred(input_ids,attention_mask,token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids,all_concepts,concept_no=i)\n",
        "                after_intervention = after_intervention[0][pred_class]\n",
        "                #print('hello')\n",
        "                #print(orig[pred_class], after_intervention)\n",
        "                #print(0/0)\n",
        "                k.append((cc,abs(after_intervention-orig[pred_class]).item()))\n",
        "                causal_score[cc] = abs(after_intervention-orig[pred_class]).item()\n",
        "                #print('model prediction after removal {}'.format(model_pred(input_ids,attention_mask,token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids,concept_no=i)))\n",
        "            #k = np.asarray(k)\n",
        "            #k = np.asarray(k)\n",
        "            #k = list(map(lambda x: ((x-np.mean(k))/np.mean(k))*100, k))\n",
        "            k.sort(key=lambda tup: tup[1], reverse=True)\n",
        "\n",
        "            #print(k)\n",
        "\n",
        "            #random.seed(counter)\n",
        "            #random.shuffle(k)\n",
        "\n",
        "            #print(k)\n",
        "\n",
        "            #print(0/0)\n",
        "\n",
        "\n",
        "            for i in k:\n",
        "                cd[i[0]].append(i[1])\n",
        "\n",
        "\n",
        "\n",
        "            #print(list(map(lambda x: x[0], list(k))))\n",
        "\n",
        "            gamma = 0.9\n",
        "            temp = []\n",
        "            for cnt,i in enumerate(list(map(lambda x: x[0], list(k)))):\n",
        "                temp.append((gamma**cnt) * concept_cat[i])\n",
        "                #temp.append((math.e**(-cnt)) * concept_cat[i])\n",
        "                #temp.append((1.1**(-cnt)) * concept_cat[i])\n",
        "\n",
        "            temp = torch.stack(temp).mean(dim=0).unsqueeze(dim=0)\n",
        "            #print(temp)\n",
        "            #print(temp.shape)\n",
        "\n",
        "            concept_repr.append(temp)\n",
        "\n",
        "\n",
        "\n",
        "            temp = []\n",
        "            for cnt,i in enumerate(list(map(lambda x: x[0], list(conc_scr)))):\n",
        "                temp.append((gamma**cnt) * concept_cat[i])\n",
        "                #temp.append((math.e**(-cnt)) * concept_cat[i])\n",
        "                #temp.append((1.1**(-cnt)) * concept_cat[i])\n",
        "\n",
        "            temp = torch.stack(temp).mean(dim=0).unsqueeze(dim=0)\n",
        "            #print(temp)\n",
        "            #print(temp.shape)\n",
        "\n",
        "            concept_repr1.append(temp)\n",
        "\n",
        "\n",
        "            #print(list(map(lambda x: x[0], list(conc_scr))))\n",
        "            #print(0/0)\n",
        "\n",
        "            #considering the k to be the causally sorted rank, we estimate ranks of\n",
        "\n",
        "            ip_attrib_causal = []\n",
        "\n",
        "\n",
        "\n",
        "            for i in range(18):\n",
        "                cc = concept_classes[i]\n",
        "                ip_attrib_causal.append((cc,((2*attribution_score[cc]*causal_score[cc]) / (attribution_score[cc]+causal_score[cc]))))\n",
        "\n",
        "\n",
        "\n",
        "            ip_attrib = []\n",
        "            for i in range(18):\n",
        "                cc = concept_classes[i]\n",
        "                ip_attrib.append((cc,attribution_score[cc]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            ip_attrib_causal.sort(key=lambda tup: tup[1], reverse=True)\n",
        "            ip_attrib.sort(key=lambda tup: tup[1], reverse=True)\n",
        "\n",
        "\n",
        "\n",
        "            ra_icace_ours = []\n",
        "            for cnt,i in enumerate(list(map(lambda x: x[0], list(k)))):\n",
        "                ra_icace_ours.append((gamma**cnt) * causal_score[i])\n",
        "                #ra_icace_ours.append((math.e**(-cnt)) * causal_score[i])\n",
        "                #ra_icace_ours.append((1.1**(-cnt)) * causal_score[i])\n",
        "\n",
        "            ra_icace_ia = []\n",
        "            for cnt,i in enumerate(list(map(lambda x: x[0], list(conc_scr)))):\n",
        "                ra_icace_ia.append((gamma**cnt) * causal_score[i])\n",
        "                #ra_icace_ia.append((math.e**(-cnt)) * causal_score[i])\n",
        "                #ra_icace_ia.append((1.1**(-cnt)) * causal_score[i])\n",
        "\n",
        "\n",
        "\n",
        "            ours_icace.append(np.mean(ra_icace_ours))\n",
        "            ia_icace.append(np.mean(ra_icace_ia))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            temp = []\n",
        "            for cnt,i in enumerate(list(map(lambda x: x[0], list(ip_attrib_causal)))):\n",
        "                temp.append((gamma**cnt) * concept_cat[i])\n",
        "                #temp.append((math.e**(-cnt)) * concept_cat[i])\n",
        "                #temp.append((1.1**(-cnt)) * concept_cat[i])\n",
        "\n",
        "            temp = torch.stack(temp).mean(dim=0).unsqueeze(dim=0)\n",
        "\n",
        "            concept_repr2.append(temp)\n",
        "\n",
        "\n",
        "\n",
        "            d1 = {}\n",
        "\n",
        "            for cnt,i in enumerate(ip_attrib_causal):\n",
        "                d1[i[0]] = cnt\n",
        "\n",
        "            r1,r2,r3 = [],[],[]\n",
        "            for cnt,i in enumerate(list(map(lambda x: x[0], list(k)))):\n",
        "                r1.append(cnt)\n",
        "                r2.append(d[i])\n",
        "                r3.append(d1[i])\n",
        "\n",
        "            #print(r1,r2)\n",
        "            corr, _ = stats.kendalltau(r1, r2)\n",
        "            res,_ = stats.spearmanr(r1, r2)\n",
        "\n",
        "            corr1, _ = stats.kendalltau(r1, r3)\n",
        "            res1,_ = stats.spearmanr(r1, r3)\n",
        "\n",
        "            #print('kendall tau', corr)\n",
        "            cr.append(corr)\n",
        "            #print('spearmans rho', res)\n",
        "            rr.append(res)\n",
        "\n",
        "            cr1.append(corr1)\n",
        "            rr1.append(res1)\n",
        "\n",
        "\n",
        "            rel_items = gc[counter]\n",
        "            causal = [i[0] for i in k]\n",
        "            #causal = random.sample(concept_classes,5)\n",
        "            attrib = [i[0] for i in ip_attrib]\n",
        "\n",
        "\n",
        "            #         mrr_causal = []\n",
        "            #         for i in range(len(causal)):\n",
        "            #             item = causal[i]\n",
        "            #             if item in rel_items:\n",
        "            #                 mrr_causal.append(1/(i+1))\n",
        "\n",
        "            #         mrr_attrib = []\n",
        "            #         for i in range(len(attrib)):\n",
        "            #             item = attrib[i]\n",
        "            #             if item in rel_items:\n",
        "            #                 mrr_attrib.append(1/(i+1))\n",
        "\n",
        "\n",
        "\n",
        "            #         mrr_causal = np.mean(np.asarray(mrr_causal))\n",
        "\n",
        "            #print('gc', rel_items)\n",
        "            #print('causal', causal)\n",
        "            #print('attrib', attrib)\n",
        "\n",
        "            K = 5\n",
        "\n",
        "            rel_rec_items_at_K = set(causal[:K]) & set(rel_items)\n",
        "            rec_items_at_K = causal[:K]\n",
        "            tot_rel_items = len(rel_items)\n",
        "\n",
        "            #print('Causal P@{} '.format(K), len(rel_rec_items_at_K) / len(rec_items_at_K) )\n",
        "            #print('Causal R@{} '.format(K), len(rel_rec_items_at_K) / tot_rel_items )\n",
        "\n",
        "            c_p+=len(rel_rec_items_at_K) / len(rec_items_at_K)\n",
        "            c_r+=len(rel_rec_items_at_K) / tot_rel_items\n",
        "\n",
        "\n",
        "            rel_rec_items_at_K = set(attrib[:K]) & set(rel_items)\n",
        "            rec_items_at_K = attrib[:K]\n",
        "            tot_rel_items = len(rel_items)\n",
        "\n",
        "            #print('Attrib P@{} '.format(K), len(rel_rec_items_at_K) / len(rec_items_at_K) )\n",
        "            #print('Attrib R@{} '.format(K), len(rel_rec_items_at_K) / tot_rel_items )\n",
        "\n",
        "            a_p+=len(rel_rec_items_at_K) / len(rec_items_at_K)\n",
        "            a_r+=len(rel_rec_items_at_K) / tot_rel_items\n",
        "\n",
        "\n",
        "            tot_precision_causal = []\n",
        "            for k in range(1,K+1):\n",
        "                tot_precision_causal.append(len(set(causal[:k]) & set(rel_items)) / len(causal[:k]))\n",
        "\n",
        "            tot_precision_attrib = []\n",
        "            for k in range(1,K+1):\n",
        "                tot_precision_attrib.append(len(set(attrib[:k]) & set(rel_items)) / len(attrib[:k]))\n",
        "\n",
        "\n",
        "\n",
        "            average_precision_causal += np.mean(np.asarray(tot_precision_causal))\n",
        "            average_precision_attrib += np.mean(np.asarray(tot_precision_attrib))\n",
        "\n",
        "            counter+=1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    multimodal_repr = torch.stack(multimodal_repr).squeeze(1)\n",
        "\n",
        "    #causal\n",
        "    concept_repr = torch.stack(concept_repr).squeeze(1)\n",
        "\n",
        "\n",
        "    #attribution\n",
        "    concept_repr1 = torch.stack(concept_repr1).squeeze(1)\n",
        "\n",
        "    #HM\n",
        "    concept_repr2 = torch.stack(concept_repr2).squeeze(1)\n",
        "\n",
        "    print('mean precision causal@5',c_p/counter)\n",
        "    print('mean recall causal@5', c_r/counter)\n",
        "\n",
        "    print('mean precision attrb@5',a_p/counter)\n",
        "    print('mean recall attrb@5', a_r/counter)\n",
        "\n",
        "    print('MAP@5 Causal ', average_precision_causal/counter)\n",
        "    print('MAP@5 Attrib ', average_precision_attrib/counter)\n",
        "\n",
        "    #     print('kendall tau', np.mean(cr))\n",
        "\n",
        "    #     print('spearmans rho', np.mean(rr))\n",
        "\n",
        "\n",
        "    #     print('-----------CAUSAL----------')\n",
        "    #     get_sim(multimodal_repr, concept_repr)\n",
        "\n",
        "    #     print('-----------Attribution----------')\n",
        "    #     get_sim(multimodal_repr, concept_repr1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T14:06:49.742461Z",
          "iopub.execute_input": "2024-09-21T14:06:49.742809Z",
          "iopub.status.idle": "2024-09-21T14:06:49.817964Z",
          "shell.execute_reply.started": "2024-09-21T14:06:49.742783Z",
          "shell.execute_reply": "2024-09-21T14:06:49.817190Z"
        },
        "trusted": true,
        "id": "_eDSgy3bV9Pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baire = ['ig', 'saliency', 'deeplift', 'deepliftshap', 'gradshap', 'ipxgrad']\n",
        "#R\n",
        "#baire = ['ig', 'saliency']\n",
        "import captum\n",
        "from scipy import stats\n",
        "for fu in range(len(baire)):\n",
        "\n",
        "\n",
        "    vb_wrapper = VB_wrapper(vb)\n",
        "    vb_wrapper.eval()\n",
        "    vb_wrapper.zero_grad()\n",
        "\n",
        "    if baire[fu]=='ig':\n",
        "        ig = IntegratedGradients(vb_wrapper)\n",
        "    elif baire[fu]=='saliency':\n",
        "        ig = Saliency(vb_wrapper)\n",
        "    elif baire[fu]=='deeplift':\n",
        "        ig = DeepLift(vb_wrapper)\n",
        "    elif baire[fu]=='deepliftshap':\n",
        "        ig = DeepLiftShap(vb_wrapper)\n",
        "    elif baire[fu]=='gradshap':\n",
        "        ig = GradientShap(vb_wrapper)\n",
        "    elif baire[fu]=='ipxgrad':\n",
        "        ig = InputXGradient(vb_wrapper)\n",
        "\n",
        "    call()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-21T14:06:52.166603Z",
          "iopub.execute_input": "2024-09-21T14:06:52.167518Z",
          "iopub.status.idle": "2024-09-21T14:57:19.349938Z",
          "shell.execute_reply.started": "2024-09-21T14:06:52.167486Z",
          "shell.execute_reply": "2024-09-21T14:57:19.348436Z"
        },
        "trusted": true,
        "id": "RwjnMKpcV9Pn",
        "outputId": "9e8f9afd-489f-4130-a772-93ba5c81211f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "0it [00:00, ?it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "<captum.attr._core.integrated_gradients.IntegratedGradients object at 0x78de9a181fc0>\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "660it [09:07,  1.21it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "mean precision causal@5 0.14423076923076897\nmean recall causal@5 0.20395489926739932\nmean precision attrb@5 0.20096153846153847\nmean recall attrb@5 0.28889079670329687\nMAP@5 Causal  0.13333333333333325\nMAP@5 Attrib  0.17977564102564095\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "0it [00:00, ?it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "<captum.attr._core.saliency.Saliency object at 0x78ddbd6e55a0>\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "660it [08:04,  1.36it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "mean precision causal@5 0.14423076923076897\nmean recall causal@5 0.20395489926739932\nmean precision attrb@5 0.11538461538461521\nmean recall attrb@5 0.1606856684981684\nMAP@5 Causal  0.13333333333333325\nMAP@5 Attrib  0.0979967948717948\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "0it [00:00, ?it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "<captum.attr._core.deep_lift.DeepLift object at 0x78ddbd6e49a0>\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "660it [08:01,  1.37it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "mean precision causal@5 0.14423076923076897\nmean recall causal@5 0.20395489926739932\nmean precision attrb@5 0.19423076923076918\nmean recall attrb@5 0.2725847069597071\nMAP@5 Causal  0.13333333333333325\nMAP@5 Attrib  0.17690705128205125\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "0it [00:00, ?it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "<captum.attr._core.deep_lift.DeepLiftShap object at 0x78de9a182080>\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "660it [08:40,  1.27it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "mean precision causal@5 0.14423076923076897\nmean recall causal@5 0.20395489926739932\nmean precision attrb@5 0.2028846153846156\nmean recall attrb@5 0.296537316849817\nMAP@5 Causal  0.13333333333333325\nMAP@5 Attrib  0.20371794871794868\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "0it [00:00, ?it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "<captum.attr._core.gradient_shap.GradientShap object at 0x78ddb860a380>\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "660it [08:07,  1.35it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "mean precision causal@5 0.14423076923076897\nmean recall causal@5 0.20395489926739932\nmean precision attrb@5 0.19711538461538472\nmean recall attrb@5 0.2831330128205129\nMAP@5 Causal  0.13333333333333325\nMAP@5 Attrib  0.18918269230769236\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "0it [00:00, ?it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "<captum.attr._core.input_x_gradient.InputXGradient object at 0x78ddba80e9e0>\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "660it [08:25,  1.31it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "mean precision causal@5 0.14423076923076897\nmean recall causal@5 0.20395489926739932\nmean precision attrb@5 0.19519230769230766\nmean recall attrb@5 0.27052426739926755\nMAP@5 Causal  0.13333333333333325\nMAP@5 Attrib  0.17149038461538454\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QtHqEnpXV9QE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xb9ORuvnV9QE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t-YE-PEWV9QE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IfvzN9WbV9QE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5CIjHfKWV9QE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q7FkNgRgV9QE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XR7ET0ZoV9QE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lcCULEo2V9QF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
